{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8f175c",
   "metadata": {},
   "source": [
    "# Phase 1.2 — Global Statistical Calibration  `[v5.1 — 351M-row IDS Ocean]`\n",
    "\n",
    "**Input:** `data/unified/ocean_v51/` (7 Hive-partitioned dirs, 4,195 files, 351,317,489 rows)  \n",
    "**Outputs:** `artifacts/scalers/global_port_map.json` + `artifacts/preprocessors_v51.pkl`\n",
    "\n",
    "| Step | Name | What it does |\n",
    "|------|------|--------------|\n",
    "| **2** | Global Port Scan | Stream all 4,195 parquet files → Counter → `global_port_map.json` |\n",
    "| **3** | Stratified Reservoir | 100% capture for small partitions (<1M rows), 1M-row cap for massive (start/mid/end) |\n",
    "| **4** | Multi-Block Fitting | RobustScaler(B1+B6) · QT(bytes/pkts) · PowerTransformer(port rarity) |\n",
    "| **5** | Artifact Sealing | `preprocessors_v51.pkl` · Top-10 rarest ports · Reservoir class distribution table |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd8030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python      : 3.13.9\n",
      "pandas      : 2.2.3\n",
      "numpy       : 2.1.3\n",
      "pyarrow     : 23.0.0\n",
      "Schema ver  : v5.1\n",
      "Imports OK.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, json, time, pickle, warnings, math\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer, PowerTransformer\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(it, **kw):\n",
    "        return it\n",
    "\n",
    "SCHEMA_VERSION = 'v5.1'\n",
    "\n",
    "print(f\"Python      : {sys.version.split()[0]}\")\n",
    "print(f\"pandas      : {pd.__version__}\")\n",
    "print(f\"numpy       : {np.__version__}\")\n",
    "print(f\"pyarrow     : {pa.__version__}\")\n",
    "print(f\"Schema ver  : {SCHEMA_VERSION}\")\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b82045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partition             #Files  Parquet files\n",
      "────────────────────────────────────────────────────────────\n",
      "  ubt_archetype=BOTNET_C2    1041\n",
      "  ubt_archetype=BRUTE_FORCE       8\n",
      "  ubt_archetype=DOS_DDOS     537\n",
      "  ubt_archetype=EXPLOIT      15\n",
      "  ubt_archetype=NORMAL    1408\n",
      "  ubt_archetype=SCAN    1182\n",
      "  ubt_archetype=THEFT_EXFIL       4\n",
      "────────────────────────────────────────────────────────────\n",
      "  TOTAL                 4195\n",
      "\n",
      "Ocean root   : c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\unified\\ocean_v51\n",
      "Scalers dir  : c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\artifacts\\scalers\n",
      "Port map     : c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\artifacts\\scalers\\global_port_map.json\n",
      "Preprocessors: c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\artifacts\\preprocessors_v51.pkl\n",
      "Config OK.\n"
     ]
    }
   ],
   "source": [
    "# ── Paths ─────────────────────────────────────────────────────────────────────\n",
    "NOTEBOOK_DIR       = Path.cwd()\n",
    "MAIN_DIR           = NOTEBOOK_DIR.parent\n",
    "OCEAN_V51_DIR      = MAIN_DIR / 'data' / 'unified' / 'ocean_v51'\n",
    "ARTIFACTS_DIR      = MAIN_DIR / 'artifacts'\n",
    "SCALERS_DIR        = ARTIFACTS_DIR / 'scalers'\n",
    "SCALERS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PORT_MAP_PATH      = SCALERS_DIR / 'global_port_map.json'\n",
    "PREPROCESSORS_PATH = ARTIFACTS_DIR / 'preprocessors_v51.pkl'\n",
    "\n",
    "# ── Sampling Config ────────────────────────────────────────────────────────────\n",
    "SMALL_PARTITION_THRESH = 1_000_000   # partitions with fewer rows → 100% capture\n",
    "MASSIVE_PARTITION_CAP  = 1_000_000   # rows to sample from each massive partition\n",
    "QT_N_QUANTILES         = 2_000\n",
    "\n",
    "# ── Column Groups ──────────────────────────────────────────────────────────────\n",
    "BLOCK1_COLS = [\n",
    "    'univ_duration', 'univ_bytes_in', 'univ_bytes_out',\n",
    "    'univ_pkts_in',  'univ_pkts_out',\n",
    "]\n",
    "BLOCK6_COLS = [\n",
    "    'mom_mean', 'mom_stddev', 'mom_sum', 'mom_min', 'mom_max',\n",
    "    'mom_rate', 'mom_srate', 'mom_drate',\n",
    "    'mom_TnBPSrcIP', 'mom_TnBPDstIP',\n",
    "    'mom_TnP_PSrcIP', 'mom_TnP_PDstIP',\n",
    "    'mom_TnP_PerProto', 'mom_TnP_Per_Dport',\n",
    "]\n",
    "QT_BYTE_PKT_COLS = ['univ_bytes_in', 'univ_bytes_out', 'univ_pkts_in', 'univ_pkts_out']\n",
    "PORT_COLS        = ['raw_sport', 'raw_dport']\n",
    "META_COLS        = ['ubt_archetype', 'dataset_source', 'univ_specific_attack']\n",
    "SAMPLE_COLS      = BLOCK1_COLS + BLOCK6_COLS + PORT_COLS + META_COLS\n",
    "\n",
    "UBT_ARCHETYPES = [\n",
    "    'NORMAL', 'SCAN', 'DOS_DDOS', 'BOTNET_C2',\n",
    "    'EXPLOIT', 'BRUTE_FORCE', 'THEFT_EXFIL', 'ANOMALY',\n",
    "]\n",
    "\n",
    "# ── Verify Ocean & Collect Partition Inventory ─────────────────────────────────\n",
    "assert OCEAN_V51_DIR.exists(), f\"Ocean dir not found: {OCEAN_V51_DIR}\"\n",
    "\n",
    "part_dirs        = sorted([d for d in OCEAN_V51_DIR.iterdir() if d.is_dir()])\n",
    "all_parquet_files = []\n",
    "\n",
    "print(f\"\\n{'Partition':<20} {'#Files':>7}  {'Parquet files'}\")\n",
    "print(\"─\" * 60)\n",
    "for pd_ in part_dirs:\n",
    "    files = sorted(pd_.glob('*.parquet'))\n",
    "    all_parquet_files.extend(files)\n",
    "    print(f\"  {pd_.name:<18} {len(files):>7}\")\n",
    "\n",
    "print(\"─\" * 60)\n",
    "print(f\"  {'TOTAL':<18} {len(all_parquet_files):>7}\")\n",
    "print(f\"\\nOcean root   : {OCEAN_V51_DIR}\")\n",
    "print(f\"Scalers dir  : {SCALERS_DIR}\")\n",
    "print(f\"Port map     : {PORT_MAP_PATH}\")\n",
    "print(f\"Preprocessors: {PREPROCESSORS_PATH}\")\n",
    "print(\"Config OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f659a55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PORT SCAN] Streaming 4,195 parquet files …\n",
      "   10.0%  (419/4,195)  rows=26,047,012  elapsed=7s  ETA=67s\n",
      "   20.0%  (838/4,195)  rows=46,259,083  elapsed=14s  ETA=56s\n",
      "   30.0%  (1,257/4,195)  rows=83,245,228  elapsed=25s  ETA=58s\n",
      "   40.0%  (1,676/4,195)  rows=99,306,431  elapsed=31s  ETA=46s\n",
      "   49.9%  (2,095/4,195)  rows=100,991,639  elapsed=32s  ETA=32s\n",
      "   59.9%  (2,514/4,195)  rows=109,540,368  elapsed=35s  ETA=24s\n",
      "   69.9%  (2,933/4,195)  rows=130,131,836  elapsed=41s  ETA=18s\n",
      "   79.9%  (3,352/4,195)  rows=181,309,256  elapsed=57s  ETA=14s\n",
      "   89.9%  (3,771/4,195)  rows=276,819,460  elapsed=91s  ETA=10s\n",
      "   99.9%  (4,190/4,195)  rows=351,226,310  elapsed=115s  ETA=0s\n",
      "  100.0%  (4,195/4,195)  rows=351,317,489  elapsed=115s  ETA=0s\n",
      "\n",
      "[SAVED] c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\artifacts\\scalers\\global_port_map.json\n",
      "  total_rows : 351,317,489\n",
      "  unique sport: 65,537   unique dport: 65,537\n",
      "\n",
      "── Top-10 Most Common SOURCE Ports ─────────────────────────────\n",
      "  port  63420  count=  19,779,774  freq=0.056302\n",
      "  port  17576  count=  13,656,778  freq=0.038873\n",
      "  port  18088  count=  13,655,455  freq=0.038869\n",
      "  port  18344  count=  13,655,446  freq=0.038869\n",
      "  port  17832  count=  13,655,401  freq=0.038869\n",
      "  port  36097  count=  13,629,683  freq=0.038796\n",
      "  port  44002  count=  13,617,492  freq=0.038761\n",
      "  port  38114  count=  13,615,742  freq=0.038756\n",
      "  port  38370  count=  13,613,868  freq=0.038751\n",
      "  port  43746  count=  13,609,767  freq=0.038739\n",
      "\n",
      "── Top-10 Most Common DEST Ports ───────────────────────────────\n",
      "  port     22  count=  73,785,176  freq=0.210024\n",
      "  port  52869  count=  65,183,425  freq=0.185540\n",
      "  port  37215  count=  60,990,774  freq=0.173606\n",
      "  port     23  count=  54,057,284  freq=0.153870\n",
      "  port     80  count=  35,525,636  freq=0.101121\n",
      "  port   8081  count=  32,678,804  freq=0.093018\n",
      "  port    443  count=   3,696,934  freq=0.010523\n",
      "  port  62336  count=   3,578,430  freq=0.010186\n",
      "  port   8080  count=   3,172,033  freq=0.009029\n",
      "  port     81  count=   2,579,786  freq=0.007343\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 2 — Global Port Scan\n",
    "#   Stream all 4,195 parquet files, reading ONLY raw_sport + raw_dport.\n",
    "#   Save Counter results → global_port_map.json (cache-aware).\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def run_global_port_scan(parquet_files, force_rerun=False):\n",
    "    \"\"\"Stream every parquet file and count raw_sport / raw_dport occurrences.\"\"\"\n",
    "    if PORT_MAP_PATH.exists() and not force_rerun:\n",
    "        print(f\"[CACHE HIT] Loading existing port map from:\\n  {PORT_MAP_PATH}\")\n",
    "        with open(PORT_MAP_PATH, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"  total_rows : {data['total_rows']:,}\")\n",
    "        print(f\"  unique sport: {len(data['sport']):,}   unique dport: {len(data['dport']):,}\")\n",
    "        return data\n",
    "\n",
    "    print(f\"[PORT SCAN] Streaming {len(parquet_files):,} parquet files …\")\n",
    "    sport_ctr  = Counter()\n",
    "    dport_ctr  = Counter()\n",
    "    total_rows = 0\n",
    "    n          = len(parquet_files)\n",
    "    milestone  = max(1, n // 10)\n",
    "    t0         = time.time()\n",
    "\n",
    "    for i, fp in enumerate(parquet_files):\n",
    "        try:\n",
    "            pf  = pq.ParquetFile(fp)\n",
    "            tbl = pf.read(columns=['raw_sport', 'raw_dport'])\n",
    "            df  = tbl.to_pandas()\n",
    "            sport_ctr.update(df['raw_sport'].dropna().astype(int).astype(str).tolist())\n",
    "            dport_ctr.update(df['raw_dport'].dropna().astype(int).astype(str).tolist())\n",
    "            total_rows += len(df)\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] {fp.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if (i + 1) % milestone == 0 or (i + 1) == n:\n",
    "            elapsed = time.time() - t0\n",
    "            pct     = (i + 1) / n * 100\n",
    "            eta     = elapsed / (i + 1) * (n - i - 1)\n",
    "            print(f\"  {pct:5.1f}%  ({i+1:,}/{n:,})  rows={total_rows:,}  \"\n",
    "                  f\"elapsed={elapsed:.0f}s  ETA={eta:.0f}s\")\n",
    "\n",
    "    port_map = {\n",
    "        'sport'      : dict(sport_ctr),\n",
    "        'dport'      : dict(dport_ctr),\n",
    "        'total_rows' : total_rows,\n",
    "    }\n",
    "    with open(PORT_MAP_PATH, 'w') as f:\n",
    "        json.dump(port_map, f, separators=(',', ':'))\n",
    "    print(f\"\\n[SAVED] {PORT_MAP_PATH}\")\n",
    "    print(f\"  total_rows : {total_rows:,}\")\n",
    "    print(f\"  unique sport: {len(sport_ctr):,}   unique dport: {len(dport_ctr):,}\")\n",
    "    return port_map\n",
    "\n",
    "\n",
    "# ── Run ────────────────────────────────────────────────────────────────────────\n",
    "port_map       = run_global_port_scan(all_parquet_files)\n",
    "TOTAL_ROWS_OCEAN = port_map['total_rows']\n",
    "\n",
    "sport_counts_raw = {k: int(v) for k, v in port_map['sport'].items()}\n",
    "dport_counts_raw = {k: int(v) for k, v in port_map['dport'].items()}\n",
    "\n",
    "sport_rarity = {k: v / TOTAL_ROWS_OCEAN for k, v in sport_counts_raw.items()}\n",
    "dport_rarity = {k: v / TOTAL_ROWS_OCEAN for k, v in dport_counts_raw.items()}\n",
    "\n",
    "# ── Top-10 most common ─────────────────────────────────────────────────────────\n",
    "print(f\"\\n── Top-10 Most Common SOURCE Ports ─────────────────────────────\")\n",
    "for port, cnt in sorted(sport_counts_raw.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  port {port:>6}  count={cnt:>12,}  freq={sport_rarity[port]:.6f}\")\n",
    "\n",
    "print(f\"\\n── Top-10 Most Common DEST Ports ───────────────────────────────\")\n",
    "for port, cnt in sorted(dport_counts_raw.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  port {port:>6}  count={cnt:>12,}  freq={dport_rarity[port]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e18ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "STRATIFIED RESERVOIR SAMPLING\n",
      "=================================================================\n",
      "  [BOTNET_C2] MASSIVE total=61,556,313  → sampling 1,000,000 rows (333,333/region×3)\n",
      "    → sampled  796,509 rows\n",
      "  [BRUTE_FORCE] MASSIVE total=1,718,568  → sampling 1,000,000 rows (333,333/region×3)\n",
      "    → sampled  999,996 rows\n",
      "  [DOS_DDOS] MASSIVE total=32,665,331  → sampling 1,000,000 rows (333,333/region×3)\n",
      "    → sampled  581,004 rows\n",
      "  [EXPLOIT] MASSIVE total=2,635,460  → sampling 1,000,000 rows (333,333/region×3)\n",
      "    → sampled  882,216 rows\n",
      "  [NORMAL] MASSIVE total=31,657,548  → sampling 1,000,000 rows (333,333/region×3)\n",
      "    → sampled  650,604 rows\n",
      "  [SCAN] MASSIVE total=221,084,172  → sampling 1,000,000 rows (333,333/region×3)\n",
      "    → sampled  999,492 rows\n",
      "  [THEFT_EXFIL] SMALL  total=97  → reading 100%\n",
      "    → sampled  97 rows\n",
      "\n",
      "Reservoir build time : 39.8s\n",
      "Reservoir total rows : 4,909,918\n",
      "Reservoir columns    : 24\n",
      "\n",
      "── Reservoir Class Distribution ────────────────────────────────────\n",
      "  Archetype                Rows        %\n",
      "  ──────────────────────────────────────\n",
      "  NORMAL                650,604    13.25%\n",
      "  SCAN                  999,492    20.36%\n",
      "  DOS_DDOS              581,004    11.83%\n",
      "  BOTNET_C2             796,509    16.22%\n",
      "  EXPLOIT               882,216    17.97%\n",
      "  BRUTE_FORCE           999,996    20.37%\n",
      "  THEFT_EXFIL                97     0.00%\n",
      "  ANOMALY                     0     0.00%\n",
      "  ──────────────────────────────────────\n",
      "  TOTAL               4,909,918   100.00%\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 3 — Stratified Reservoir Sampling\n",
    "#   Small partitions (< 1M rows)  → 100% capture\n",
    "#   Massive partitions (≥ 1M rows) → exactly 1,000,000 rows via start/mid/end\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def _get_avail_cols(fp, wanted):\n",
    "    \"\"\"Return the intersection of wanted columns with those actually in the file.\"\"\"\n",
    "    schema = pq.read_schema(fp)\n",
    "    avail  = set(schema.names)\n",
    "    return [c for c in wanted if c in avail]\n",
    "\n",
    "\n",
    "def _read_parquet_safe(fp, columns=None):\n",
    "    \"\"\"Read a parquet file, silently dropping any requested cols that don't exist.\"\"\"\n",
    "    try:\n",
    "        cols = _get_avail_cols(fp, columns) if columns else None\n",
    "        tbl  = pq.ParquetFile(fp).read(columns=cols)\n",
    "        return tbl.to_pandas()\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARN] read failed {fp.name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def _sub_sample_file(df, per_chunk):\n",
    "    \"\"\"\n",
    "    Take up to per_chunk rows from start, middle, and end of df.\n",
    "    NO drop_duplicates — raw row slices to preserve statistical distribution.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    if n == 0 or per_chunk <= 0:\n",
    "        return df.iloc[:0]\n",
    "    pc  = min(per_chunk, n)\n",
    "    mid = max(0, (n - pc) // 2)\n",
    "    parts = [\n",
    "        df.iloc[:pc],\n",
    "        df.iloc[mid: mid + pc],\n",
    "        df.iloc[max(0, n - pc):],\n",
    "    ]\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "\n",
    "def _fast_row_count(fp):\n",
    "    \"\"\"Get row count using parquet footer metadata (no data read).\"\"\"\n",
    "    try:\n",
    "        return pq.read_metadata(fp).num_rows\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def sample_partition(part_dir, target=MASSIVE_PARTITION_CAP, columns=None):\n",
    "    \"\"\"\n",
    "    Build a stratified sample from one partition directory.\n",
    "    Injects ubt_archetype from directory name (Hive partition key not stored in files).\n",
    "    Returns a DataFrame with numeric columns forced to float32.\n",
    "    \"\"\"\n",
    "    # Extract archetype from Hive-style dir name: `ubt_archetype=BOTNET_C2`\n",
    "    dir_name   = part_dir.name\n",
    "    archetype  = dir_name.split('=', 1)[1] if '=' in dir_name else dir_name\n",
    "\n",
    "    # Build column list — exclude ubt_archetype since it's a partition key, not in files\n",
    "    base_cols  = columns or SAMPLE_COLS\n",
    "    file_cols  = [c for c in base_cols if c != 'ubt_archetype']\n",
    "\n",
    "    files  = sorted(part_dir.glob('*.parquet'))\n",
    "    label  = archetype\n",
    "\n",
    "    if not files:\n",
    "        print(f\"  [{label}] No parquet files found — skipping.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Fast total row count via footer metadata\n",
    "    file_rows  = [(fp, _fast_row_count(fp)) for fp in files]\n",
    "    total_rows = sum(r for _, r in file_rows)\n",
    "\n",
    "    # ── Small partition: 100% ──────────────────────────────────────────────────\n",
    "    if total_rows < SMALL_PARTITION_THRESH:\n",
    "        print(f\"  [{label}] SMALL  total={total_rows:,}  → reading 100%\")\n",
    "        chunks = [_read_parquet_safe(fp, file_cols) for fp, _ in file_rows]\n",
    "        df = pd.concat([c for c in chunks if len(c)], ignore_index=True)\n",
    "\n",
    "    # ── Massive partition: start / middle / end regions ───────────────────────\n",
    "    else:\n",
    "        budget_per_region = target // 3\n",
    "        print(f\"  [{label}] MASSIVE total={total_rows:,}  → sampling {target:,} rows \"\n",
    "              f\"({budget_per_region:,}/region×3)\")\n",
    "\n",
    "        n_files = len(file_rows)\n",
    "        r1_end  = n_files // 3\n",
    "        r2_end  = 2 * n_files // 3\n",
    "\n",
    "        regions = [\n",
    "            ('start', file_rows[:r1_end]),\n",
    "            ('mid',   file_rows[r1_end:r2_end]),\n",
    "            ('end',   file_rows[r2_end:]),\n",
    "        ]\n",
    "\n",
    "        sampled_regions = []\n",
    "        for region_name, region_files in regions:\n",
    "            if not region_files:\n",
    "                continue\n",
    "            # Budget per file within this region\n",
    "            per_file_rows = max(1, budget_per_region // max(1, len(region_files)))\n",
    "            per_chunk     = max(1, per_file_rows // 3)\n",
    "\n",
    "            region_chunks  = []\n",
    "            region_count   = 0\n",
    "            for fp, _ in region_files:\n",
    "                if region_count >= budget_per_region:\n",
    "                    break\n",
    "                chunk = _read_parquet_safe(fp, file_cols)\n",
    "                if len(chunk) == 0:\n",
    "                    continue\n",
    "                sub       = _sub_sample_file(chunk, per_chunk)\n",
    "                remaining = budget_per_region - region_count\n",
    "                sub       = sub.iloc[:remaining]\n",
    "                region_chunks.append(sub)\n",
    "                region_count += len(sub)\n",
    "\n",
    "            if region_chunks:\n",
    "                sampled_regions.append(pd.concat(region_chunks, ignore_index=True))\n",
    "\n",
    "        df = pd.concat(sampled_regions, ignore_index=True) if sampled_regions else pd.DataFrame()\n",
    "\n",
    "    # ── Inject Hive partition key ──────────────────────────────────────────────\n",
    "    if len(df):\n",
    "        df['ubt_archetype'] = archetype\n",
    "\n",
    "    # ── Force float32 on all numeric columns ──────────────────────────────────\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    print(f\"    → sampled  {len(df):,} rows\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ── Build Reservoir ────────────────────────────────────────────────────────────\n",
    "print(\"=\" * 65)\n",
    "print(\"STRATIFIED RESERVOIR SAMPLING\")\n",
    "print(\"=\" * 65)\n",
    "t_res = time.time()\n",
    "\n",
    "reservoir_parts = []\n",
    "for pd_ in part_dirs:\n",
    "    reservoir_parts.append(sample_partition(pd_))\n",
    "\n",
    "reservoir = pd.concat([r for r in reservoir_parts if len(r)], ignore_index=True)\n",
    "print(f\"\\nReservoir build time : {time.time() - t_res:.1f}s\")\n",
    "print(f\"Reservoir total rows : {len(reservoir):,}\")\n",
    "print(f\"Reservoir columns    : {len(reservoir.columns)}\")\n",
    "\n",
    "# ── Class Distribution Table ───────────────────────────────────────────────────\n",
    "print(f\"\\n── Reservoir Class Distribution ────────────────────────────────────\")\n",
    "print(f\"  {'Archetype':<18} {'Rows':>10}   {'%':>6}\")\n",
    "print(\"  \" + \"─\" * 38)\n",
    "if 'ubt_archetype' in reservoir.columns:\n",
    "    vc = reservoir['ubt_archetype'].value_counts()\n",
    "    for arch in UBT_ARCHETYPES:\n",
    "        cnt = vc.get(arch, 0)\n",
    "        pct = cnt / len(reservoir) * 100 if len(reservoir) else 0\n",
    "        print(f\"  {arch:<18} {cnt:>10,}   {pct:>6.2f}%\")\n",
    "    print(\"  \" + \"─\" * 38)\n",
    "    print(f\"  {'TOTAL':<18} {len(reservoir):>10,}   100.00%\")\n",
    "else:\n",
    "    print(\"  [WARN] 'ubt_archetype' column not found in reservoir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e33be9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "STEP 4 — MULTI-BLOCK FITTING\n",
      "=================================================================\n",
      "\n",
      "[A1] Block 1 — RobustScaler(quantile_range=(5,95)) + log1p\n",
      "  univ_duration                 n=4,909,918  center=0.0000  scale=0.7947\n",
      "  univ_bytes_in                 n=4,909,918  center=0.0000  scale=6.7226\n",
      "  univ_bytes_out                n=4,909,918  center=0.0000  scale=7.9714\n",
      "  univ_pkts_in                  n=4,909,918  center=1.0986  scale=1.3863\n",
      "  univ_pkts_out                 n=4,909,918  center=0.0000  scale=2.0794\n",
      "\n",
      "[A2] Block 6 — RobustScaler(quantile_range=(5,95)) + log1p+shift\n",
      "  mom_mean                      n=   29,533  shift=1.0000e-06  center=1.3058\n",
      "  mom_stddev                    n=   29,533  shift=1.0000e-06  center=0.5955\n",
      "  mom_sum                       n=   29,533  shift=1.0000e-06  center=2.2491\n",
      "  mom_min                       n=   29,533  shift=1.0000e-06  center=0.0001\n",
      "  mom_max                       n=   29,533  shift=1.0000e-06  center=1.5966\n",
      "  mom_rate                      n=   29,533  shift=1.0000e-06  center=0.3429\n",
      "  mom_srate                     n=   29,533  shift=1.0000e-06  center=0.3029\n",
      "  mom_drate                     n=   29,533  shift=1.0000e-06  center=0.0000\n",
      "  mom_TnBPSrcIP                 n=   29,533  shift=1.0000e-06  center=10.6454\n",
      "  mom_TnBPDstIP                 n=   29,533  shift=1.0000e-06  center=10.7790\n",
      "  mom_TnP_PSrcIP                n=   29,533  shift=1.0000e-06  center=6.3008\n",
      "  mom_TnP_PDstIP                n=   29,533  shift=1.0000e-06  center=6.3986\n",
      "  mom_TnP_PerProto              n=   29,533  shift=1.0000e-06  center=6.5525\n",
      "  mom_TnP_Per_Dport             n=   29,533  shift=1.0000e-06  center=6.5410\n",
      "\n",
      "[B] QuantileTransformer(n_quantiles=2000, output='uniform') on bytes/pkts\n",
      "  univ_bytes_in                 n=4,909,918  quantiles=2000\n",
      "  univ_bytes_out                n=4,909,918  quantiles=2000\n",
      "  univ_pkts_in                  n=4,909,918  quantiles=2000\n",
      "  univ_pkts_out                 n=4,909,918  quantiles=2000\n",
      "\n",
      "[C] Port Rarity → PowerTransformer(yeo-johnson, standardize=True)\n",
      "  raw_sport     n=4,909,918  lambda=-59.6738\n",
      "  raw_dport     n=4,909,918  lambda=5.2572\n",
      "\n",
      "[DONE] All scalers fitted.\n",
      "  block1_scalers  : 5 cols\n",
      "  block6_scalers  : 14 cols\n",
      "  qt_byte_pkt     : 4 cols\n",
      "  pt_sport_rarity : OK\n",
      "  pt_dport_rarity : OK\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 4 — Multi-Block Fitting\n",
    "#   A. Block 1 & Block 6  → RobustScaler(quantile_range=(5, 95))  after log1p\n",
    "#   B. Bytes / Pkts cols  → QuantileTransformer(n=2000, uniform)\n",
    "#   C. Port rarity values → PowerTransformer(yeo-johnson)\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def _to_float64_valid(reservoir, col, sentinel=-1.0, clip_min=None):\n",
    "    \"\"\"\n",
    "    Return a clean 1-D float64 array for `col`:\n",
    "      • Drop NaN\n",
    "      • Drop sentinel rows (default -1)\n",
    "      • Optionally clip at clip_min\n",
    "    \"\"\"\n",
    "    if col not in reservoir.columns:\n",
    "        return np.array([], dtype=np.float64)\n",
    "    s = reservoir[col].astype(np.float64)\n",
    "    s = s.dropna()\n",
    "    s = s[s != sentinel]\n",
    "    if clip_min is not None:\n",
    "        s = s[s >= clip_min]\n",
    "    return s.values\n",
    "\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"STEP 4 — MULTI-BLOCK FITTING\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# ── A1. Block 1 — RobustScaler(5,95) after log1p ──────────────────────────────\n",
    "print(\"\\n[A1] Block 1 — RobustScaler(quantile_range=(5,95)) + log1p\")\n",
    "block1_scalers = {}\n",
    "for col in BLOCK1_COLS:\n",
    "    vals = _to_float64_valid(reservoir, col, sentinel=-1.0, clip_min=0.0)\n",
    "    if len(vals) < 10:\n",
    "        print(f\"  [SKIP] {col} — too few valid rows ({len(vals)})\")\n",
    "        continue\n",
    "    X   = np.log1p(vals).reshape(-1, 1)\n",
    "    rs  = RobustScaler(quantile_range=(5, 95))\n",
    "    rs.fit(X)\n",
    "    block1_scalers[col] = rs\n",
    "    print(f\"  {col:<28}  n={len(vals):>9,}  center={rs.center_[0]:.4f}  scale={rs.scale_[0]:.4f}\")\n",
    "\n",
    "# ── A2. Block 6 — RobustScaler(5,95) after log1p + shift ─────────────────────\n",
    "print(\"\\n[A2] Block 6 — RobustScaler(quantile_range=(5,95)) + log1p+shift\")\n",
    "block6_scalers = {}\n",
    "for col in BLOCK6_COLS:\n",
    "    vals = _to_float64_valid(reservoir, col, sentinel=-1.0)\n",
    "    if len(vals) < 10:\n",
    "        print(f\"  [SKIP] {col} — too few valid rows ({len(vals)})\")\n",
    "        continue\n",
    "    shift = float(max(0.0, -vals.min()) + 1e-6)   # ensure all values > 0 for log1p\n",
    "    X     = np.log1p(vals + shift).reshape(-1, 1)\n",
    "    rs    = RobustScaler(quantile_range=(5, 95))\n",
    "    rs.fit(X)\n",
    "    block6_scalers[col] = {'scaler': rs, 'shift': shift}\n",
    "    print(f\"  {col:<28}  n={len(vals):>9,}  shift={shift:.4e}  center={rs.center_[0]:.4f}\")\n",
    "\n",
    "# ── B. QuantileTransformer on bytes / pkts ─────────────────────────────────────\n",
    "print(f\"\\n[B] QuantileTransformer(n_quantiles={QT_N_QUANTILES}, output='uniform') on bytes/pkts\")\n",
    "qt_byte_pkt = {}\n",
    "for col in QT_BYTE_PKT_COLS:\n",
    "    vals = _to_float64_valid(reservoir, col, sentinel=-1.0, clip_min=0.0)\n",
    "    if len(vals) < QT_N_QUANTILES:\n",
    "        print(f\"  [SKIP] {col} — only {len(vals)} valid rows (need ≥ {QT_N_QUANTILES})\")\n",
    "        continue\n",
    "    qt = QuantileTransformer(n_quantiles=QT_N_QUANTILES, output_distribution='uniform',\n",
    "                             subsample=int(2e6), random_state=42)\n",
    "    qt.fit(vals.reshape(-1, 1))\n",
    "    qt_byte_pkt[col] = qt\n",
    "    print(f\"  {col:<28}  n={len(vals):>9,}  quantiles={qt.n_quantiles_}\")\n",
    "\n",
    "# ── C. Port Rarity → PowerTransformer (yeo-johnson) ──────────────────────────\n",
    "print(\"\\n[C] Port Rarity → PowerTransformer(yeo-johnson, standardize=True)\")\n",
    "\n",
    "def _fit_port_rarity_pt(reservoir, port_col, rarity_dict):\n",
    "    \"\"\"Map reservoir port values through rarity_dict, then fit PowerTransformer.\"\"\"\n",
    "    if port_col not in reservoir.columns:\n",
    "        print(f\"  [SKIP] {port_col} — column missing\")\n",
    "        return None\n",
    "    ports    = reservoir[port_col].dropna().astype(int).astype(str)\n",
    "    rarities = ports.map(lambda p: rarity_dict.get(p, 0.0)).values.reshape(-1, 1)\n",
    "    rarities = rarities.astype(np.float64)\n",
    "    # keep only non-zero (zero means port unseen in full scan → masked)\n",
    "    rarities = rarities[rarities[:, 0] > 0].reshape(-1, 1)\n",
    "    if len(rarities) < 10:\n",
    "        print(f\"  [SKIP] {port_col} — too few non-zero rarity values ({len(rarities)})\")\n",
    "        return None\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    pt.fit(rarities)\n",
    "    print(f\"  {port_col:<12}  n={len(rarities):>9,}  lambda={pt.lambdas_[0]:.4f}\")\n",
    "    return pt\n",
    "\n",
    "pt_sport_rarity = _fit_port_rarity_pt(reservoir, 'raw_sport', sport_rarity)\n",
    "pt_dport_rarity = _fit_port_rarity_pt(reservoir, 'raw_dport', dport_rarity)\n",
    "\n",
    "print(\"\\n[DONE] All scalers fitted.\")\n",
    "print(f\"  block1_scalers  : {len(block1_scalers)} cols\")\n",
    "print(f\"  block6_scalers  : {len(block6_scalers)} cols\")\n",
    "print(f\"  qt_byte_pkt     : {len(qt_byte_pkt)} cols\")\n",
    "print(f\"  pt_sport_rarity : {'OK' if pt_sport_rarity else 'SKIPPED'}\")\n",
    "print(f\"  pt_dport_rarity : {'OK' if pt_dport_rarity else 'SKIPPED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9378465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ARTIFACT SEALING COMPLETE\n",
      "=================================================================\n",
      "\n",
      "  preprocessors_v51.pkl   → c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\artifacts\\preprocessors_v51.pkl\n",
      "                            2.34 MB\n",
      "  global_port_map.json    → c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\artifacts\\scalers\\global_port_map.json\n",
      "                            1523.2 KB\n",
      "\n",
      "── Top-10 Rarest SOURCE Ports (lowest freq/total_rows) ──────────────\n",
      "    Port         Count          Freq\n",
      "  ────────────────────────────────────\n",
      "    6667           204      5.81e-07\n",
      "   20000           224      6.38e-07\n",
      "   10624           226      6.43e-07\n",
      "   14523           230      6.55e-07\n",
      "   24638           230      6.55e-07\n",
      "   17209           230      6.55e-07\n",
      "    6666           231      6.58e-07\n",
      "   11219           232      6.60e-07\n",
      "    7614           232      6.60e-07\n",
      "   19386           233      6.63e-07\n",
      "\n",
      "── Top-10 Rarest DEST Ports (lowest freq/total_rows) ────────────────\n",
      "    Port         Count          Freq\n",
      "  ────────────────────────────────────\n",
      "   30674            25      7.12e-08\n",
      "   16663            25      7.12e-08\n",
      "   15644            26      7.40e-08\n",
      "   31351            26      7.40e-08\n",
      "   41698            26      7.40e-08\n",
      "   22905            27      7.69e-08\n",
      "   42727            27      7.69e-08\n",
      "   54100            27      7.69e-08\n",
      "   32086            28      7.97e-08\n",
      "   42352            28      7.97e-08\n",
      "\n",
      "── Final Reservoir Class Distribution ───────────────────────────────\n",
      "  Archetype                Rows        %   Top dataset_source\n",
      "  ──────────────────────────────────────────────────────────────────────\n",
      "  NORMAL                650,604    13.25%   iot23\n",
      "  SCAN                  999,492    20.36%   iot23\n",
      "  DOS_DDOS              581,004    11.83%   iot23\n",
      "  BOTNET_C2             796,509    16.22%   iot23\n",
      "  EXPLOIT               882,216    17.97%   toniot\n",
      "  BRUTE_FORCE           999,996    20.37%   toniot\n",
      "  THEFT_EXFIL                97     0.00%   botiot\n",
      "  ANOMALY                     0     0.00%   N/A\n",
      "  ──────────────────────────────────────────────────────────────────────\n",
      "  TOTAL               4,909,918   100.00%\n",
      "\n",
      "── Bundle Contents ───────────────────────────────────────────────────\n",
      "  Keys in preprocessors_v51:\n",
      "    schema_version         → str\n",
      "    total_rows_ocean       → int\n",
      "    reservoir_rows         → int\n",
      "    qt_n_quantiles         → int\n",
      "    block1_scalers         → dict(5 entries)\n",
      "    block6_scalers         → dict(14 entries)\n",
      "    qt_byte_pkt            → dict(4 entries)\n",
      "    sport_rarity_map       → dict(65537 entries)\n",
      "    dport_rarity_map       → dict(65537 entries)\n",
      "    pt_sport_rarity        → PowerTransformer\n",
      "    pt_dport_rarity        → PowerTransformer\n",
      "    block1_cols            → list\n",
      "    block6_cols            → list\n",
      "    qt_byte_pkt_cols       → list\n",
      "    port_cols              → list\n",
      "\n",
      "=================================================================\n",
      "Phase 1.2 — Global Statistical Calibration COMPLETE\n",
      "  Ocean rows scanned : 351,317,489\n",
      "  Reservoir rows     : 4,909,918\n",
      "  Artifact size      : 2.34 MB\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 5 — Artifact Sealing\n",
    "#   Bundle all preprocessors into preprocessors_v51.pkl (protocol=4)\n",
    "#   Print: Top-10 rarest ports + Reservoir class distribution + sizes\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "preprocessors_v51 = {\n",
    "    'schema_version'   : SCHEMA_VERSION,\n",
    "    'total_rows_ocean' : TOTAL_ROWS_OCEAN,\n",
    "    'reservoir_rows'   : len(reservoir),\n",
    "    'qt_n_quantiles'   : QT_N_QUANTILES,\n",
    "    # scalers\n",
    "    'block1_scalers'   : block1_scalers,\n",
    "    'block6_scalers'   : block6_scalers,\n",
    "    'qt_byte_pkt'      : qt_byte_pkt,\n",
    "    # port rarity\n",
    "    'sport_rarity_map' : sport_rarity,\n",
    "    'dport_rarity_map' : dport_rarity,\n",
    "    'pt_sport_rarity'  : pt_sport_rarity,\n",
    "    'pt_dport_rarity'  : pt_dport_rarity,\n",
    "    # column metadata\n",
    "    'block1_cols'      : BLOCK1_COLS,\n",
    "    'block6_cols'      : BLOCK6_COLS,\n",
    "    'qt_byte_pkt_cols' : QT_BYTE_PKT_COLS,\n",
    "    'port_cols'        : PORT_COLS,\n",
    "}\n",
    "\n",
    "with open(PREPROCESSORS_PATH, 'wb') as f:\n",
    "    pickle.dump(preprocessors_v51, f, protocol=4)\n",
    "\n",
    "pkl_size_mb = os.path.getsize(PREPROCESSORS_PATH) / 1e6\n",
    "port_size_kb = os.path.getsize(PORT_MAP_PATH) / 1e3 if PORT_MAP_PATH.exists() else 0\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"ARTIFACT SEALING COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"\\n  preprocessors_v51.pkl   → {PREPROCESSORS_PATH}\")\n",
    "print(f\"                            {pkl_size_mb:.2f} MB\")\n",
    "print(f\"  global_port_map.json    → {PORT_MAP_PATH}\")\n",
    "print(f\"                            {port_size_kb:.1f} KB\")\n",
    "\n",
    "# ── Top-10 Rarest SOURCE Ports ─────────────────────────────────────────────────\n",
    "print(f\"\\n── Top-10 Rarest SOURCE Ports (lowest freq/total_rows) ──────────────\")\n",
    "print(f\"  {'Port':>6}  {'Count':>12}  {'Freq':>12}\")\n",
    "print(\"  \" + \"─\" * 36)\n",
    "rarest_sport = sorted(sport_rarity.items(), key=lambda x: x[1])[:10]\n",
    "for port, freq in rarest_sport:\n",
    "    cnt = sport_counts_raw.get(port, 0)\n",
    "    print(f\"  {port:>6}  {cnt:>12,}  {freq:>12.2e}\")\n",
    "\n",
    "# ── Top-10 Rarest DEST Ports ───────────────────────────────────────────────────\n",
    "print(f\"\\n── Top-10 Rarest DEST Ports (lowest freq/total_rows) ────────────────\")\n",
    "print(f\"  {'Port':>6}  {'Count':>12}  {'Freq':>12}\")\n",
    "print(\"  \" + \"─\" * 36)\n",
    "rarest_dport = sorted(dport_rarity.items(), key=lambda x: x[1])[:10]\n",
    "for port, freq in rarest_dport:\n",
    "    cnt = dport_counts_raw.get(port, 0)\n",
    "    print(f\"  {port:>6}  {cnt:>12,}  {freq:>12.2e}\")\n",
    "\n",
    "# ── Final Reservoir Class Distribution ────────────────────────────────────────\n",
    "print(f\"\\n── Final Reservoir Class Distribution ───────────────────────────────\")\n",
    "print(f\"  {'Archetype':<18} {'Rows':>10}   {'%':>6}   {'Top dataset_source'}\")\n",
    "print(\"  \" + \"─\" * 70)\n",
    "if 'ubt_archetype' in reservoir.columns:\n",
    "    vc = reservoir['ubt_archetype'].value_counts()\n",
    "    for arch in UBT_ARCHETYPES:\n",
    "        cnt = vc.get(arch, 0)\n",
    "        pct = cnt / len(reservoir) * 100 if len(reservoir) else 0\n",
    "        if 'dataset_source' in reservoir.columns and cnt > 0:\n",
    "            top_src = (reservoir[reservoir['ubt_archetype'] == arch]['dataset_source']\n",
    "                       .value_counts().index[0] if cnt > 0 else 'N/A')\n",
    "        else:\n",
    "            top_src = 'N/A'\n",
    "        print(f\"  {arch:<18} {cnt:>10,}   {pct:>6.2f}%   {top_src}\")\n",
    "    print(\"  \" + \"─\" * 70)\n",
    "    print(f\"  {'TOTAL':<18} {len(reservoir):>10,}   100.00%\")\n",
    "\n",
    "# ── Bundle Contents Summary ────────────────────────────────────────────────────\n",
    "print(f\"\\n── Bundle Contents ───────────────────────────────────────────────────\")\n",
    "print(f\"  Keys in preprocessors_v51:\")\n",
    "for k, v in preprocessors_v51.items():\n",
    "    if isinstance(v, dict):\n",
    "        print(f\"    {k:<22} → dict({len(v)} entries)\")\n",
    "    elif hasattr(v, '__class__'):\n",
    "        print(f\"    {k:<22} → {type(v).__name__}\")\n",
    "    else:\n",
    "        print(f\"    {k:<22} → {v}\")\n",
    "\n",
    "print(f\"\\n{'='*65}\")\n",
    "print(f\"Phase 1.2 — Global Statistical Calibration COMPLETE\")\n",
    "print(f\"  Ocean rows scanned : {TOTAL_ROWS_OCEAN:,}\")\n",
    "print(f\"  Reservoir rows     : {len(reservoir):,}\")\n",
    "print(f\"  Artifact size      : {pkl_size_mb:.2f} MB\")\n",
    "print(f\"{'='*65}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
