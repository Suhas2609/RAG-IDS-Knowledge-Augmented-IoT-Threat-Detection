{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383e63b7",
   "metadata": {},
   "source": [
    "# Phase 1.2 â€” Preprocessing & Normalization\n",
    "\n",
    "**Objective:** Transform the three aligned Parquet files produced by Phase 1.1\n",
    "(`toniot_aligned.parquet`, `iot23_complete.parquet`, `botiot_52col.parquet`) into\n",
    "mathematically normalized, stratified, and encoded numpy tensors ready for Neural\n",
    "Network training.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "| Step | Operation | Fit on | Apply to |\n",
    "|---|---|---|---|\n",
    "| 1 | **Stratified Subsampling** | â€” | All 3 Parquets |\n",
    "| 2 | **Train / Val / Test Split** | â€” | Subsample |\n",
    "| 3 | **Sentinel Imputation** | Train | Train, Val |\n",
    "| 4a | **Log1p + StandardScaler** | Train | Train, Val |\n",
    "| 4b | **OneHotEncoder** (proto, state) | Train | Train, Val |\n",
    "| 4c | **Frequency Encoding** (ports) | Train | Train, Val |\n",
    "| 5 | **Export `.npy` tensors + `preprocessors.pkl`** | â€” | â€” |\n",
    "\n",
    "## Key Constraints\n",
    "- **Memory safe:** Input is ~351M rows; must subsample before loading into RAM.\n",
    "- **No leakage:** All statistics (medians, means, stds, frequencies, encoders) are\n",
    "  fitted *exclusively* on the Training set.\n",
    "- **Rare class preservation:** Classes with fewer rows than the cap keep 100% of\n",
    "  their samples â€” never downsampled.\n",
    "- **Sentinel handling:** Group B sentinel `-1`/`-1.0` â†’ `NaN` â†’ median impute\n",
    "  (train median applied to val/test). Categorical `<absent>` treated as a valid\n",
    "  learnable token and handled by OneHotEncoder `handle_unknown='ignore'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61b17a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… pyarrow 23.0.0\n",
      "âœ… scikit-learn 1.7.0\n",
      "âœ… tqdm available\n",
      "\n",
      "âœ… Python 3.13.9\n",
      "âœ… pandas  2.2.3\n",
      "âœ… numpy   2.1.3\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 1 | Imports + Dependency Check\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import sys, os, json, time, pickle, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# â”€â”€ PyArrow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    print(f\"âœ… pyarrow {pa.__version__}\")\n",
    "except ImportError:\n",
    "    raise SystemExit(\"pyarrow not found â€” run: pip install pyarrow\")\n",
    "\n",
    "# â”€â”€ scikit-learn â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import sklearn\n",
    "    print(f\"âœ… scikit-learn {sklearn.__version__}\")\n",
    "except ImportError:\n",
    "    raise SystemExit(\"scikit-learn not found â€” run: pip install scikit-learn\")\n",
    "\n",
    "# â”€â”€ tqdm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    print(\"âœ… tqdm available\")\n",
    "except ImportError:\n",
    "    def tqdm(iterable, **kwargs):  # silent fallback\n",
    "        return iterable\n",
    "    print(\"â„¹ï¸  tqdm not found â€” progress bars disabled\")\n",
    "\n",
    "print(f\"\\nâœ… Python {sys.version.split()[0]}\")\n",
    "print(f\"âœ… pandas  {pd.__version__}\")\n",
    "print(f\"âœ… numpy   {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8511636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸ“ INPUT PARQUET FILES\n",
      "=================================================================\n",
      "  âœ…  toniot      258.1 MB        22,339,021 rows   toniot_aligned.parquet\n",
      "  âœ…  iot23      5830.1 MB       325,309,946 rows   iot23_complete.parquet\n",
      "  âœ…  botiot       82.3 MB         3,668,522 rows   botiot_52col.parquet\n",
      "\n",
      "âœ… All inputs found â€” output dir: c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 2 | Paths + Constants\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "NOTEBOOK_DIR  = Path.cwd()              # .../Phase_1/\n",
    "MAIN_DIR      = NOTEBOOK_DIR.parent    # .../main_folder/\n",
    "UNIFIED_DIR   = MAIN_DIR / \"data\" / \"unified\"\n",
    "PROCESSED_DIR = MAIN_DIR / \"data\" / \"processed\"\n",
    "ARTIFACTS_DIR = MAIN_DIR / \"artifacts\"\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Parquet inputs (produced by Phase 1.1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PARQUET_FILES = {\n",
    "    \"toniot\": UNIFIED_DIR / \"toniot_aligned.parquet\",\n",
    "    \"iot23\":  UNIFIED_DIR / \"iot23_complete.parquet\",\n",
    "    \"botiot\": UNIFIED_DIR / \"botiot_52col.parquet\",\n",
    "}\n",
    "\n",
    "# â”€â”€ Output paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_TRAIN_PATH        = PROCESSED_DIR / \"X_train.npy\"\n",
    "Y_TRAIN_PATH        = PROCESSED_DIR / \"y_train.npy\"\n",
    "X_VAL_PATH          = PROCESSED_DIR / \"X_val.npy\"\n",
    "Y_VAL_PATH          = PROCESSED_DIR / \"y_val.npy\"\n",
    "X_TEST_RAW_PATH     = PROCESSED_DIR / \"X_test_raw.parquet\"   # raw, unscaled â€” saved early\n",
    "PREPROCESSORS_PATH  = ARTIFACTS_DIR / \"preprocessors.pkl\"\n",
    "SUBSAMPLE_PATH      = PROCESSED_DIR / \"subsample.parquet\"           # written by run_subsample.py\n",
    "SUBSAMPLE_META_PATH = PROCESSED_DIR / \"subsample_metadata.parquet\"  # IPs + timestamps for RAG\n",
    "\n",
    "# â”€â”€ Column Schema (from Phase 1.1 FINAL_COLUMNS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Group A numerical features (log1p â†’ StandardScaler)\n",
    "NUM_COLS = [\n",
    "    \"univ_duration\", \"univ_src_bytes\", \"univ_dst_bytes\",\n",
    "    \"univ_src_pkts\",  \"univ_dst_pkts\",\n",
    "]\n",
    "\n",
    "# Port columns (frequency encoding)\n",
    "PORT_COLS = [\"univ_src_port\", \"univ_dst_port\"]\n",
    "\n",
    "# Boolean columns (keep as int8 â€” no scaling)\n",
    "BOOL_COLS = [\"univ_has_src_port\", \"univ_has_dst_port\"]\n",
    "\n",
    "# Categorical columns (OneHotEncoder)\n",
    "CAT_COLS = [\"univ_proto\", \"univ_state\"]\n",
    "\n",
    "# Group B numerical columns with sentinel -1 / -1.0 (impute then scale)\n",
    "SENTINEL_NUM_COLS = [\n",
    "    # Zeek shared\n",
    "    \"zeek_missed_bytes\", \"zeek_src_ip_bytes\", \"zeek_dst_ip_bytes\",\n",
    "    # TON-IoT only\n",
    "    \"toniot_dns_qclass\", \"toniot_dns_qtype\", \"toniot_dns_rcode\",\n",
    "    \"toniot_http_request_body_len\", \"toniot_http_response_body_len\",\n",
    "    \"toniot_http_status_code\",\n",
    "    # Bot-IoT behavioral windows\n",
    "    \"botiot_mean\", \"botiot_stddev\", \"botiot_sum\", \"botiot_min\",\n",
    "    \"botiot_max\",  \"botiot_rate\",   \"botiot_srate\", \"botiot_drate\",\n",
    "    \"botiot_TnBPSrcIP\", \"botiot_TnBPDstIP\",\n",
    "    \"botiot_TnP_PSrcIP\", \"botiot_TnP_PDstIP\",\n",
    "    \"botiot_TnP_PerProto\", \"botiot_TnP_Per_Dport\",\n",
    "    \"botiot_AR_P_Proto_P_SrcIP\", \"botiot_AR_P_Proto_P_DstIP\",\n",
    "    \"botiot_N_IN_Conn_P_DstIP\",  \"botiot_N_IN_Conn_P_SrcIP\",\n",
    "    \"botiot_AR_P_Proto_P_Sport\", \"botiot_AR_P_Proto_P_Dport\",\n",
    "    \"botiot_Pkts_P_State_P_Protocol_P_DestIP\",\n",
    "    \"botiot_Pkts_P_State_P_Protocol_P_SrcIP\",\n",
    "]\n",
    "\n",
    "# Group B categorical sentinels â€” treat <absent> as a valid token (no imputation)\n",
    "SENTINEL_CAT_COLS = [\"zeek_service\", \"zeek_history\"]\n",
    "\n",
    "# Metadata columns â€” saved separately for RAG, NOT included in X tensors\n",
    "META_COLS = [\"dataset_source\", \"meta_src_ip\", \"meta_dst_ip\", \"meta_timestamp\"]\n",
    "\n",
    "# Label columns\n",
    "LABEL_BINARY     = \"univ_label_binary\"\n",
    "LABEL_MULTICLASS = \"univ_label_multiclass\"\n",
    "LABEL_STR        = \"univ_label_str\"\n",
    "ATTACK_COL       = \"univ_specific_attack\"\n",
    "\n",
    "# 5-class taxonomy\n",
    "LABEL_CLASS_NAMES = {\n",
    "    0: \"Normal\",\n",
    "    1: \"Reconnaissance\",\n",
    "    2: \"Volumetric_Attack\",\n",
    "    3: \"C2_Botnet\",\n",
    "    4: \"Exploit_and_Theft\",\n",
    "}\n",
    "\n",
    "# â”€â”€ Verify inputs exist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ“ INPUT PARQUET FILES\")\n",
    "print(\"=\" * 65)\n",
    "all_found = True\n",
    "for name, path in PARQUET_FILES.items():\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / 1024**2\n",
    "        nrows   = pq.ParquetFile(str(path)).metadata.num_rows\n",
    "        print(f\"  âœ…  {name:<8s}  {size_mb:>7.1f} MB   {nrows:>15,} rows   {path.name}\")\n",
    "    else:\n",
    "        print(f\"  âŒ  {name:<8s}  MISSING â†’ {path}\")\n",
    "        all_found = False\n",
    "\n",
    "if not all_found:\n",
    "    raise FileNotFoundError(\"One or more Parquet files missing. Run Phase 1.1 first.\")\n",
    "\n",
    "print(f\"\\nâœ… All inputs found â€” output dir: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde782ba",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 â€” Stratified Subsampling\n",
    "\n",
    "**Problem:** 351M rows cannot fit in RAM.  \n",
    "**Solution:** Stream each Parquet in row-group chunks, greedily collecting rows per class\n",
    "until each class's target cap is reached.\n",
    "\n",
    "**Cap policy:**\n",
    "- Majority classes (Normal, Reconnaissance) â†’ capped at `500,000` to prevent imbalance.\n",
    "- Minority classes (C2_Botnet, Volumetric_Attack) â†’ capped at their actual count â€” i.e.,\n",
    "  100% kept if they don't exceed the cap.\n",
    "- Rarest class (Exploit_and_Theft) â†’ **always 100%** of available samples.\n",
    "\n",
    "Result: ~2â€“3M balanced rows in a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ddc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… stratified_subsample() defined\n",
      "\n",
      "Class caps:\n",
      "  Class 0 Normal              : 500000\n",
      "  Class 1 Reconnaissance      : 500000\n",
      "  Class 2 Volumetric_Attack   : 500000\n",
      "  Class 3 C2_Botnet           : ALL (keep 100%)\n",
      "  Class 4 Exploit_and_Theft   : ALL (keep 100%)\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 3 | Stratified Subsampling â€” Function\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Target row cap per class (set to None = keep all rows of that class)\n",
    "# âš ï¸  HONEST CAPS â€” must match run_subsample.py exactly for reproducibility\n",
    "CLASS_CAPS = {\n",
    "    0: 500_000,    # Normal             â€” large majority, cap to prevent dominance\n",
    "    1: 500_000,    # Reconnaissance     â€” large majority, cap to prevent dominance\n",
    "    2: 500_000,    # Volumetric_Attack  â€” keep up to 500k\n",
    "    3: 2_000_000,  # C2_Botnet          â€” CAPPED (true total is ~61M from IoT-23;\n",
    "                   #                       keeping all requires 50+ GB RAM)\n",
    "    4: None,       # Exploit_and_Theft  â€” keep 100% (~4.86M rows, feasible in RAM)\n",
    "}\n",
    "\n",
    "def stratified_subsample(\n",
    "    parquet_files: dict,\n",
    "    class_caps: dict,\n",
    "    label_col: str = LABEL_MULTICLASS,\n",
    "    seed: int = 42,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Stream all Parquet files row-group by row-group, collecting rows per class\n",
    "    until each class cap is reached.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parquet_files : dict  {name: Path}  â€” Phase 1.1 outputs\n",
    "    class_caps    : dict  {class_id: int | None}  â€” None = keep all\n",
    "    label_col     : str   â€” column holding the integer class label\n",
    "    seed          : int   â€” for reproducible row-group shuffling within a class\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame  â€” concatenated subsample, shuffled\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Buckets: class_id â†’ list of DataFrames\n",
    "    buckets: dict[int, list] = {cls: [] for cls in class_caps}\n",
    "    counts:  dict[int, int]  = {cls: 0   for cls in class_caps}\n",
    "\n",
    "    # True if a class is completely filled (cap reached)\n",
    "    def is_full(cls):\n",
    "        cap = class_caps[cls]\n",
    "        return cap is not None and counts[cls] >= cap\n",
    "\n",
    "    for ds_name, pq_path in parquet_files.items():\n",
    "        pf     = pq.ParquetFile(str(pq_path))\n",
    "        n_rg   = pf.metadata.num_row_groups\n",
    "        print(f\"\\n  â–¶  {ds_name:<8s}  ({n_rg} row-groups)\")\n",
    "\n",
    "        # Check whether ALL classes from this file are already full â†’ skip\n",
    "        if all(is_full(c) for c in class_caps):\n",
    "            print(\"       All class caps already reached â€” skipping file.\")\n",
    "            continue\n",
    "\n",
    "        rg_order = rng.permutation(n_rg)          # randomise row-group order\n",
    "\n",
    "        for rg_idx in tqdm(rg_order, desc=f\"  {ds_name}\", leave=False):\n",
    "            if all(is_full(c) for c in class_caps):\n",
    "                break\n",
    "\n",
    "            # Read only the label column first to check which classes are present\n",
    "            rg_labels = (\n",
    "                pf.read_row_group(rg_idx, columns=[label_col])\n",
    "                  .to_pandas()[label_col]\n",
    "            )\n",
    "\n",
    "            # Determine which classes in this row-group still need more rows\n",
    "            needed_classes = {c for c in class_caps if not is_full(c)}\n",
    "            has_needed = rg_labels.isin(needed_classes).any()\n",
    "            if not has_needed:\n",
    "                continue\n",
    "\n",
    "            # Read the full row-group\n",
    "            rg_df = pf.read_row_group(rg_idx).to_pandas()\n",
    "\n",
    "            for cls in needed_classes:\n",
    "                cap      = class_caps[cls]\n",
    "                cls_rows = rg_df[rg_df[label_col] == cls]\n",
    "                if cls_rows.empty:\n",
    "                    continue\n",
    "\n",
    "                if cap is not None:\n",
    "                    remaining = cap - counts[cls]\n",
    "                    if len(cls_rows) > remaining:\n",
    "                        cls_rows = cls_rows.sample(remaining, random_state=seed)\n",
    "\n",
    "                buckets[cls].append(cls_rows)\n",
    "                counts[cls] += len(cls_rows)\n",
    "\n",
    "        # Status after each file\n",
    "        for cls, cnt in counts.items():\n",
    "            cap_str = str(class_caps[cls]) if class_caps[cls] else \"ALL\"\n",
    "            print(f\"     Class {cls} {LABEL_CLASS_NAMES[cls]:<20s}: {cnt:>9,} / {cap_str}\")\n",
    "\n",
    "    # Concatenate all buckets\n",
    "    print(\"\\n  ğŸ”— Concatenating buckets â€¦\")\n",
    "    frames = []\n",
    "    for cls in class_caps:\n",
    "        if buckets[cls]:\n",
    "            frames.append(pd.concat(buckets[cls], ignore_index=True))\n",
    "\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # Global shuffle â€” ignore_index avoids an internal full-array copy\n",
    "    df = df.sample(frac=1, random_state=seed, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"âœ… stratified_subsample() defined\")\n",
    "print(f\"\\nClass caps (must match run_subsample.py):\")\n",
    "for cls, cap in CLASS_CAPS.items():\n",
    "    cap_display = f\"{cap:,}\" if cap else \"ALL (keep 100%)\"\n",
    "    print(f\"  Class {cls} {LABEL_CLASS_NAMES[cls]:<20s}: {cap_display}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92da81bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸ²  STEP 1 â€” STRATIFIED SUBSAMPLING\n",
      "=================================================================\n",
      "  âœ… Found pre-built subsample (251.2 MB) â€” loading from disk â€¦\n",
      "  â±  Loaded in 4.5s\n",
      "\n",
      "  Total rows   : 8,362,241\n",
      "  Columns      : 52\n",
      "  RAM usage    : ~6390 MB\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Class distribution in subsample:\n",
      "  Class 0 Normal              :   500,000  (5.98%)\n",
      "  Class 1 Reconnaissance      :   500,000  (5.98%)\n",
      "  Class 2 Volumetric_Attack   :   500,000  (5.98%)\n",
      "  Class 3 C2_Botnet           : 2,000,000  (23.92%)\n",
      "  Class 4 Exploit_and_Theft   : 4,862,241  (58.15%)\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 4 | Execute Stratified Subsampling\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# âš ï¸  This step streams 351M rows and is MEMORY INTENSIVE.\n",
    "#\n",
    "# RECOMMENDED: Run the standalone script FIRST (outside VS Code) to avoid\n",
    "# kernel OOM crashes:\n",
    "#\n",
    "#   python main_folder/scripts/run_subsample.py\n",
    "#\n",
    "# Once run_subsample.py has written  data/processed/subsample.parquet,\n",
    "# this cell will load from that file instantly instead of re-streaming.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ²  STEP 1 â€” STRATIFIED SUBSAMPLING\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "if SUBSAMPLE_PATH.exists():\n",
    "    # â”€â”€ Fast path: pre-built subsample exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    size_mb = SUBSAMPLE_PATH.stat().st_size / 1024**2\n",
    "    print(f\"  âœ… Found pre-built subsample ({size_mb:.1f} MB) â€” loading from disk â€¦\")\n",
    "    t0 = time.time()\n",
    "    df_sample = pd.read_parquet(SUBSAMPLE_PATH, engine=\"pyarrow\")\n",
    "    print(f\"  â±  Loaded in {time.time()-t0:.1f}s\")\n",
    "else:\n",
    "    # â”€â”€ Slow path: run in-process (may crash VS Code on large machines) â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"  âš ï¸  subsample.parquet not found â€” running in-process subsampling.\")\n",
    "    print(\"  â„¹ï¸  If VS Code crashes, run:  python main_folder/scripts/run_subsample.py\")\n",
    "    print(\"       then re-run this cell.\\n\")\n",
    "    t0 = time.time()\n",
    "    df_sample = stratified_subsample(\n",
    "        parquet_files = PARQUET_FILES,\n",
    "        class_caps    = CLASS_CAPS,\n",
    "        label_col     = LABEL_MULTICLASS,\n",
    "        seed          = 42,\n",
    "    )\n",
    "    # Save for future runs\n",
    "    print(\"\\n  ğŸ’¾ Saving subsample for future runs â€¦\")\n",
    "    df_sample.to_parquet(SUBSAMPLE_PATH, index=False, engine=\"pyarrow\")\n",
    "    print(f\"  âœ… Saved â†’ {SUBSAMPLE_PATH.name}\")\n",
    "    print(f\"  â±  Total time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "print(f\"\\n  Total rows   : {len(df_sample):,}\")\n",
    "print(f\"  Columns      : {len(df_sample.columns)}\")\n",
    "print(f\"  RAM usage    : ~{df_sample.memory_usage(deep=True).sum() / 1024**2:.0f} MB\")\n",
    "\n",
    "print(f\"\\n{'â”€'*65}\")\n",
    "print(\"Class distribution in subsample:\")\n",
    "dist = df_sample[LABEL_MULTICLASS].value_counts().sort_index()\n",
    "total = len(df_sample)\n",
    "for cls_id, cnt in dist.items():\n",
    "    print(f\"  Class {cls_id} {LABEL_CLASS_NAMES.get(cls_id, '?'):<20s}: {cnt:>9,}  ({cnt/total*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10425a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 â€” Train / Val / Test Split\n",
    "\n",
    "**Stratified 80 / 10 / 10 split** â€” same class proportions in every split.\n",
    "\n",
    "The **Test set is saved immediately as raw Parquet** and removed from memory.\n",
    "No statistics from Test can leak into the preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4b823d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "âœ‚ï¸   STEP 2 â€” STRATIFIED SPLIT  (80 / 10 / 10)\n",
      "=================================================================\n",
      "  Saving raw test set (836,225 rows) â†’ X_test_raw.parquet\n",
      "  âœ… Test set saved and removed from memory\n",
      "\n",
      "  Train rows   : 6,689,792  (â‰ˆ80%)\n",
      "  Val   rows   : 836,224  (â‰ˆ10%)\n",
      "  Test  rows   : saved to parquet â†’ not in memory\n",
      "\n",
      "  âœ… RAG metadata saved: train_meta.parquet, val_meta.parquet\n",
      "\n",
      "  Working feature columns : 44  (+1 label)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Class proportions (should be ~equal across splits):\n",
      "  Class Name                      Train      Val\n",
      "  0     Normal                  400,000   50,000\n",
      "  1     Reconnaissance          400,000   50,000\n",
      "  2     Volumetric_Attack       400,000   50,000\n",
      "  3     C2_Botnet              1,600,000  200,000\n",
      "  4     Exploit_and_Theft      3,889,792  486,224\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 5 | Stratified Train / Val / Test Split\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"âœ‚ï¸   STEP 2 â€” STRATIFIED SPLIT  (80 / 10 / 10)\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "SPLIT_SEED   = 42\n",
    "TEST_SIZE    = 0.10\n",
    "VAL_SIZE     = 0.10   # fraction of the non-test portion â†’ 10% of total\n",
    "\n",
    "# â”€â”€ First split: train+val vs test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_trainval, df_test = train_test_split(\n",
    "    df_sample,\n",
    "    test_size    = TEST_SIZE,\n",
    "    stratify     = df_sample[LABEL_MULTICLASS],\n",
    "    random_state = SPLIT_SEED,\n",
    ")\n",
    "\n",
    "# â”€â”€ Save test set immediately (raw, unscaled) â€” then drop from RAM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"  Saving raw test set ({len(df_test):,} rows) â†’ {X_TEST_RAW_PATH.name}\")\n",
    "df_test.to_parquet(X_TEST_RAW_PATH, index=False, engine=\"pyarrow\")\n",
    "del df_test   # â† prevent leakage\n",
    "print(\"  âœ… Test set saved and removed from memory\")\n",
    "\n",
    "# â”€â”€ Second split: train vs val â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# val_size relative to trainval = 10/90 â‰ˆ 0.1111  â†’ gives 10% of total\n",
    "val_ratio = VAL_SIZE / (1 - TEST_SIZE)\n",
    "df_train, df_val = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size    = val_ratio,\n",
    "    stratify     = df_trainval[LABEL_MULTICLASS],\n",
    "    random_state = SPLIT_SEED,\n",
    ")\n",
    "del df_trainval\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val   = df_val.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n  Train rows   : {len(df_train):,}  (â‰ˆ80%)\")\n",
    "print(f\"  Val   rows   : {len(df_val):,}  (â‰ˆ10%)\")\n",
    "print(f\"  Test  rows   : saved to parquet â†’ not in memory\")\n",
    "\n",
    "# â”€â”€ Detach metadata now (save separately for RAG) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_meta = df_train[META_COLS + [LABEL_MULTICLASS, LABEL_STR, ATTACK_COL]].copy()\n",
    "val_meta   = df_val[META_COLS   + [LABEL_MULTICLASS, LABEL_STR, ATTACK_COL]].copy()\n",
    "\n",
    "train_meta.to_parquet(PROCESSED_DIR / \"train_meta.parquet\", index=False)\n",
    "val_meta.to_parquet(PROCESSED_DIR   / \"val_meta.parquet\",   index=False)\n",
    "print(\"\\n  âœ… RAG metadata saved: train_meta.parquet, val_meta.parquet\")\n",
    "\n",
    "# Drop meta + string label cols from working frames (keep only features + int labels)\n",
    "DROP_FROM_X = META_COLS + [LABEL_STR, ATTACK_COL, LABEL_BINARY]\n",
    "df_train.drop(columns=DROP_FROM_X, inplace=True, errors=\"ignore\")\n",
    "df_val.drop(  columns=DROP_FROM_X, inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(f\"\\n  Working feature columns : {len(df_train.columns) - 1}  (+1 label)\")\n",
    "\n",
    "# â”€â”€ Verify stratification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n{'â”€'*65}\")\n",
    "print(\"  Class proportions (should be ~equal across splits):\")\n",
    "print(f\"  {'Class':<5s} {'Name':<22s} {'Train':>8s} {'Val':>8s}\")\n",
    "for cls in sorted(LABEL_CLASS_NAMES):\n",
    "    tr_cnt = (df_train[LABEL_MULTICLASS] == cls).sum()\n",
    "    vl_cnt = (df_val[LABEL_MULTICLASS]   == cls).sum()\n",
    "    print(f\"  {cls:<5d} {LABEL_CLASS_NAMES[cls]:<22s} {tr_cnt:>8,} {vl_cnt:>8,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453059b",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 â€” Sentinel Imputation\n",
    "\n",
    "Group B numerical columns carry `-1` / `-1.0` as a sentinel meaning\n",
    "\"this column does not exist for this dataset\".\n",
    "\n",
    "**Strategy:**\n",
    "1. Replace `-1`/`-1.0` with `NaN`.\n",
    "2. Calculate the **median** of valid (non-NaN) values **on the Training set only**.\n",
    "3. Fill NaN in Train with its own median; fill NaN in Val with the **same training median**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a351e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸ”§  STEP 3 â€” SENTINEL IMPUTATION\n",
      "=================================================================\n",
      "  Fitting medians on Training set â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Impute-fit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:10<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Fitted and applied imputation for 31 columns\n",
      "\n",
      "  Sample imputer medians:\n",
      "    zeek_missed_bytes                                 : 0.0000\n",
      "    zeek_src_ip_bytes                                 : 130.0000\n",
      "    zeek_dst_ip_bytes                                 : 132.0000\n",
      "    toniot_dns_qclass                                 : 0.0000\n",
      "    toniot_dns_qtype                                  : 0.0000\n",
      "    toniot_dns_rcode                                  : 0.0000\n",
      "    toniot_http_request_body_len                      : 0.0000\n",
      "    toniot_http_response_body_len                     : 0.0000\n",
      "    ... and 23 more\n",
      "\n",
      "  âœ… Sentinel -1 remaining in Train: 0  (should be 0)\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 6 | Sentinel Imputation (Fit on Train)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ”§  STEP 3 â€” SENTINEL IMPUTATION\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "SENTINEL_INT   = -1\n",
    "SENTINEL_FLOAT = -1.0\n",
    "\n",
    "imputer_stats: dict[str, float] = {}\n",
    "\n",
    "print(\"  Fitting medians on Training set â€¦\")\n",
    "\n",
    "for col in tqdm(SENTINEL_NUM_COLS, desc=\"  Impute-fit\"):\n",
    "    if col not in df_train.columns:\n",
    "        continue\n",
    "\n",
    "    # Replace both sentinel forms with NaN\n",
    "    train_col = df_train[col].astype(float).replace(\n",
    "        {float(SENTINEL_INT): np.nan, SENTINEL_FLOAT: np.nan}\n",
    "    )\n",
    "\n",
    "    # Calculate median of valid values (Training only)\n",
    "    median_val = float(train_col.median())   # NaN-safe\n",
    "    if np.isnan(median_val):\n",
    "        median_val = 0.0    # fallback: all values were sentinel â†’ impute 0\n",
    "\n",
    "    imputer_stats[col] = median_val\n",
    "\n",
    "    # Fill Training set\n",
    "    df_train[col] = train_col.fillna(median_val).astype(float)\n",
    "\n",
    "\n",
    "def apply_imputation(df: pd.DataFrame, stats: dict) -> pd.DataFrame:\n",
    "    \"\"\"Apply pre-fitted imputation medians to a DataFrame (val / test).\"\"\"\n",
    "    df = df.copy()\n",
    "    for col, median_val in stats.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        df[col] = (\n",
    "            df[col].astype(float)\n",
    "                   .replace({float(SENTINEL_INT): np.nan, SENTINEL_FLOAT: np.nan})\n",
    "                   .fillna(median_val)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "df_val = apply_imputation(df_val, imputer_stats)\n",
    "\n",
    "print(f\"  âœ… Fitted and applied imputation for {len(imputer_stats)} columns\")\n",
    "print(f\"\\n  Sample imputer medians:\")\n",
    "sample_cols = list(imputer_stats.items())[:8]\n",
    "for col, med in sample_cols:\n",
    "    print(f\"    {col:<50s}: {med:.4f}\")\n",
    "if len(imputer_stats) > 8:\n",
    "    print(f\"    ... and {len(imputer_stats) - 8} more\")\n",
    "\n",
    "# Verify no -1 sentinels remain in sentinel columns\n",
    "sentinel_remaining_train = 0\n",
    "for col in SENTINEL_NUM_COLS:\n",
    "    if col in df_train.columns:\n",
    "        sentinel_remaining_train += (df_train[col] == SENTINEL_FLOAT).sum()\n",
    "print(f\"\\n  âœ… Sentinel -1 remaining in Train: {sentinel_remaining_train}  (should be 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161a9f3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 â€” Feature Engineering\n",
    "\n",
    "### 4a. Numerical: `log1p` â†’ `StandardScaler`\n",
    "Network traffic features are heavy-tailed (src_bytes can span 0â€“10â¹).\n",
    "`log1p` compresses extreme values before Z-score normalization.\n",
    "\n",
    "### 4b. Categorical: `OneHotEncoder`\n",
    "Applied to `univ_proto` (5 tokens) and `univ_state` (5 tokens).\n",
    "`<absent>` is a valid vocab token and will produce its own OHE column.\n",
    "\n",
    "### 4c. Port Frequency Encoding\n",
    "Port numbers range 0â€“65535. Replacing each port with its frequency (probability\n",
    "of occurrence in Training) gives a single continuous feature that captures\n",
    "how \"common\" the port is â€” without inflating dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cacbe899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸ“  STEP 4a â€” NUMERICAL: log1p + StandardScaler\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  log1p: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:05<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scaled 36 numerical columns\n",
      "  Train mean (first 3 cols): {'univ_duration': -0.0, 'univ_src_bytes': -0.0, 'univ_dst_bytes': -0.0}\n",
      "  Train std  (first 3 cols): {'univ_duration': 1.0, 'univ_src_bytes': 1.0, 'univ_dst_bytes': 1.0}\n",
      "  âœ… StandardScaler fitted on Train, applied to Val\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 7 | Feature Engineering 4a â€” log1p + StandardScaler (Numerical)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ“  STEP 4a â€” NUMERICAL: log1p + StandardScaler\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Columns to scale: Group A numericals + Group B sentinel-imputed numericals\n",
    "ALL_NUM_COLS = NUM_COLS + SENTINEL_NUM_COLS\n",
    "ALL_NUM_COLS = [c for c in ALL_NUM_COLS if c in df_train.columns]\n",
    "\n",
    "# â”€â”€ log1p transform â€” clip negatives to 0 first (sentinel cols already imputed) â”€\n",
    "for col in tqdm(ALL_NUM_COLS, desc=\"  log1p\"):\n",
    "    df_train[col] = np.log1p(np.maximum(df_train[col].astype(float), 0.0))\n",
    "    df_val[col]   = np.log1p(np.maximum(df_val[col].astype(float),   0.0))\n",
    "\n",
    "# â”€â”€ StandardScaler â€” fit on Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scaler = StandardScaler()\n",
    "df_train[ALL_NUM_COLS] = scaler.fit_transform(df_train[ALL_NUM_COLS].astype(float))\n",
    "df_val[ALL_NUM_COLS]   = scaler.transform(df_val[ALL_NUM_COLS].astype(float))\n",
    "\n",
    "print(f\"  Scaled {len(ALL_NUM_COLS)} numerical columns\")\n",
    "print(f\"  Train mean (first 3 cols): \"\n",
    "      f\"{df_train[ALL_NUM_COLS[:3]].mean().round(4).to_dict()}\")\n",
    "print(f\"  Train std  (first 3 cols): \"\n",
    "      f\"{df_train[ALL_NUM_COLS[:3]].std().round(4).to_dict()}\")\n",
    "print(\"  âœ… StandardScaler fitted on Train, applied to Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1ff73e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸ”¡  STEP 4b â€” CATEGORICAL: OneHotEncoder\n",
      "=================================================================\n",
      "  ğŸ—‘ï¸  Dropped source-identity columns (not encoded): ['zeek_service', 'zeek_history']\n",
      "  Encoding columns: ['univ_proto', 'univ_state']\n",
      "  OHE output shape  â€” Train: (6689792, 8), Val: (836224, 8)\n",
      "  Generated 8 OHE features:\n",
      "    â€¢ univ_proto_arp\n",
      "    â€¢ univ_proto_icmp\n",
      "    â€¢ univ_proto_tcp\n",
      "    â€¢ univ_proto_udp\n",
      "    â€¢ univ_state_attempt\n",
      "    â€¢ univ_state_established\n",
      "    â€¢ univ_state_other\n",
      "    â€¢ univ_state_rejected\n",
      "  âœ… OneHotEncoder fitted on Train, applied to Val\n",
      "  âœ… Source-identity columns excluded â€” model must learn attacks, not dataset origin\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 8 | Feature Engineering 4b â€” OneHotEncoder (Categorical)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ”¡  STEP 4b â€” CATEGORICAL: OneHotEncoder\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# â”€â”€ Source-identity leak guard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# SENTINEL_CAT_COLS = [\"zeek_service\", \"zeek_history\"]\n",
    "#\n",
    "# These columns are NOT included in OHE. Here is why:\n",
    "#   - Bot-IoT has no Zeek data  â†’ both columns are \"<absent>\"\n",
    "#   - TON-IoT has no zeek_history â†’ it is \"<absent>\"\n",
    "#   - IoT-23 has genuine Zeek values\n",
    "#\n",
    "# If we OHE \"<absent>\", the model gets a feature \"zeek_history_<absent>\"\n",
    "# which == 1 for every non-IoT-23 row. The NN would learn dataset source\n",
    "# as a proxy rather than true attack patterns â€” a form of data leakage.\n",
    "#\n",
    "# Fix: drop both Zeek sentinel categoricals from the feature matrix entirely.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Drop zeek sentinel categoricals before OHE (source-identity leak prevention)\n",
    "ZEEK_SOURCE_COLS = [c for c in SENTINEL_CAT_COLS if c in df_train.columns]\n",
    "if ZEEK_SOURCE_COLS:\n",
    "    df_train.drop(columns=ZEEK_SOURCE_COLS, inplace=True)\n",
    "    df_val.drop(  columns=ZEEK_SOURCE_COLS, inplace=True)\n",
    "    print(f\"  ğŸ—‘ï¸  Dropped source-identity columns (not encoded): {ZEEK_SOURCE_COLS}\")\n",
    "\n",
    "# Only OHE true protocol/state categoricals\n",
    "ALL_CAT_COLS = [c for c in CAT_COLS if c in df_train.columns]\n",
    "print(f\"  Encoding columns: {ALL_CAT_COLS}\")\n",
    "\n",
    "# Fill any NaN in categorical columns with '<absent>'\n",
    "for col in ALL_CAT_COLS:\n",
    "    df_train[col] = df_train[col].fillna(\"<absent>\").astype(str)\n",
    "    df_val[col]   = df_val[col].fillna(\"<absent>\").astype(str)\n",
    "\n",
    "ohe = OneHotEncoder(\n",
    "    handle_unknown = \"ignore\",    # unseen tokens in val â†’ all-zero row\n",
    "    sparse_output  = False,       # dense numpy array\n",
    "    dtype          = np.float32,\n",
    ")\n",
    "\n",
    "# Fit on Train\n",
    "ohe_train = ohe.fit_transform(df_train[ALL_CAT_COLS])\n",
    "ohe_val   = ohe.transform(df_val[ALL_CAT_COLS])\n",
    "\n",
    "ohe_feature_names = ohe.get_feature_names_out(ALL_CAT_COLS)\n",
    "\n",
    "print(f\"  OHE output shape  â€” Train: {ohe_train.shape}, Val: {ohe_val.shape}\")\n",
    "print(f\"  Generated {len(ohe_feature_names)} OHE features:\")\n",
    "for name in ohe_feature_names:\n",
    "    print(f\"    â€¢ {name}\")\n",
    "\n",
    "# Drop original categorical columns from DataFrames\n",
    "df_train.drop(columns=ALL_CAT_COLS, inplace=True)\n",
    "df_val.drop(  columns=ALL_CAT_COLS, inplace=True)\n",
    "\n",
    "print(\"  âœ… OneHotEncoder fitted on Train, applied to Val\")\n",
    "print(\"  âœ… Source-identity columns excluded â€” model must learn attacks, not dataset origin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cf56d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸ”¢  STEP 4c â€” PORT FREQUENCY ENCODING\n",
      "=================================================================\n",
      "  univ_src_port       : 65,283 unique ports in Train â†’ encoded as frequency\n",
      "  univ_dst_port       : 64,998 unique ports in Train â†’ encoded as frequency\n",
      "  âœ… Frequency encoding applied â€” unseen Val ports mapped to 0.0\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 9 | Feature Engineering 4c â€” Port Frequency Encoding\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ”¢  STEP 4c â€” PORT FREQUENCY ENCODING\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "port_freq_maps: dict[str, dict] = {}\n",
    "\n",
    "for col in PORT_COLS:\n",
    "    if col not in df_train.columns:\n",
    "        continue\n",
    "\n",
    "    # Compute frequency (probability) of each port value in Training set\n",
    "    freq = df_train[col].value_counts(normalize=True).to_dict()\n",
    "    port_freq_maps[col] = freq\n",
    "\n",
    "    # Map train ports â†’ frequency; unknown ports â†’ 0.0\n",
    "    df_train[col] = df_train[col].map(freq).fillna(0.0).astype(np.float32)\n",
    "    df_val[col]   = df_val[col].map(freq).fillna(0.0).astype(np.float32)\n",
    "\n",
    "    n_unique = len(freq)\n",
    "    print(f\"  {col:<20s}: {n_unique:,} unique ports in Train â†’ encoded as frequency\")\n",
    "\n",
    "print(\"  âœ… Frequency encoding applied â€” unseen Val ports mapped to 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c677f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸ§©  ASSEMBLING FINAL FEATURE MATRICES\n",
      "=================================================================\n",
      "  X_train : (6689792, 48)   dtype=float32\n",
      "  y_train : (6689792,)   dtype=int8\n",
      "  X_val   : (836224, 48)   dtype=float32\n",
      "  y_val   : (836224,)   dtype=int8\n",
      "  Feature count: 48\n",
      "  âœ… No NaN / Inf in X_train or X_val\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 10 | Assemble Final Feature Matrices\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ§©  ASSEMBLING FINAL FEATURE MATRICES\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Extract labels before dropping\n",
    "y_train = df_train.pop(LABEL_MULTICLASS).to_numpy(dtype=np.int8)\n",
    "y_val   = df_val.pop(LABEL_MULTICLASS).to_numpy(dtype=np.int8)\n",
    "\n",
    "# Remaining numeric columns in the DataFrames (NUM_COLS + SENTINEL_NUM_COLS + port\n",
    "# + BOOL_COLS â€” all already scaled/encoded/frequency-encoded)\n",
    "dense_feature_cols = [c for c in df_train.columns if c in df_train.columns]\n",
    "\n",
    "X_train_dense = df_train[dense_feature_cols].to_numpy(dtype=np.float32)\n",
    "X_val_dense   = df_val[dense_feature_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Horizontally stack dense features + OHE columns\n",
    "X_train = np.hstack([X_train_dense, ohe_train]).astype(np.float32)\n",
    "X_val   = np.hstack([X_val_dense,   ohe_val  ]).astype(np.float32)\n",
    "\n",
    "# Record feature names for later interpretability\n",
    "feature_names = dense_feature_cols + list(ohe_feature_names)\n",
    "\n",
    "print(f\"  X_train : {X_train.shape}   dtype={X_train.dtype}\")\n",
    "print(f\"  y_train : {y_train.shape}   dtype={y_train.dtype}\")\n",
    "print(f\"  X_val   : {X_val.shape}   dtype={X_val.dtype}\")\n",
    "print(f\"  y_val   : {y_val.shape}   dtype={y_val.dtype}\")\n",
    "print(f\"  Feature count: {X_train.shape[1]}\")\n",
    "\n",
    "# Sanity check: no NaNs in final arrays\n",
    "assert not np.isnan(X_train).any(), \"âŒ NaN detected in X_train!\"\n",
    "assert not np.isnan(X_val).any(),   \"âŒ NaN detected in X_val!\"\n",
    "assert not np.isinf(X_train).any(), \"âŒ Inf detected in X_train!\"\n",
    "print(\"  âœ… No NaN / Inf in X_train or X_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35cae8",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 â€” Export Processed Tensors\n",
    "\n",
    "- `X_train.npy`, `y_train.npy` â€” Training tensors\n",
    "- `X_val.npy`, `y_val.npy` â€” Validation tensors\n",
    "- `X_test_raw.parquet` â€” Raw (unscaled) test set (already saved in Step 2)\n",
    "- `preprocessors.pkl` â€” All fitted objects (scaler, OHE, port freq maps, imputer stats)\n",
    "- `feature_names.json` â€” Ordered list of feature names for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10fd84dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸ’¾  STEP 5 â€” EXPORTING TENSORS + PREPROCESSORS\n",
      "=================================================================\n",
      "  Saved tensors in 2.7s:\n",
      "    âœ…  X_train.npy                shape=(6689792, 48)  1224.9 MB\n",
      "    âœ…  y_train.npy                shape=(6689792,)  6.4 MB\n",
      "    âœ…  X_val.npy                  shape=(836224, 48)  153.1 MB\n",
      "    âœ…  y_val.npy                  shape=(836224,)  0.8 MB\n",
      "\n",
      "    âœ…  preprocessors.pkl          1531.4 KB\n",
      "    âœ…  phase1_2_feature_names.json  48 features listed\n",
      "\n",
      "  Output directory: c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 11 | Export Tensors + Preprocessors\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ’¾  STEP 5 â€” EXPORTING TENSORS + PREPROCESSORS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# â”€â”€ 1. Numpy tensors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "t0 = time.time()\n",
    "np.save(X_TRAIN_PATH, X_train)\n",
    "np.save(Y_TRAIN_PATH, y_train)\n",
    "np.save(X_VAL_PATH,   X_val)\n",
    "np.save(Y_VAL_PATH,   y_val)\n",
    "print(f\"  Saved tensors in {time.time()-t0:.1f}s:\")\n",
    "for path, arr in [\n",
    "    (X_TRAIN_PATH, X_train),\n",
    "    (Y_TRAIN_PATH, y_train),\n",
    "    (X_VAL_PATH,   X_val),\n",
    "    (Y_VAL_PATH,   y_val),\n",
    "]:\n",
    "    size_mb = path.stat().st_size / 1024**2\n",
    "    print(f\"    âœ…  {path.name:<25s}  shape={arr.shape}  {size_mb:.1f} MB\")\n",
    "\n",
    "# â”€â”€ 2. Preprocessor bundle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "preprocessors = {\n",
    "    \"scaler\":          scaler,\n",
    "    \"ohe\":             ohe,\n",
    "    \"port_freq_maps\":  port_freq_maps,\n",
    "    \"imputer_stats\":   imputer_stats,\n",
    "    \"all_num_cols\":    ALL_NUM_COLS,\n",
    "    \"all_cat_cols\":    ALL_CAT_COLS,\n",
    "    \"port_cols\":       PORT_COLS,\n",
    "    \"bool_cols\":       BOOL_COLS,\n",
    "    \"sentinel_num_cols\": SENTINEL_NUM_COLS,\n",
    "    \"feature_names\":   feature_names,\n",
    "    \"label_class_names\": LABEL_CLASS_NAMES,\n",
    "    \"meta\": {\n",
    "        \"phase\":           \"Phase_1_2_Preprocessing_Normalization\",\n",
    "        \"n_features\":      X_train.shape[1],\n",
    "        \"n_train\":         len(y_train),\n",
    "        \"n_val\":           len(y_val),\n",
    "        \"class_caps\":      CLASS_CAPS,\n",
    "        \"split_seed\":      SPLIT_SEED,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(PREPROCESSORS_PATH, \"wb\") as fh:\n",
    "    pickle.dump(preprocessors, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"\\n    âœ…  {PREPROCESSORS_PATH.name:<25s}  \"\n",
    "      f\"{PREPROCESSORS_PATH.stat().st_size/1024:.1f} KB\")\n",
    "\n",
    "# â”€â”€ 3. Feature names JSON (human-readable) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "feat_names_path = ARTIFACTS_DIR / \"phase1_2_feature_names.json\"\n",
    "with open(feat_names_path, \"w\") as fh:\n",
    "    json.dump({\"feature_names\": feature_names, \"n_features\": len(feature_names)}, fh, indent=2)\n",
    "print(f\"    âœ…  {feat_names_path.name:<25s}  {len(feature_names)} features listed\")\n",
    "\n",
    "print(f\"\\n  Output directory: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8353802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”¬  FINAL VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "ğŸ“  Array shapes:\n",
      "   X_train : (6689792, 48)   dtype=float32\n",
      "   y_train : (6689792,)   dtype=int8\n",
      "   X_val   : (836224, 48)   dtype=float32\n",
      "   y_val   : (836224,)   dtype=int8\n",
      "\n",
      "ğŸ“Š  Class distribution:\n",
      "   Class Name                        Train  Train%       Val    Val%\n",
      "   0     Normal                    400,000   5.98%    50,000   5.98%\n",
      "   1     Reconnaissance            400,000   5.98%    50,000   5.98%\n",
      "   2     Volumetric_Attack         400,000   5.98%    50,000   5.98%\n",
      "   3     C2_Botnet               1,600,000  23.92%   200,000  23.92%\n",
      "   4     Exploit_and_Theft       3,889,792  58.15%   486,224  58.15%\n",
      "\n",
      "âœ…  Sanity checks:\n",
      "   NaN in X_train : False  (should be False)\n",
      "   NaN in X_val   : False  (should be False)\n",
      "   Inf in X_train : False  (should be False)\n",
      "   Feature count  : 48\n",
      "\n",
      "ğŸ“¦  Output files:\n",
      "   subsample.parquet                         251.2 MB\n",
      "   train_meta.parquet                        83.4 MB\n",
      "   val_meta.parquet                          10.4 MB\n",
      "   X_test_raw.parquet                        25.0 MB\n",
      "   X_train.npy                               1224.9 MB\n",
      "   X_val.npy                                 153.1 MB\n",
      "   y_train.npy                               6.4 MB\n",
      "   y_val.npy                                 0.8 MB\n",
      "\n",
      "======================================================================\n",
      "âœ…  Phase 1.2 complete â€” tensors ready for Phase 2 model training\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 12 | Final Verification\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”¬  FINAL VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Reload from disk to confirm round-trip integrity\n",
    "X_train_check = np.load(X_TRAIN_PATH)\n",
    "y_train_check = np.load(Y_TRAIN_PATH)\n",
    "X_val_check   = np.load(X_VAL_PATH)\n",
    "y_val_check   = np.load(Y_VAL_PATH)\n",
    "\n",
    "print(\"\\nğŸ“  Array shapes:\")\n",
    "print(f\"   X_train : {X_train_check.shape}   dtype={X_train_check.dtype}\")\n",
    "print(f\"   y_train : {y_train_check.shape}   dtype={y_train_check.dtype}\")\n",
    "print(f\"   X_val   : {X_val_check.shape}   dtype={X_val_check.dtype}\")\n",
    "print(f\"   y_val   : {y_val_check.shape}   dtype={y_val_check.dtype}\")\n",
    "\n",
    "print(\"\\nğŸ“Š  Class distribution:\")\n",
    "print(f\"   {'Class':<5s} {'Name':<22s} {'Train':>10s} {'Train%':>7s}  {'Val':>8s} {'Val%':>7s}\")\n",
    "for cls in sorted(LABEL_CLASS_NAMES):\n",
    "    tr_cnt = (y_train_check == cls).sum()\n",
    "    vl_cnt = (y_val_check   == cls).sum()\n",
    "    tr_pct = tr_cnt / len(y_train_check) * 100\n",
    "    vl_pct = vl_cnt / len(y_val_check)   * 100\n",
    "    print(f\"   {cls:<5d} {LABEL_CLASS_NAMES[cls]:<22s} {tr_cnt:>10,} {tr_pct:>6.2f}%  {vl_cnt:>8,} {vl_pct:>6.2f}%\")\n",
    "\n",
    "print(\"\\nâœ…  Sanity checks:\")\n",
    "print(f\"   NaN in X_train : {np.isnan(X_train_check).any()}  (should be False)\")\n",
    "print(f\"   NaN in X_val   : {np.isnan(X_val_check).any()}  (should be False)\")\n",
    "print(f\"   Inf in X_train : {np.isinf(X_train_check).any()}  (should be False)\")\n",
    "print(f\"   Feature count  : {X_train_check.shape[1]}\")\n",
    "\n",
    "print(\"\\nğŸ“¦  Output files:\")\n",
    "for p in sorted(PROCESSED_DIR.iterdir()):\n",
    "    print(f\"   {p.name:<40s}  {p.stat().st_size/1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ…  Phase 1.2 complete â€” tensors ready for Phase 2 model training\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a69ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Staging files...\n",
      "  âœ… staged: Phase_1_2_Preprocessing_Normalization.ipynb\n",
      "STDERR: The following paths are ignored by one of your .gitignore files:\n",
      "main_folder/artifacts/preprocessors.pkl\n",
      "hint: Use -f if you really want to add them.\n",
      "hint: Disable this message with \"git config set advice.addIgnoredFile false\"\n",
      "  âš ï¸  staged: preprocessors.pkl\n",
      "  âœ… staged: phase1_2_feature_names.json\n",
      "  âœ… staged: Phase_1_1_Universal_Schema_Alignment.md\n",
      "\n",
      "ğŸ“Œ Committing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 66fca55] Phase_1_2: Preprocessing & Normalization Ã¢â‚¬â€ stratified subsample, 80/10/10 split, sentinel imputation, log1p+StandardScaler, OHE, port-freq encoding; X_train/X_val tensors + preprocessors.pkl exported\n",
      " 2 files changed, 398 insertions(+), 140 deletions(-)\n",
      "\n",
      "ğŸ“Œ Pushing to remote...\n",
      "\n",
      "âœ… Phase 1.2 committed and pushed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 13 | Git Commit\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "MAIN_DIR     = NOTEBOOK_DIR.parent\n",
    "REPO_ROOT    = MAIN_DIR.parent\n",
    "\n",
    "def git(args: list, cwd=REPO_ROOT):\n",
    "    result = subprocess.run(\n",
    "        [\"git\"] + args,\n",
    "        cwd            = str(cwd),\n",
    "        capture_output = True,\n",
    "        text           = True,\n",
    "    )\n",
    "    if result.stdout.strip():\n",
    "        print(result.stdout.strip())\n",
    "    if result.stderr.strip() and result.returncode != 0:\n",
    "        print(\"STDERR:\", result.stderr.strip())\n",
    "    return result.returncode\n",
    "\n",
    "print(\"ğŸ“Œ Staging files...\")\n",
    "files_to_stage = [\n",
    "    \"main_folder/Phase_1/Phase_1_2_Preprocessing_Normalization.ipynb\",\n",
    "    \"main_folder/artifacts/preprocessors.pkl\",\n",
    "    \"main_folder/artifacts/phase1_2_feature_names.json\",\n",
    "    \"main_folder/Phase_1_1_Universal_Schema_Alignment.md\",\n",
    "]\n",
    "\n",
    "for f in files_to_stage:\n",
    "    rc = git([\"add\", f])\n",
    "    status = \"âœ…\" if rc == 0 else \"âš ï¸ \"\n",
    "    print(f\"  {status} staged: {f.split('/')[-1]}\")\n",
    "\n",
    "print(\"\\nğŸ“Œ Committing...\")\n",
    "commit_msg = (\n",
    "    \"Phase_1_2: Preprocessing & Normalization â€” \"\n",
    "    \"stratified subsample, 80/10/10 split, sentinel imputation, \"\n",
    "    \"log1p+StandardScaler, OHE, port-freq encoding; \"\n",
    "    \"X_train/X_val tensors + preprocessors.pkl exported\"\n",
    ")\n",
    "git([\"commit\", \"-m\", commit_msg])\n",
    "\n",
    "print(\"\\nğŸ“Œ Pushing to remote...\")\n",
    "git([\"push\"])\n",
    "\n",
    "print(\"\\nâœ… Phase 1.2 committed and pushed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
