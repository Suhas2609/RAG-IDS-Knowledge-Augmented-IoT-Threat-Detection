{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d959e6e5",
   "metadata": {},
   "source": [
    "# Phase 1.1 â€” Universal Schema Alignment\n",
    "\n",
    "**Objective:** Synthesize TON-IoT, IoT-23, and Bot-IoT into a single homogeneous\n",
    "schema using Option B (Union) with Group A/B partitioning.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### Group A â€” Universal Core (9 behavioral features + 2 ports + 2 booleans + 3 labels)\n",
    "Present in all datasets. Renamed to `univ_` prefix. Used for cross-dataset RAG retrieval.\n",
    "\n",
    "| `univ_` field | TON-IoT source | IoT-23 source | Bot-IoT source |\n",
    "|---|---|---|---|\n",
    "| `univ_duration` | `duration` | `duration` | `dur` |\n",
    "| `univ_src_bytes` | `src_bytes` | `orig_bytes` | `sbytes` |\n",
    "| `univ_dst_bytes` | `dst_bytes` | `resp_bytes` | `dbytes` |\n",
    "| `univ_src_pkts` | `src_pkts` | `orig_pkts` | `spkts` |\n",
    "| `univ_dst_pkts` | `dst_pkts` | `resp_pkts` | `dpkts` |\n",
    "| `univ_proto` | `proto` | `proto` | `proto` |\n",
    "| `univ_state` | `conn_state` | `conn_state` | `state` |\n",
    "| `univ_src_port` | `src_port` | `id.orig_p` | `sport` |\n",
    "| `univ_dst_port` | `dst_port` | `id.resp_p` | `dport` |\n",
    "| `univ_has_src_port` | derived | derived | derived (Argus -1 flag) |\n",
    "| `univ_has_dst_port` | derived | derived | derived (Argus -1 flag) |\n",
    "| `univ_label_binary` | `label` | `label` | `attack` |\n",
    "| `univ_label_multiclass` | mapped from `type` | mapped from `detailed-label` | mapped from `category`/`subcategory` |\n",
    "| `univ_label_str` | human-readable | human-readable | human-readable |\n",
    "\n",
    "### Group B â€” Dataset-Conditional Features (injected with sentinel when absent)\n",
    "- **Sentinel for missing categoricals:** `\"unknown\"` (learnable distinct state)\n",
    "- **Sentinel for missing numericals (float):** `-1.0`\n",
    "- **Sentinel for missing numericals (int):** `-1`\n",
    "\n",
    "### Processing Strategy\n",
    "- File-by-file chunked streaming (100k rows/chunk) â†’ write row-groups to Parquet\n",
    "- PyArrow ParquetWriter for incremental, schema-consistent append\n",
    "- OOM ceiling: ~50 MB RAM per chunk at any time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a467d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… pyarrow 23.0.0  â€” Parquet write/read enabled\n",
      "âœ… tqdm available\n",
      "\n",
      "âœ… Python 3.13.9\n",
      "âœ… pandas  2.2.3\n",
      "âœ… numpy   2.1.3\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 1 | Imports + Dependency Check\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import sys, os, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€ PyArrow (Parquet engine) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    print(f\"âœ… pyarrow {pa.__version__}  â€” Parquet write/read enabled\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  pyarrow not found â€” installing...\")\n",
    "    os.system(f\"{sys.executable} -m pip install pyarrow -q\")\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    print(f\"âœ… pyarrow {pa.__version__}  â€” installed successfully\")\n",
    "\n",
    "# â”€â”€ tqdm (progress bars) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    print(f\"âœ… tqdm available\")\n",
    "except ImportError:\n",
    "    # Graceful fallback â€” tqdm is optional\n",
    "    class tqdm:\n",
    "        def __init__(self, iterable=None, **kw): self._it = iterable or []\n",
    "        def __iter__(self): return iter(self._it)\n",
    "        def update(self, *a): pass\n",
    "        def close(self): pass\n",
    "    print(\"â„¹ï¸  tqdm not available â€” plain progress output will be used\")\n",
    "\n",
    "print(f\"\\nâœ… Python {sys.version.split()[0]}\")\n",
    "print(f\"âœ… pandas  {pd.__version__}\")\n",
    "print(f\"âœ… numpy   {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f326b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸ“ PATH CONFIGURATION\n",
      "=================================================================\n",
      "  âœ…  TON-IoT data        c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\ton_iot\n",
      "  âœ…  IoT-23 data         c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\iot_23\n",
      "  âœ…  Bot-IoT data        c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\bot_iot\n",
      "  âœ…  Unified out         c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\unified\n",
      "  âœ…  Artifacts           c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\artifacts\n",
      "\n",
      "  TON-IoT CSV files  : 23\n",
      "  IoT-23 log files   : 23\n",
      "  Bot-IoT CSV files  : 4\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 2 | Paths + Processing Constants\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "NOTEBOOK_DIR  = Path.cwd()                                 # .../Phase_1/\n",
    "MAIN_DIR      = NOTEBOOK_DIR.parent                        # .../main_folder/\n",
    "ARTIFACTS_DIR = MAIN_DIR / \"artifacts\"\n",
    "DATA_DIR      = MAIN_DIR / \"data\"\n",
    "UNIFIED_DIR   = DATA_DIR / \"unified\"\n",
    "\n",
    "# Source data directories\n",
    "TONIOT_DATA_DIR = DATA_DIR / \"ton_iot\"\n",
    "IOT23_DATA_DIR  = DATA_DIR / \"iot_23\"\n",
    "BOTIOT_DATA_DIR = DATA_DIR / \"bot_iot\"\n",
    "\n",
    "# Output Parquet files (one per source dataset, appended row-group by row-group)\n",
    "TONIOT_PARQUET  = UNIFIED_DIR / \"toniot_aligned.parquet\"\n",
    "IOT23_PARQUET   = UNIFIED_DIR / \"iot23_aligned.parquet\"\n",
    "BOTIOT_PARQUET  = UNIFIED_DIR / \"botiot_aligned.parquet\"\n",
    "\n",
    "# Chunked processing â€” rows per pd.read_csv chunk\n",
    "# 100k rows Ã— 48 cols Ã— 8 bytes â‰ˆ 37 MB RAM per chunk (safe for any machine)\n",
    "CHUNK_SIZE = 100_000\n",
    "\n",
    "# Create unified output directory\n",
    "UNIFIED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Verify source data exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ“ PATH CONFIGURATION\")\n",
    "print(\"=\" * 65)\n",
    "for label, path in [\n",
    "    (\"TON-IoT data\", TONIOT_DATA_DIR),\n",
    "    (\"IoT-23 data\",  IOT23_DATA_DIR),\n",
    "    (\"Bot-IoT data\", BOTIOT_DATA_DIR),\n",
    "    (\"Unified out\",  UNIFIED_DIR),\n",
    "    (\"Artifacts\",    ARTIFACTS_DIR),\n",
    "]:\n",
    "    status = \"âœ…\" if path.exists() else \"âŒ MISSING\"\n",
    "    print(f\"  {status}  {label:<18s}  {path}\")\n",
    "\n",
    "toniot_files = sorted(TONIOT_DATA_DIR.glob(\"Network_dataset_*.csv\"))\n",
    "iot23_files  = sorted(IOT23_DATA_DIR.rglob(\"conn.log.labeled\"))\n",
    "botiot_files = sorted(BOTIOT_DATA_DIR.glob(\"UNSW_2018_IoT_Botnet_Full5pc_*.csv\"))\n",
    "\n",
    "print(f\"\\n  TON-IoT CSV files  : {len(toniot_files)}\")\n",
    "print(f\"  IoT-23 log files   : {len(iot23_files)}\")\n",
    "print(f\"  Bot-IoT CSV files  : {len(botiot_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e4e045",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 â€” Schema Registry\n",
    "\n",
    "Define all column mappings (Group A and Group B) in one authoritative location.\n",
    "Every subsequent transformation function references these registries â€” no magic strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4b182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Group A columns    : 9 universal features per dataset\n",
      "âœ… Total output cols  : 48\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Final column manifest:\n",
      "   1. dataset_source                                      [Group A]\n",
      "   2. univ_duration                                       [Group A]\n",
      "   3. univ_src_bytes                                      [Group A]\n",
      "   4. univ_dst_bytes                                      [Group A]\n",
      "   5. univ_src_pkts                                       [Group A]\n",
      "   6. univ_dst_pkts                                       [Group A]\n",
      "   7. univ_proto                                          [Group A]\n",
      "   8. univ_state                                          [Group A]\n",
      "   9. univ_src_port                                       [Group A]\n",
      "  10. univ_dst_port                                       [Group A]\n",
      "  11. univ_has_src_port                                   [Group A]\n",
      "  12. univ_has_dst_port                                   [Group A]\n",
      "  13. univ_label_binary                                   [Group A]\n",
      "  14. univ_label_multiclass                               [Group A]\n",
      "  15. univ_label_str                                      [Group A]\n",
      "  16. zeek_service                                        [Group B / Zeek]\n",
      "  17. zeek_missed_bytes                                   [Group B / Zeek]\n",
      "  18. zeek_history                                        [Group B / Zeek]\n",
      "  19. zeek_src_ip_bytes                                   [Group B / Zeek]\n",
      "  20. zeek_dst_ip_bytes                                   [Group B / Zeek]\n",
      "  21. toniot_dns_qclass                                   [Group B / TON-IoT]\n",
      "  22. toniot_dns_qtype                                    [Group B / TON-IoT]\n",
      "  23. toniot_dns_rcode                                    [Group B / TON-IoT]\n",
      "  24. toniot_http_request_body_len                        [Group B / TON-IoT]\n",
      "  25. toniot_http_response_body_len                       [Group B / TON-IoT]\n",
      "  26. toniot_http_status_code                             [Group B / TON-IoT]\n",
      "  27. botiot_mean                                         [Group B / Bot-IoT]\n",
      "  28. botiot_stddev                                       [Group B / Bot-IoT]\n",
      "  29. botiot_sum                                          [Group B / Bot-IoT]\n",
      "  30. botiot_min                                          [Group B / Bot-IoT]\n",
      "  31. botiot_max                                          [Group B / Bot-IoT]\n",
      "  32. botiot_rate                                         [Group B / Bot-IoT]\n",
      "  33. botiot_srate                                        [Group B / Bot-IoT]\n",
      "  34. botiot_drate                                        [Group B / Bot-IoT]\n",
      "  35. botiot_TnBPSrcIP                                    [Group B / Bot-IoT]\n",
      "  36. botiot_TnBPDstIP                                    [Group B / Bot-IoT]\n",
      "  37. botiot_TnP_PSrcIP                                   [Group B / Bot-IoT]\n",
      "  38. botiot_TnP_PDstIP                                   [Group B / Bot-IoT]\n",
      "  39. botiot_TnP_PerProto                                 [Group B / Bot-IoT]\n",
      "  40. botiot_TnP_Per_Dport                                [Group B / Bot-IoT]\n",
      "  41. botiot_AR_P_Proto_P_SrcIP                           [Group B / Bot-IoT]\n",
      "  42. botiot_AR_P_Proto_P_DstIP                           [Group B / Bot-IoT]\n",
      "  43. botiot_N_IN_Conn_P_DstIP                            [Group B / Bot-IoT]\n",
      "  44. botiot_N_IN_Conn_P_SrcIP                            [Group B / Bot-IoT]\n",
      "  45. botiot_AR_P_Proto_P_Sport                           [Group B / Bot-IoT]\n",
      "  46. botiot_AR_P_Proto_P_Dport                           [Group B / Bot-IoT]\n",
      "  47. botiot_Pkts_P_State_P_Protocol_P_DestIP             [Group B / Bot-IoT]\n",
      "  48. botiot_Pkts_P_State_P_Protocol_P_SrcIP              [Group B / Bot-IoT]\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 3 | Group A Registry â€” Universal Core Column Mappings\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Each dict maps:   source_column_name  â†’  univ_column_name\n",
    "# These are the columns that exist (under different names) in ALL three datasets.\n",
    "# After renaming, every output row has these columns fully populated from real data.\n",
    "\n",
    "GROUP_A_TONIOT = {\n",
    "    \"duration\":    \"univ_duration\",    # float64 (seconds)\n",
    "    \"src_bytes\":   \"univ_src_bytes\",   # int64  (note: object dtype in source â†’ needs cast)\n",
    "    \"dst_bytes\":   \"univ_dst_bytes\",   # int64\n",
    "    \"src_pkts\":    \"univ_src_pkts\",    # int64\n",
    "    \"dst_pkts\":    \"univ_dst_pkts\",    # int64\n",
    "    \"proto\":       \"univ_proto\",       # string â†’ lowercase\n",
    "    \"conn_state\":  \"univ_state\",       # string â†’ mapped to univ_state vocab\n",
    "    \"src_port\":    \"univ_src_port\",    # int64  (TON-IoT ports are never -1)\n",
    "    \"dst_port\":    \"univ_dst_port\",    # int64\n",
    "}\n",
    "\n",
    "GROUP_A_IOT23 = {\n",
    "    \"duration\":     \"univ_duration\",   # object dtype â†’ cast float64 ('-' â†’ 0.0)\n",
    "    \"orig_bytes\":   \"univ_src_bytes\",  # object dtype â†’ cast int64   ('-' â†’ 0)\n",
    "    \"resp_bytes\":   \"univ_dst_bytes\",  # object dtype â†’ cast int64   ('-' â†’ 0)\n",
    "    \"orig_pkts\":    \"univ_src_pkts\",   # float64 â†’ int64\n",
    "    \"resp_pkts\":    \"univ_dst_pkts\",   # float64 â†’ int64\n",
    "    \"proto\":        \"univ_proto\",      # string â†’ lowercase\n",
    "    \"conn_state\":   \"univ_state\",      # string â†’ mapped to univ_state vocab\n",
    "    \"id.orig_p\":    \"univ_src_port\",   # float64 â†’ int64  ('-' â†’ 0)\n",
    "    \"id.resp_p\":    \"univ_dst_port\",   # float64 â†’ int64\n",
    "}\n",
    "\n",
    "GROUP_A_BOTIOT = {\n",
    "    \"dur\":    \"univ_duration\",    # float64\n",
    "    \"sbytes\": \"univ_src_bytes\",   # int64\n",
    "    \"dbytes\": \"univ_dst_bytes\",   # int64\n",
    "    \"spkts\":  \"univ_src_pkts\",    # int64\n",
    "    \"dpkts\":  \"univ_dst_pkts\",    # int64\n",
    "    \"proto\":  \"univ_proto\",       # string â†’ lowercase\n",
    "    \"state\":  \"univ_state\",       # Argus codes â†’ univ_state vocab\n",
    "    \"sport\":  \"univ_src_port\",    # int64  (-1 for ARP/non-port â†’ 0 + has_port flag)\n",
    "    \"dport\":  \"univ_dst_port\",    # int64  (-1 for ARP/non-port â†’ 0 + has_port flag)\n",
    "}\n",
    "\n",
    "# â”€â”€ Final ordered column list for all output Parquet files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Defined here so every dataset writer uses the exact same schema.\n",
    "FINAL_COLUMNS = [\n",
    "    # â”€â”€ Group A: universal core â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"dataset_source\",\n",
    "    \"univ_duration\",\n",
    "    \"univ_src_bytes\",\n",
    "    \"univ_dst_bytes\",\n",
    "    \"univ_src_pkts\",\n",
    "    \"univ_dst_pkts\",\n",
    "    \"univ_proto\",\n",
    "    \"univ_state\",\n",
    "    \"univ_src_port\",\n",
    "    \"univ_dst_port\",\n",
    "    \"univ_has_src_port\",        # int8 boolean (1=port present, 0=no port)\n",
    "    \"univ_has_dst_port\",        # int8 boolean\n",
    "    # â”€â”€ Labels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"univ_label_binary\",        # int8:  0=Normal, 1=Attack\n",
    "    \"univ_label_multiclass\",    # int8:  0-4 (5-class taxonomy)\n",
    "    \"univ_label_str\",           # string: human-readable class name\n",
    "    # â”€â”€ Group B: Zeek shared (TON-IoT + IoT-23; sentinel=-1/\"unknown\" for Bot-IoT)\n",
    "    \"zeek_service\",             # string   sentinel=\"<absent>\" (Bot-IoT); \"unknown\" = Zeek observed but unidentified\n",
    "    \"zeek_missed_bytes\",        # int64    sentinel=-1\n",
    "    \"zeek_history\",             # string   sentinel=\"<absent>\" (TON-IoT + Bot-IoT); IoT-23 has real history strings\n",
    "    \"zeek_src_ip_bytes\",        # int64    sentinel=-1\n",
    "    \"zeek_dst_ip_bytes\",        # int64    sentinel=-1\n",
    "    # â”€â”€ Group B: TON-IoT only (sentinel=-1 for IoT-23 + Bot-IoT) â”€â”€â”€â”€â”€â”€\n",
    "    \"toniot_dns_qclass\",        # int64    sentinel=-1\n",
    "    \"toniot_dns_qtype\",         # int64    sentinel=-1\n",
    "    \"toniot_dns_rcode\",         # int64    sentinel=-1\n",
    "    \"toniot_http_request_body_len\",    # int64    sentinel=-1\n",
    "    \"toniot_http_response_body_len\",   # int64    sentinel=-1\n",
    "    \"toniot_http_status_code\",         # int64    sentinel=-1\n",
    "    # â”€â”€ Group B: Bot-IoT behavioral windows (sentinel=-1.0/-1 for others)\n",
    "    \"botiot_mean\",              # float64  sentinel=-1.0\n",
    "    \"botiot_stddev\",            # float64  sentinel=-1.0\n",
    "    \"botiot_sum\",               # float64  sentinel=-1.0\n",
    "    \"botiot_min\",               # float64  sentinel=-1.0\n",
    "    \"botiot_max\",               # float64  sentinel=-1.0\n",
    "    \"botiot_rate\",              # float64  sentinel=-1.0\n",
    "    \"botiot_srate\",             # float64  sentinel=-1.0\n",
    "    \"botiot_drate\",             # float64  sentinel=-1.0\n",
    "    \"botiot_TnBPSrcIP\",                        # int64    sentinel=-1\n",
    "    \"botiot_TnBPDstIP\",                        # int64    sentinel=-1\n",
    "    \"botiot_TnP_PSrcIP\",                       # int64    sentinel=-1\n",
    "    \"botiot_TnP_PDstIP\",                       # int64    sentinel=-1\n",
    "    \"botiot_TnP_PerProto\",                     # int64    sentinel=-1\n",
    "    \"botiot_TnP_Per_Dport\",                    # int64    sentinel=-1\n",
    "    \"botiot_AR_P_Proto_P_SrcIP\",               # float64  sentinel=-1.0\n",
    "    \"botiot_AR_P_Proto_P_DstIP\",               # float64  sentinel=-1.0\n",
    "    \"botiot_N_IN_Conn_P_DstIP\",                # int64    sentinel=-1\n",
    "    \"botiot_N_IN_Conn_P_SrcIP\",                # int64    sentinel=-1\n",
    "    \"botiot_AR_P_Proto_P_Sport\",               # float64  sentinel=-1.0\n",
    "    \"botiot_AR_P_Proto_P_Dport\",               # float64  sentinel=-1.0\n",
    "    \"botiot_Pkts_P_State_P_Protocol_P_DestIP\", # int64    sentinel=-1\n",
    "    \"botiot_Pkts_P_State_P_Protocol_P_SrcIP\",  # int64    sentinel=-1\n",
    "]\n",
    "\n",
    "print(f\"âœ… Group A columns    : {len(GROUP_A_TONIOT)} universal features per dataset\")\n",
    "print(f\"âœ… Total output cols  : {len(FINAL_COLUMNS)}\")\n",
    "print(f\"\\n{'â”€'*65}\")\n",
    "print(\"Final column manifest:\")\n",
    "for i, c in enumerate(FINAL_COLUMNS, 1):\n",
    "    section = (\"Group A\"    if c.startswith(\"univ_\") or c == \"dataset_source\"\n",
    "               else \"Labels\" if \"label\" in c\n",
    "               else \"Group B / Zeek\"   if c.startswith(\"zeek_\")\n",
    "               else \"Group B / TON-IoT\" if c.startswith(\"toniot_\")\n",
    "               else \"Group B / Bot-IoT\")\n",
    "    print(f\"  {i:>2}. {c:<50s}  [{section}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf009877",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 â€” Transformation Dictionaries\n",
    "\n",
    "State mapping, protocol normalization, and 5-class label taxonomy.\n",
    "These are closed, authoritative lookup tables â€” not derived at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b3192d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… UNIVERSAL_STATE_MAP   : 24 Zeek/Argus codes â†’ 5 vocab tokens\n",
      "âœ… TONIOT_LABEL_MAP      : 9 type strings â†’ 5 classes\n",
      "âœ… IOT23_LABEL_MAP       : 18 detailed-label strings â†’ 5 classes\n",
      "âœ… BOTIOT_CATEGORY_MAP   : 5 category strings â†’ 5 classes\n",
      "\n",
      "ğŸ“‹ 5-Class Taxonomy:\n",
      "   Class 0 â†’ Normal\n",
      "   Class 1 â†’ Reconnaissance\n",
      "   Class 2 â†’ Volumetric_Attack\n",
      "   Class 3 â†’ C2_Botnet\n",
      "   Class 4 â†’ Exploit_and_Theft\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 4 | Transformation Maps\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# â”€â”€ 4A: Universal State Vocabulary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Zeek (conn_state) and Argus (state) codes â†’ 5-token univ_state vocab.\n",
    "# \"unknown\" catches '-', '?', or any unseen code.\n",
    "UNIVERSAL_STATE_MAP = {\n",
    "    # â”€â”€ attempt: SYN sent / request initiated, no complete handshake â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"S0\":      \"attempt\",      # Zeek: SYN sent, no response at all\n",
    "    \"S1\":      \"attempt\",      # Zeek: SYN+SYN-ACK seen, no final ACK\n",
    "    \"OTH\":     \"attempt\",      # Zeek: no SYN seen (mid-stream capture)\n",
    "    \"REQ\":     \"attempt\",      # Argus: request sent\n",
    "    # â”€â”€ established: connection completed normally â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"SF\":      \"established\",  # Zeek: normal close (FIN both sides)\n",
    "    \"S2\":      \"established\",  # Zeek: connection closing\n",
    "    \"S3\":      \"established\",  # Zeek: connection closing\n",
    "    \"CON\":     \"established\",  # Argus: connection established\n",
    "    \"FIN\":     \"established\",  # Argus: clean finish\n",
    "    # â”€â”€ rejected: RST or explicit denial â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"REJ\":     \"rejected\",     # Zeek: RST in response to SYN\n",
    "    \"RSTO\":    \"rejected\",     # Zeek: RST from originator\n",
    "    \"RSTOS0\":  \"rejected\",     # Zeek: RST+SYN, no SYN-ACK\n",
    "    \"RSTR\":    \"rejected\",     # Zeek: RST from responder\n",
    "    \"RSTRH\":   \"rejected\",     # Zeek: RST from responder, half-open\n",
    "    \"RST\":     \"rejected\",     # Argus: reset\n",
    "    # â”€â”€ other: partial/ambiguous â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"SHR\":     \"other\",        # Zeek: SYN+RST (simultaneous)\n",
    "    \"SH\":      \"other\",        # Zeek: SYN-ACK with no SYN\n",
    "    \"INT\":     \"other\",        # Argus: partial/internal flow\n",
    "    \"URN\":     \"other\",        # Argus: unknown\n",
    "    \"ECO\":     \"other\",        # Argus: ICMP echo\n",
    "    \"ECR\":     \"other\",        # Argus: ICMP echo reply\n",
    "    # â”€â”€ unknown: missing / not applicable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"-\":       \"unknown\",\n",
    "    \"?\":       \"unknown\",\n",
    "    \"(empty)\": \"unknown\",\n",
    "}\n",
    "\n",
    "VALID_STATE_VOCAB = {\"attempt\", \"established\", \"rejected\", \"other\", \"unknown\"}\n",
    "\n",
    "# â”€â”€ 4B: Protocol Normalization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Keep base set; map esoteric or numeric variants to 'other'\n",
    "VALID_PROTO_VOCAB = {\"tcp\", \"udp\", \"icmp\", \"arp\", \"other\"}\n",
    "\n",
    "# â”€â”€ 4C: 5-Class Label Taxonomy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# Class 0 â€” Normal      (benign baseline)\n",
    "# Class 1 â€” Recon       (port scans, fingerprinting, vulnerability sweeps)\n",
    "# Class 2 â€” Volumetric  (DoS, DDoS, flooding)\n",
    "# Class 3 â€” C2_Botnet   (command & control, heartbeats, Mirai, Torii)\n",
    "# Class 4 â€” Exploit     (payload injection, exfiltration, keylogging, ransomware)\n",
    "\n",
    "LABEL_CLASS_NAMES = {\n",
    "    0: \"Normal\",\n",
    "    1: \"Reconnaissance\",\n",
    "    2: \"Volumetric_Attack\",\n",
    "    3: \"C2_Botnet\",\n",
    "    4: \"Exploit_and_Theft\",\n",
    "}\n",
    "\n",
    "# TON-IoT: label source = 'type' column (string)\n",
    "TONIOT_LABEL_MAP = {\n",
    "    # Class 0 â€” Normal\n",
    "    \"normal\":    0,\n",
    "    # Class 1 â€” Reconnaissance\n",
    "    \"scanning\":  1,\n",
    "    # Class 2 â€” Volumetric\n",
    "    \"ddos\":      2,\n",
    "    \"dos\":       2,\n",
    "    # Class 4 â€” Exploit & Theft\n",
    "    \"backdoor\":  4,\n",
    "    \"injection\": 4,\n",
    "    \"password\":  4,\n",
    "    \"ransomware\":4,\n",
    "    \"xss\":       4,\n",
    "}\n",
    "\n",
    "# IoT-23: label source = 'detailed-label' column (from compound column split)\n",
    "IOT23_LABEL_MAP = {\n",
    "    # Class 0 â€” Normal\n",
    "    \"-\":                                    0,   # benign rows (detailed-label = '-')\n",
    "    \"benign\":                               0,\n",
    "    # Class 1 â€” Reconnaissance\n",
    "    \"PartOfAHorizontalPortScan\":            1,   # ~65.7% of all IoT-23 rows\n",
    "    \"PartOfAHorizontalPortScan-Attack\":     1,\n",
    "    # Class 2 â€” Volumetric\n",
    "    \"DDoS\":                                 2,\n",
    "    \"Attack\":                               2,   # generic attack label (mapped as volumetric)\n",
    "    # Class 3 â€” C2_Botnet\n",
    "    \"Okiru\":                                3,\n",
    "    \"Okiru-Attack\":                         3,\n",
    "    \"C&C\":                                  3,\n",
    "    \"C&C-HeartBeat\":                        3,\n",
    "    \"C&C-HeartBeat-Attack\":                 3,\n",
    "    \"C&C-HeartBeat-FileDownload\":           3,\n",
    "    \"C&C-FileDownload\":                     3,\n",
    "    \"C&C-Mirai\":                            3,\n",
    "    \"C&C-Torii\":                            3,\n",
    "    \"C&C-PartOfAHorizontalPortScan\":        3,\n",
    "    \"Torii\":                                3,\n",
    "    # Class 4 â€” Exploit & Theft\n",
    "    \"FileDownload\":                         4,\n",
    "}\n",
    "\n",
    "# Bot-IoT: label source = 'category' + 'subcategory' columns\n",
    "# Primary mapping on 'category'; subcategory used for Theft disambiguation\n",
    "BOTIOT_CATEGORY_MAP = {\n",
    "    # Class 0 â€” Normal\n",
    "    \"Normal\":          0,\n",
    "    # Class 2 â€” Volumetric\n",
    "    \"DoS\":             2,\n",
    "    \"DDoS\":            2,\n",
    "    # Class 1 â€” Reconnaissance\n",
    "    \"Reconnaissance\":  1,\n",
    "    # Class 4 â€” Exploit & Theft\n",
    "    \"Theft\":           4,\n",
    "}\n",
    "\n",
    "print(\"âœ… UNIVERSAL_STATE_MAP   : {} Zeek/Argus codes â†’ {} vocab tokens\".format(\n",
    "    len(UNIVERSAL_STATE_MAP), len(VALID_STATE_VOCAB)))\n",
    "print(\"âœ… TONIOT_LABEL_MAP      : {} type strings â†’ 5 classes\".format(len(TONIOT_LABEL_MAP)))\n",
    "print(\"âœ… IOT23_LABEL_MAP       : {} detailed-label strings â†’ 5 classes\".format(len(IOT23_LABEL_MAP)))\n",
    "print(\"âœ… BOTIOT_CATEGORY_MAP   : {} category strings â†’ 5 classes\".format(len(BOTIOT_CATEGORY_MAP)))\n",
    "print(\"\\nğŸ“‹ 5-Class Taxonomy:\")\n",
    "for cls, name in LABEL_CLASS_NAMES.items():\n",
    "    print(f\"   Class {cls} â†’ {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acc49f",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 â€” Core Transformation Engine\n",
    "\n",
    "Pure functions with no side effects. Each function receives a chunk and returns a\n",
    "transformed chunk. Dataset-specific `build_*` functions compose these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "414652f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Utility functions defined:\n",
      "   safe_int(), safe_float(), normalize_proto(), map_state()\n",
      "   handle_port()  â† hex-aware (handles 0x0303 style Bot-IoT ports)\n",
      "   map_label_multiclass()\n",
      "   inject_group_b_sentinels(), reorder_and_validate()\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 5 | Core Utility Functions\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def safe_int(series: pd.Series, sentinel_str: str = \"-\", fill: int = 0) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Safely cast a series that may contain Zeek/Argus sentinel strings to int64.\n",
    "    Steps: replace sentinel strings â†’ coerce non-numerics to NaN â†’ fill NaN â†’ cast.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        pd.to_numeric(series.replace(sentinel_str, str(fill)), errors=\"coerce\")\n",
    "        .fillna(fill)\n",
    "        .astype(\"int64\")\n",
    "    )\n",
    "\n",
    "\n",
    "def safe_float(series: pd.Series, sentinel_str: str = \"-\", fill: float = 0.0) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Safely cast a series that may contain Zeek sentinel strings to float64.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        pd.to_numeric(series.replace(sentinel_str, str(fill)), errors=\"coerce\")\n",
    "        .fillna(fill)\n",
    "        .astype(\"float64\")\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_proto(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Lowercase all protocol strings; map anything outside base vocab to 'other'.\n",
    "    Base vocab: tcp, udp, icmp, arp, other.\n",
    "    \"\"\"\n",
    "    lowered = series.str.lower().str.strip()\n",
    "    return lowered.where(lowered.isin(VALID_PROTO_VOCAB), other=\"other\")\n",
    "\n",
    "\n",
    "def map_state(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map Zeek conn_state or Argus state codes to the 5-token univ_state vocab.\n",
    "    Any code not in UNIVERSAL_STATE_MAP maps to 'unknown'.\n",
    "    \"\"\"\n",
    "    return series.str.strip().map(UNIVERSAL_STATE_MAP).fillna(\"unknown\")\n",
    "\n",
    "\n",
    "def handle_port(series: pd.Series, sentinel_value: int = -1):\n",
    "    \"\"\"\n",
    "    Handle port series that may contain integers, floats, hex strings (e.g. '0x0303'),\n",
    "    or the sentinel_value (-1 for Bot-IoT ARP rows).\n",
    "\n",
    "    Steps:\n",
    "      1. pd.to_numeric  â€“ fast path for numeric / float / decimal-string types\n",
    "      2. Vectorised hex parse for any remaining '0x...' strings\n",
    "      3. NaN  â†’ 0  (unknown/unparseable port treated as 0)\n",
    "      4. Derive has_port flag  (1 = real port present, 0 = was sentinel/0)\n",
    "      5. Replace sentinel_value with 0\n",
    "\n",
    "    Returns: (port_int64_series, has_port_int8_series)\n",
    "    \"\"\"\n",
    "    # Step 1: fast numeric coerce â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    numeric = pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "    # Step 2: parse hex strings that pd.to_numeric skips â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    str_s = series.astype(str).str.strip().str.lower()\n",
    "    hex_mask = str_s.str.startswith(\"0x\")\n",
    "    if hex_mask.any():\n",
    "        numeric = numeric.copy()\n",
    "        numeric.loc[hex_mask] = str_s.loc[hex_mask].apply(\n",
    "            lambda x: int(x, 16) if x not in (\"nan\", \"none\", \"\") else np.nan\n",
    "        )\n",
    "\n",
    "    # Step 3: NaN â†’ 0 then cast â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    numeric = numeric.fillna(0).astype(\"int64\")\n",
    "\n",
    "    # Steps 4 & 5: sentinel logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    has_port = (numeric != sentinel_value).astype(\"int8\")\n",
    "    port_val  = numeric.where(numeric != sentinel_value, other=0)\n",
    "    return port_val, has_port\n",
    "\n",
    "\n",
    "def map_label_multiclass(series: pd.Series, label_map: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map a label string series to 0-4 integer multiclass.\n",
    "    Strips whitespace and lowercases for TON-IoT (already lowercase).\n",
    "    Unmapped strings â†’ -1 (to catch at validation time).\n",
    "    \"\"\"\n",
    "    return series.str.strip().map(label_map).fillna(-1).astype(\"int8\")\n",
    "\n",
    "\n",
    "def inject_group_b_sentinels(df: pd.DataFrame, dataset: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each Group B column NOT produced by the current dataset, inject:\n",
    "      - Categorical columns  â†’ \"unknown\"\n",
    "      - Float64 columns      â†’ -1.0\n",
    "      - Int64 columns        â†’ -1\n",
    "\n",
    "    dataset: one of 'toniot', 'iot23', 'botiot'\n",
    "    \"\"\"\n",
    "    # â”€â”€ Columns absent in each dataset (need sentinel injection) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    BOT_IOT_FLOAT_COLS = [\n",
    "        \"botiot_mean\", \"botiot_stddev\", \"botiot_sum\",\n",
    "        \"botiot_min\",  \"botiot_max\",\n",
    "        \"botiot_rate\", \"botiot_srate\", \"botiot_drate\",\n",
    "        \"botiot_AR_P_Proto_P_SrcIP\", \"botiot_AR_P_Proto_P_DstIP\",\n",
    "        \"botiot_AR_P_Proto_P_Sport\", \"botiot_AR_P_Proto_P_Dport\",\n",
    "    ]\n",
    "    BOT_IOT_INT_COLS = [\n",
    "        \"botiot_TnBPSrcIP\",  \"botiot_TnBPDstIP\",\n",
    "        \"botiot_TnP_PSrcIP\", \"botiot_TnP_PDstIP\",\n",
    "        \"botiot_TnP_PerProto\", \"botiot_TnP_Per_Dport\",\n",
    "        \"botiot_N_IN_Conn_P_DstIP\", \"botiot_N_IN_Conn_P_SrcIP\",\n",
    "        \"botiot_Pkts_P_State_P_Protocol_P_DestIP\",\n",
    "        \"botiot_Pkts_P_State_P_Protocol_P_SrcIP\",\n",
    "    ]\n",
    "    TONIOT_INT_COLS = [\n",
    "        \"toniot_dns_qclass\", \"toniot_dns_qtype\", \"toniot_dns_rcode\",\n",
    "        \"toniot_http_request_body_len\", \"toniot_http_response_body_len\",\n",
    "        \"toniot_http_status_code\",\n",
    "    ]\n",
    "    ZEEK_INT_COLS  = [\"zeek_missed_bytes\", \"zeek_src_ip_bytes\", \"zeek_dst_ip_bytes\"]\n",
    "\n",
    "    n = len(df)\n",
    "\n",
    "    # Absent-feature sentinel is \"<absent>\" (not \"unknown\").\n",
    "    # \"unknown\" is a *valid Zeek observation* (service Zeek couldn't identify).\n",
    "    # \"<absent>\" unambiguously means: this column was never collected by this\n",
    "    # dataset. Keeping both distinct lets the model and downstream code tell\n",
    "    # the difference between \"feature present but unknown\" vs \"feature absent\".\n",
    "    ABSENT = \"<absent>\"\n",
    "\n",
    "    if dataset == \"toniot\":\n",
    "        for c in BOT_IOT_FLOAT_COLS: df[c] = np.full(n, -1.0, dtype=\"float64\")\n",
    "        for c in BOT_IOT_INT_COLS:   df[c] = np.full(n, -1,   dtype=\"int64\")\n",
    "        df[\"zeek_history\"] = ABSENT   # TON-IoT has no history column\n",
    "\n",
    "    elif dataset == \"iot23\":\n",
    "        for c in BOT_IOT_FLOAT_COLS: df[c] = np.full(n, -1.0, dtype=\"float64\")\n",
    "        for c in BOT_IOT_INT_COLS:   df[c] = np.full(n, -1,   dtype=\"int64\")\n",
    "        for c in TONIOT_INT_COLS:    df[c] = np.full(n, -1,   dtype=\"int64\")\n",
    "\n",
    "    elif dataset == \"botiot\":\n",
    "        df[\"zeek_service\"]    = ABSENT   # Bot-IoT has no Zeek service detection\n",
    "        df[\"zeek_history\"]    = ABSENT   # Bot-IoT has no Zeek history string\n",
    "        for c in ZEEK_INT_COLS:   df[c] = np.full(n, -1, dtype=\"int64\")\n",
    "        for c in TONIOT_INT_COLS: df[c] = np.full(n, -1, dtype=\"int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def reorder_and_validate(df: pd.DataFrame, dataset: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure output DataFrame has all FINAL_COLUMNS in the canonical order.\n",
    "    Missing columns raise ValueError to catch bugs early.\n",
    "    \"\"\"\n",
    "    missing = [c for c in FINAL_COLUMNS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"[{dataset}] Missing output columns before reorder: {missing}\"\n",
    "        )\n",
    "    return df[FINAL_COLUMNS].copy()\n",
    "\n",
    "\n",
    "print(\"âœ… Utility functions defined:\")\n",
    "print(\"   safe_int(), safe_float(), normalize_proto(), map_state()\")\n",
    "print(\"   handle_port()  â† hex-aware (handles 0x0303 style Bot-IoT ports)\")\n",
    "print(\"   map_label_multiclass()\")\n",
    "print(\"   inject_group_b_sentinels(), reorder_and_validate()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca69ca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… transform_toniot_chunk() defined\n",
      "âœ… transform_iot23_chunk() defined\n",
      "âœ… transform_botiot_chunk() defined\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 6 | Dataset-Specific Chunk Transformers\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# â”€â”€ TON-IoT transformer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def transform_toniot_chunk(chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform one chunk of TON-IoT raw data into the universal aligned schema.\n",
    "\n",
    "    TON-IoT quirks handled:\n",
    "    - src_bytes is dtype=object (string) â€” some rows have non-numeric values\n",
    "    - Label source: 'type' (multiclass string) and 'label' (binary int)\n",
    "    - No port sentinels â€” ports are always valid integers\n",
    "    - No zeek_history (Zeek conn.log doesn't have history in this dataset)\n",
    "    - uid column is sparsely present (only network_dataset_6); if absent, ignore\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame()\n",
    "\n",
    "    # â”€â”€ Group A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out[\"dataset_source\"]  = \"toniot\"\n",
    "    out[\"univ_duration\"]   = safe_float(chunk[\"duration\"])\n",
    "    out[\"univ_src_bytes\"]  = safe_int(chunk[\"src_bytes\"])\n",
    "    out[\"univ_dst_bytes\"]  = safe_int(chunk[\"dst_bytes\"].astype(str))\n",
    "    out[\"univ_src_pkts\"]   = chunk[\"src_pkts\"].astype(\"int64\")\n",
    "    out[\"univ_dst_pkts\"]   = chunk[\"dst_pkts\"].astype(\"int64\")\n",
    "    out[\"univ_proto\"]      = normalize_proto(chunk[\"proto\"])\n",
    "    out[\"univ_state\"]      = map_state(chunk[\"conn_state\"])\n",
    "\n",
    "    # Ports: TON-IoT ports are always valid (no -1 sentinel)\n",
    "    out[\"univ_src_port\"]    = chunk[\"src_port\"].astype(\"int64\")\n",
    "    out[\"univ_dst_port\"]    = chunk[\"dst_port\"].astype(\"int64\")\n",
    "    out[\"univ_has_src_port\"] = np.ones(len(chunk), dtype=\"int8\")\n",
    "    out[\"univ_has_dst_port\"] = np.ones(len(chunk), dtype=\"int8\")\n",
    "\n",
    "    # â”€â”€ Labels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out[\"univ_label_binary\"]     = chunk[\"label\"].astype(\"int8\")\n",
    "    out[\"univ_label_multiclass\"] = map_label_multiclass(chunk[\"type\"], TONIOT_LABEL_MAP)\n",
    "    out[\"univ_label_str\"]        = out[\"univ_label_multiclass\"].map(LABEL_CLASS_NAMES).fillna(\"UNMAPPED\")\n",
    "\n",
    "    # â”€â”€ Group B: Zeek shared â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Replace Zeek '-' placeholder with 'unknown' for string; 0 for numerics\n",
    "    out[\"zeek_service\"]      = chunk[\"service\"].replace(\"-\", \"unknown\")\n",
    "    out[\"zeek_missed_bytes\"] = chunk[\"missed_bytes\"].astype(\"int64\")\n",
    "    # zeek_history: TON-IoT doesn't have a 'history' column â†’ absent sentinel\n",
    "    # Use \"<absent>\" not \"unknown\" â€” Zeek logs \"unknown\" for observable-but-\n",
    "    # unidentified sessions; \"<absent>\" means the column was never collected.\n",
    "    out[\"zeek_history\"]      = \"<absent>\"\n",
    "    out[\"zeek_src_ip_bytes\"] = chunk[\"src_ip_bytes\"].astype(\"int64\")\n",
    "    out[\"zeek_dst_ip_bytes\"] = chunk[\"dst_ip_bytes\"].astype(\"int64\")\n",
    "\n",
    "    # â”€â”€ Group B: TON-IoT only â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out[\"toniot_dns_qclass\"]             = chunk[\"dns_qclass\"].astype(\"int64\")\n",
    "    out[\"toniot_dns_qtype\"]              = chunk[\"dns_qtype\"].astype(\"int64\")\n",
    "    out[\"toniot_dns_rcode\"]              = chunk[\"dns_rcode\"].astype(\"int64\")\n",
    "    out[\"toniot_http_request_body_len\"]  = chunk[\"http_request_body_len\"].astype(\"int64\")\n",
    "    out[\"toniot_http_response_body_len\"] = chunk[\"http_response_body_len\"].astype(\"int64\")\n",
    "    out[\"toniot_http_status_code\"]       = chunk[\"http_status_code\"].astype(\"int64\")\n",
    "\n",
    "    # â”€â”€ Group B: Bot-IoT sentinels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out = inject_group_b_sentinels(out, \"toniot\")\n",
    "\n",
    "    return reorder_and_validate(out, \"toniot\")\n",
    "\n",
    "\n",
    "# â”€â”€ IoT-23 transformer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# IoT-23 compound column name and separator (from Phase 0 analysis)\n",
    "IOT23_COMPOUND_COL = \"tunnel_parents   label   detailed-label\"\n",
    "IOT23_COMPOUND_SEP = \"   \"   # 3 spaces â€” Zeek internal separator\n",
    "\n",
    "IOT23_COLS = [\n",
    "    \"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\",\n",
    "    \"proto\", \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\",\n",
    "    \"conn_state\", \"local_orig\", \"local_resp\", \"missed_bytes\", \"history\",\n",
    "    \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\",\n",
    "    IOT23_COMPOUND_COL,\n",
    "]\n",
    "\n",
    "def transform_iot23_chunk(chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform one chunk of IoT-23 raw data into the universal aligned schema.\n",
    "\n",
    "    IoT-23 quirks handled:\n",
    "    - Tab-separated Zeek log; 8 metadata header lines skipped at read time\n",
    "    - Compound last column: 'tunnel_parents   label   detailed-label'\n",
    "      split by 3 spaces â†’ [tunnel_parents, label, detailed-label]\n",
    "    - duration, orig_bytes, resp_bytes are dtype=object (Zeek '-' sentinel)\n",
    "    - id.orig_p and id.resp_p are float64\n",
    "    - Zeek '-' / '(empty)' mean 'not applicable'\n",
    "    - detailed-label is the source for 5-class taxonomy\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame()\n",
    "\n",
    "    # â”€â”€ Split IoT-23 compound label column â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    split = chunk[IOT23_COMPOUND_COL].fillna(\"-   -   -\").str.split(\n",
    "        IOT23_COMPOUND_SEP, expand=True, n=2\n",
    "    )\n",
    "    split.columns = [\"tunnel_parents_raw\", \"label_raw\", \"detailed_label_raw\"]\n",
    "\n",
    "    # â”€â”€ Group A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out[\"dataset_source\"]  = \"iot23\"\n",
    "    out[\"univ_duration\"]   = safe_float(chunk[\"duration\"])\n",
    "    out[\"univ_src_bytes\"]  = safe_int(chunk[\"orig_bytes\"])\n",
    "    out[\"univ_dst_bytes\"]  = safe_int(chunk[\"resp_bytes\"])\n",
    "    out[\"univ_src_pkts\"]   = safe_int(chunk[\"orig_pkts\"].astype(str))\n",
    "    out[\"univ_dst_pkts\"]   = safe_int(chunk[\"resp_pkts\"].astype(str))\n",
    "    out[\"univ_proto\"]      = normalize_proto(chunk[\"proto\"])\n",
    "    out[\"univ_state\"]      = map_state(chunk[\"conn_state\"])\n",
    "\n",
    "    # Ports â€” id.orig_p / id.resp_p are float64; '-' sentinel should not occur\n",
    "    # but coerce defensively\n",
    "    src_port_raw = safe_float(chunk[\"id.orig_p\"].astype(str)).astype(\"int64\")\n",
    "    dst_port_raw = safe_float(chunk[\"id.resp_p\"].astype(str)).astype(\"int64\")\n",
    "    out[\"univ_src_port\"],    out[\"univ_has_src_port\"] = handle_port(src_port_raw, sentinel_value=0)\n",
    "    out[\"univ_dst_port\"],    out[\"univ_has_dst_port\"] = handle_port(dst_port_raw, sentinel_value=0)\n",
    "\n",
    "    # â”€â”€ Labels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    label_col = split[\"label_raw\"].str.strip().str.lower()\n",
    "    out[\"univ_label_binary\"] = label_col.map(\n",
    "        {\"benign\": 0, \"malicious\": 1, \"-\": 0}\n",
    "    ).fillna(1).astype(\"int8\")\n",
    "\n",
    "    detailed_col = split[\"detailed_label_raw\"].str.strip()\n",
    "    out[\"univ_label_multiclass\"] = map_label_multiclass(detailed_col, IOT23_LABEL_MAP)\n",
    "    out[\"univ_label_str\"] = out[\"univ_label_multiclass\"].map(LABEL_CLASS_NAMES).fillna(\"UNMAPPED\")\n",
    "\n",
    "    # â”€â”€ Group B: Zeek shared â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out[\"zeek_service\"]      = chunk[\"service\"].replace(\"-\", \"unknown\")\n",
    "    out[\"zeek_missed_bytes\"] = safe_int(chunk[\"missed_bytes\"].astype(str))\n",
    "    out[\"zeek_history\"]      = chunk[\"history\"].replace(\"-\", \"unknown\").fillna(\"unknown\")\n",
    "    out[\"zeek_src_ip_bytes\"] = safe_int(chunk[\"orig_ip_bytes\"].astype(str))\n",
    "    out[\"zeek_dst_ip_bytes\"] = safe_int(chunk[\"resp_ip_bytes\"].astype(str))\n",
    "\n",
    "    # â”€â”€ Group B: TON-IoT and Bot-IoT sentinels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out = inject_group_b_sentinels(out, \"iot23\")\n",
    "\n",
    "    return reorder_and_validate(out, \"iot23\")\n",
    "\n",
    "\n",
    "# â”€â”€ Bot-IoT transformer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def transform_botiot_chunk(chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform one chunk of Bot-IoT raw data into the universal aligned schema.\n",
    "\n",
    "    Bot-IoT quirks handled:\n",
    "    - Argus format: sport/dport = -1 for non-port protocols (ARP) â†’ has_port flag\n",
    "    - state uses Argus codes (CON, RST, REQ, INT, FIN, URN)\n",
    "    - Label source: 'attack' (binary int), 'category' (multiclass string)\n",
    "    - 22 behavioral window features are Bot-IoT Group B columns\n",
    "    - No DNS/HTTP/SSL/Zeek-specific features â†’ all TON-IoT Group B = -1\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame()\n",
    "\n",
    "    # â”€â”€ Group A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out[\"dataset_source\"] = \"botiot\"\n",
    "    out[\"univ_duration\"]  = chunk[\"dur\"].astype(\"float64\")\n",
    "    out[\"univ_src_bytes\"] = chunk[\"sbytes\"].astype(\"int64\")\n",
    "    out[\"univ_dst_bytes\"] = chunk[\"dbytes\"].astype(\"int64\")\n",
    "    out[\"univ_src_pkts\"]  = chunk[\"spkts\"].astype(\"int64\")\n",
    "    out[\"univ_dst_pkts\"]  = chunk[\"dpkts\"].astype(\"int64\")\n",
    "    out[\"univ_proto\"]     = normalize_proto(chunk[\"proto\"])\n",
    "    out[\"univ_state\"]     = map_state(chunk[\"state\"])\n",
    "\n",
    "    # Ports â€” Bot-IoT uses -1 as sentinel for non-port protocols (ARP)\n",
    "    out[\"univ_src_port\"],    out[\"univ_has_src_port\"] = handle_port(chunk[\"sport\"], sentinel_value=-1)\n",
    "    out[\"univ_dst_port\"],    out[\"univ_has_dst_port\"] = handle_port(chunk[\"dport\"], sentinel_value=-1)\n",
    "\n",
    "    # â”€â”€ Labels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out[\"univ_label_binary\"]     = chunk[\"attack\"].astype(\"int8\")\n",
    "    out[\"univ_label_multiclass\"] = map_label_multiclass(chunk[\"category\"], BOTIOT_CATEGORY_MAP)\n",
    "    out[\"univ_label_str\"]        = out[\"univ_label_multiclass\"].map(LABEL_CLASS_NAMES).fillna(\"UNMAPPED\")\n",
    "\n",
    "    # â”€â”€ Group B: Bot-IoT behavioral windows â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out[\"botiot_mean\"]   = chunk[\"mean\"].astype(\"float64\")\n",
    "    out[\"botiot_stddev\"] = chunk[\"stddev\"].astype(\"float64\")\n",
    "    out[\"botiot_sum\"]    = chunk[\"sum\"].astype(\"float64\")\n",
    "    out[\"botiot_min\"]    = chunk[\"min\"].astype(\"float64\")\n",
    "    out[\"botiot_max\"]    = chunk[\"max\"].astype(\"float64\")\n",
    "    out[\"botiot_rate\"]   = chunk[\"rate\"].astype(\"float64\")\n",
    "    out[\"botiot_srate\"]  = chunk[\"srate\"].astype(\"float64\")\n",
    "    out[\"botiot_drate\"]  = chunk[\"drate\"].astype(\"float64\")\n",
    "    out[\"botiot_TnBPSrcIP\"]  = chunk[\"TnBPSrcIP\"].astype(\"int64\")\n",
    "    out[\"botiot_TnBPDstIP\"]  = chunk[\"TnBPDstIP\"].astype(\"int64\")\n",
    "    out[\"botiot_TnP_PSrcIP\"] = chunk[\"TnP_PSrcIP\"].astype(\"int64\")\n",
    "    out[\"botiot_TnP_PDstIP\"] = chunk[\"TnP_PDstIP\"].astype(\"int64\")\n",
    "    out[\"botiot_TnP_PerProto\"]  = chunk[\"TnP_PerProto\"].astype(\"int64\")\n",
    "    out[\"botiot_TnP_Per_Dport\"] = chunk[\"TnP_Per_Dport\"].astype(\"int64\")\n",
    "    out[\"botiot_AR_P_Proto_P_SrcIP\"]  = chunk[\"AR_P_Proto_P_SrcIP\"].astype(\"float64\")\n",
    "    out[\"botiot_AR_P_Proto_P_DstIP\"]  = chunk[\"AR_P_Proto_P_DstIP\"].astype(\"float64\")\n",
    "    out[\"botiot_N_IN_Conn_P_DstIP\"]   = chunk[\"N_IN_Conn_P_DstIP\"].astype(\"int64\")\n",
    "    out[\"botiot_N_IN_Conn_P_SrcIP\"]   = chunk[\"N_IN_Conn_P_SrcIP\"].astype(\"int64\")\n",
    "    out[\"botiot_AR_P_Proto_P_Sport\"]  = chunk[\"AR_P_Proto_P_Sport\"].astype(\"float64\")\n",
    "    out[\"botiot_AR_P_Proto_P_Dport\"]  = chunk[\"AR_P_Proto_P_Dport\"].astype(\"float64\")\n",
    "    out[\"botiot_Pkts_P_State_P_Protocol_P_DestIP\"] = chunk[\"Pkts_P_State_P_Protocol_P_DestIP\"].astype(\"int64\")\n",
    "    out[\"botiot_Pkts_P_State_P_Protocol_P_SrcIP\"]  = chunk[\"Pkts_P_State_P_Protocol_P_SrcIP\"].astype(\"int64\")\n",
    "\n",
    "    # â”€â”€ Group B: Zeek and TON-IoT sentinels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out = inject_group_b_sentinels(out, \"botiot\")\n",
    "\n",
    "    return reorder_and_validate(out, \"botiot\")\n",
    "\n",
    "\n",
    "print(\"âœ… transform_toniot_chunk() defined\")\n",
    "print(\"âœ… transform_iot23_chunk() defined\")\n",
    "print(\"âœ… transform_botiot_chunk() defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e438e639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… stream_to_parquet() helper defined\n",
      "   CHUNK_SIZE = 100,000 rows\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 7 | Parquet Writer Helper\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def stream_to_parquet(\n",
    "    files: list,\n",
    "    transform_fn,\n",
    "    output_path: Path,\n",
    "    dataset_label: str,\n",
    "    read_kwargs: dict = None,\n",
    "    use_chunks: bool = True,\n",
    "    force_rerun: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream-process all `files` via `transform_fn` and write to a single Parquet\n",
    "    file using PyArrow ParquetWriter (incremental row-group append).\n",
    "\n",
    "    Memory footprint at any moment = 1 chunk Ã— ~37 MB.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list of Path\n",
    "        Source CSV or TSV files to iterate over.\n",
    "    transform_fn : callable\n",
    "        Function that accepts a pandas DataFrame chunk and returns an aligned DataFrame.\n",
    "    output_path : Path\n",
    "        Destination Parquet file path.\n",
    "    dataset_label : str\n",
    "        Short label for progress messages ('TON-IoT', 'IoT-23', 'Bot-IoT').\n",
    "    read_kwargs : dict\n",
    "        Extra keyword arguments passed to pd.read_csv().\n",
    "    use_chunks : bool\n",
    "        If True, read each file in CHUNK_SIZE chunks. If False, read whole file\n",
    "        at once (useful for small files).\n",
    "    force_rerun : bool\n",
    "        If False (default), skip processing when a complete Parquet already exists\n",
    "        (idempotent / resume-safe). Pass True to delete and rebuild from scratch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys: total_rows, n_files, elapsed_sec, output_path\n",
    "    \"\"\"\n",
    "    if read_kwargs is None:\n",
    "        read_kwargs = {}\n",
    "\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # â”€â”€ Idempotency guard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # If the file already exists and force_rerun is False, skip reprocessing.\n",
    "    # A 45-min IoT-23 run that crashes at file 22/23 can be restarted without\n",
    "    # losing 95% of completed work.\n",
    "    # NOTE: wrap pq.ParquetFile in try/except â€” a corrupt file (no valid footer)\n",
    "    # raises ArrowInvalid; treat it like missing and delete + rebuild.\n",
    "    if output_path.exists() and not force_rerun:\n",
    "        try:\n",
    "            pf_existing = pq.ParquetFile(str(output_path))\n",
    "            existing_rows = pf_existing.metadata.num_rows\n",
    "            size_mb = output_path.stat().st_size / 1024**2\n",
    "            print(f\"â© Skipping {dataset_label} â€” Parquet already exists \"\n",
    "                  f\"({existing_rows:,} rows, {size_mb:.1f} MB). \"\n",
    "                  f\"Pass force_rerun=True to rebuild.\")\n",
    "            return {\n",
    "                \"dataset\": dataset_label,\n",
    "                \"total_rows\": existing_rows,\n",
    "                \"n_files\": len(files),\n",
    "                \"elapsed_sec\": 0.0,\n",
    "                \"size_mb\": round(size_mb, 1),\n",
    "                \"output_path\": str(output_path),\n",
    "            }\n",
    "        except Exception as _corrupt_err:\n",
    "            print(f\"âš ï¸  {dataset_label}: existing Parquet is corrupt ({_corrupt_err}). \"\n",
    "                  f\"Deleting and rebuildingâ€¦\")\n",
    "            output_path.unlink()\n",
    "\n",
    "    if output_path.exists():\n",
    "        output_path.unlink()   # force_rerun=True: delete and rebuild\n",
    "\n",
    "    writer    = None\n",
    "    pq_schema = None\n",
    "    total_rows = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(f\"\\n{'='*65}\")\n",
    "    print(f\"ğŸš€ Processing {dataset_label}  ({len(files)} files)\")\n",
    "    print(f\"   â†’ Output: {output_path.name}\")\n",
    "    print(f\"{'='*65}\")\n",
    "\n",
    "    for fi, fpath in enumerate(files, 1):\n",
    "        fpath = Path(fpath)\n",
    "        file_rows = 0\n",
    "        print(f\"  [{fi:>2}/{len(files)}] {fpath.name:<50s}\", end=\"\", flush=True)\n",
    "\n",
    "        try:\n",
    "            if use_chunks:\n",
    "                reader = pd.read_csv(\n",
    "                    fpath, chunksize=CHUNK_SIZE, low_memory=False, **read_kwargs\n",
    "                )\n",
    "            else:\n",
    "                reader = [pd.read_csv(fpath, low_memory=False, **read_kwargs)]\n",
    "\n",
    "            for chunk in reader:\n",
    "                if chunk.empty:\n",
    "                    continue\n",
    "                aligned = transform_fn(chunk)\n",
    "                table   = pa.Table.from_pandas(aligned, preserve_index=False)\n",
    "\n",
    "                if writer is None:\n",
    "                    pq_schema = table.schema\n",
    "                    writer = pq.ParquetWriter(\n",
    "                        str(output_path),\n",
    "                        schema=pq_schema,\n",
    "                        compression=\"snappy\",\n",
    "                    )\n",
    "                else:\n",
    "                    # Cast to established schema to avoid type drift between chunks\n",
    "                    table = table.cast(pq_schema)\n",
    "\n",
    "                writer.write_table(table)\n",
    "                file_rows  += len(aligned)\n",
    "                total_rows += len(aligned)\n",
    "                del aligned, table  # free memory immediately\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n    âŒ  ERROR: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "        print(f\"  {file_rows:>10,} rows\")\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    size_mb = output_path.stat().st_size / 1024**2 if output_path.exists() else 0\n",
    "\n",
    "    print(f\"\\n{'â”€'*65}\")\n",
    "    print(f\"âœ… {dataset_label} complete\")\n",
    "    print(f\"   Total rows written  : {total_rows:,}\")\n",
    "    print(f\"   Parquet size        : {size_mb:.1f} MB\")\n",
    "    print(f\"   Elapsed             : {elapsed:.1f}s\")\n",
    "    print(f\"   Throughput          : {total_rows/max(elapsed,1)/1e6:.2f}M rows/sec\")\n",
    "\n",
    "    return {\n",
    "        \"dataset\": dataset_label,\n",
    "        \"total_rows\": total_rows,\n",
    "        \"n_files\": len(files),\n",
    "        \"elapsed_sec\": round(elapsed, 1),\n",
    "        \"size_mb\": round(size_mb, 1),\n",
    "        \"output_path\": str(output_path),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ… stream_to_parquet() helper defined\")\n",
    "print(f\"   CHUNK_SIZE = {CHUNK_SIZE:,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab39ec8",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 â€” TON-IoT Processing\n",
    "\n",
    "22,339,021 rows Ã— 47 cols â†’ aligned schema. File-by-file, 100k-row chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875d74a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 TON-IoT files\n",
      "\n",
      "=================================================================\n",
      "ğŸš€ Processing TON-IoT  (23 files)\n",
      "   â†’ Output: toniot_aligned.parquet\n",
      "=================================================================\n",
      "  [ 1/23] Network_dataset_1.csv                                1,000,000 rows\n",
      "  [ 2/23] Network_dataset_10.csv                               1,000,000 rows\n",
      "  [ 3/23] Network_dataset_11.csv                               1,000,000 rows\n",
      "  [ 4/23] Network_dataset_12.csv                               1,000,000 rows\n",
      "  [ 5/23] Network_dataset_13.csv                               1,000,000 rows\n",
      "  [ 6/23] Network_dataset_14.csv                               1,000,000 rows\n",
      "  [ 7/23] Network_dataset_15.csv                               1,000,000 rows\n",
      "  [ 8/23] Network_dataset_16.csv                               1,000,000 rows\n",
      "  [ 9/23] Network_dataset_17.csv                               1,000,000 rows\n",
      "  [10/23] Network_dataset_18.csv                               1,000,000 rows\n",
      "  [11/23] Network_dataset_19.csv                               1,000,000 rows\n",
      "  [12/23] Network_dataset_2.csv                                1,000,000 rows\n",
      "  [13/23] Network_dataset_20.csv                               1,000,000 rows\n",
      "  [14/23] Network_dataset_21.csv                               1,000,000 rows\n",
      "  [15/23] Network_dataset_22.csv                               1,000,000 rows\n",
      "  [16/23] Network_dataset_23.csv                                 339,021 rows\n",
      "  [17/23] Network_dataset_3.csv                                1,000,000 rows\n",
      "  [18/23] Network_dataset_4.csv                                1,000,000 rows\n",
      "  [19/23] Network_dataset_5.csv                                1,000,000 rows\n",
      "  [20/23] Network_dataset_6.csv                                1,000,000 rows\n",
      "  [21/23] Network_dataset_7.csv                                1,000,000 rows\n",
      "  [22/23] Network_dataset_8.csv                                1,000,000 rows\n",
      "  [23/23] Network_dataset_9.csv                                1,000,000 rows\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… TON-IoT complete\n",
      "   Total rows written  : 22,339,021\n",
      "   Parquet size        : 236.8 MB\n",
      "   Elapsed             : 253.0s\n",
      "   Throughput          : 0.09M rows/sec\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 8 | Execute TON-IoT Alignment\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "toniot_files = sorted(TONIOT_DATA_DIR.glob(\"Network_dataset_*.csv\"))\n",
    "print(f\"Found {len(toniot_files)} TON-IoT files\")\n",
    "\n",
    "# TON-IoT read kwargs:\n",
    "#   - no sep override (standard CSV)\n",
    "#   - low_memory=False silences DtypeWarning for src_bytes (object col)\n",
    "TONIOT_READ_KW = {\n",
    "    \"encoding\": \"utf-8\",\n",
    "    \"on_bad_lines\": \"skip\",\n",
    "}\n",
    "\n",
    "toniot_stats = stream_to_parquet(\n",
    "    files         = toniot_files,\n",
    "    transform_fn  = transform_toniot_chunk,\n",
    "    output_path   = TONIOT_PARQUET,\n",
    "    dataset_label = \"TON-IoT\",\n",
    "    read_kwargs   = TONIOT_READ_KW,\n",
    "    use_chunks    = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090652c",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 â€” IoT-23 Processing\n",
    "\n",
    "~325M rows across 23 Zeek log files. Tab-separated with 8-line Zeek header.\n",
    "Compound label column `tunnel_parents   label   detailed-label` split at runtime.\n",
    "This is the largest dataset â€” chunked streaming is critical here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7993bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 IoT-23 files\n",
      "   âš ï¸  Cannot delete locked file â€” will write to .tmp and rename instead.\n",
      "\n",
      "=================================================================\n",
      "ğŸš€ Processing IoT-23  (23 files)\n",
      "   â†’ Output: iot23_aligned.parquet\n",
      "=================================================================\n",
      "  [ 1/23] CTU-Honeypot-Capture-4-1                \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [ 2/23] CTU-Honeypot-Capture-5-1                \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [ 3/23] Somfy-01                                \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [ 4/23] CTU-IoT-Malware-Capture-1-1             \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [ 5/23] CTU-IoT-Malware-Capture-17-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [ 6/23] CTU-IoT-Malware-Capture-20-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [ 7/23] CTU-IoT-Malware-Capture-21-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [ 8/23] CTU-IoT-Malware-Capture-3-1             \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [ 9/23] CTU-IoT-Malware-Capture-33-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [10/23] CTU-IoT-Malware-Capture-34-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [11/23] CTU-IoT-Malware-Capture-35-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [12/23] CTU-IoT-Malware-Capture-36-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [13/23] CTU-IoT-Malware-Capture-39-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [14/23] CTU-IoT-Malware-Capture-42-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [15/23] CTU-IoT-Malware-Capture-43-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [16/23] CTU-IoT-Malware-Capture-44-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [17/23] CTU-IoT-Malware-Capture-48-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [18/23] CTU-IoT-Malware-Capture-49-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [19/23] CTU-IoT-Malware-Capture-52-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [20/23] CTU-IoT-Malware-Capture-60-1            \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [21/23] CTU-IoT-Malware-Capture-7-1             \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [22/23] CTU-IoT-Malware-Capture-8-1             \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "  [23/23] CTU-IoT-Malware-Capture-9-1             \n",
      "    âš ï¸  Column count mismatch in conn.log.labeled: expected 23, got 21 â€” skipping file to avoid silent data misalignment\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… IoT-23 complete\n",
      "   Total rows written  : 0\n",
      "   Parquet size        : 192.5 MB\n",
      "   Elapsed             : 0.0s  (0.0 min)\n",
      "   Throughput          : 0.00M rows/sec\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 9 | Execute IoT-23 Alignment\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "iot23_files = sorted(IOT23_DATA_DIR.rglob(\"conn.log.labeled\"))\n",
    "print(f\"Found {len(iot23_files)} IoT-23 files\")\n",
    "\n",
    "# IoT-23 needs custom streaming due to:\n",
    "#   - TSV format (sep='\\t')\n",
    "#   - 8-line Zeek metadata header (skiprows=8)\n",
    "#   - Fixed column names from #fields (names=IOT23_COLS)\n",
    "#   - #close footer rows that appear in the last chunk (need to filter)\n",
    "\n",
    "def stream_iot23_to_parquet(force_rerun: bool = False):\n",
    "    \"\"\"\n",
    "    Custom IoT-23 Parquet writer.\n",
    "    Cannot use generic stream_to_parquet directly because:\n",
    "     1. TSV + skiprows=8 + fixed names must be set at read time\n",
    "     2. #close footer rows appear in mid/end chunks â†’ need inline filter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    force_rerun : bool\n",
    "        If False (default), skip reprocessing when a complete Parquet already\n",
    "        exists (idempotent / resume-safe). Pass True to delete and rebuild.\n",
    "    \"\"\"\n",
    "    output_path = IOT23_PARQUET\n",
    "\n",
    "    # â”€â”€ Idempotency guard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if output_path.exists() and not force_rerun:\n",
    "        try:\n",
    "            pf_existing = pq.ParquetFile(str(output_path))\n",
    "            existing_rows = pf_existing.metadata.num_rows\n",
    "            size_mb = output_path.stat().st_size / 1024**2\n",
    "            print(f\"â© Skipping IoT-23 â€” Parquet already exists \"\n",
    "                  f\"({existing_rows:,} rows, {size_mb:.1f} MB). \"\n",
    "                  f\"Pass force_rerun=True to rebuild.\")\n",
    "            return {\n",
    "                \"dataset\": \"IoT-23\",\n",
    "                \"total_rows\": existing_rows,\n",
    "                \"n_files\": len(iot23_files),\n",
    "                \"elapsed_sec\": 0.0,\n",
    "                \"size_mb\": round(size_mb, 1),\n",
    "                \"output_path\": str(output_path),\n",
    "            }\n",
    "        except Exception as _corrupt_err:\n",
    "            print(f\"âš ï¸  IoT-23: existing Parquet is corrupt ({_corrupt_err}). \"\n",
    "                  f\"Deleting and rebuildingâ€¦\")\n",
    "            try:\n",
    "                output_path.unlink()\n",
    "            except PermissionError:\n",
    "                print(f\"   âš ï¸  Cannot delete locked file â€” will write to .tmp and rename instead.\")\n",
    "\n",
    "    if output_path.exists():\n",
    "        try:\n",
    "            output_path.unlink()   # force_rerun=True: delete and rebuild\n",
    "        except PermissionError:\n",
    "            print(f\"   âš ï¸  Cannot delete locked file â€” will write to .tmp and rename instead.\")\n",
    "\n",
    "    # Write to a .tmp staging file; atomically rename to final name when complete.\n",
    "    # This avoids Windows Explorer locking the target filename while we write.\n",
    "    staging_path = output_path.with_suffix(\".parquet.tmp\")\n",
    "    if staging_path.exists():\n",
    "        staging_path.unlink()  # clean up any leftover tmp from a prior crash\n",
    "    writer     = None\n",
    "    pq_schema  = None\n",
    "    total_rows = 0\n",
    "    t0         = time.time()\n",
    "\n",
    "    print(f\"\\n{'='*65}\")\n",
    "    print(f\"ğŸš€ Processing IoT-23  ({len(iot23_files)} files)\")\n",
    "    print(f\"   â†’ Output: {output_path.name}\")\n",
    "    print(f\"{'='*65}\")\n",
    "\n",
    "    for fi, fpath in enumerate(iot23_files, 1):\n",
    "        file_rows = 0\n",
    "        print(f\"  [{fi:>2}/{len(iot23_files)}] {fpath.parent.parent.name:<40s}\", end=\"\", flush=True)\n",
    "\n",
    "        try:\n",
    "            # â”€â”€ Column header validation (Phase 0 regression guard) â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            # Dynamically read the #fields line to verify column count matches\n",
    "            # IOT23_COLS before reading any data. If a file has extra/shifted\n",
    "            # columns, pd.read_csv(names=IOT23_COLS) would silently misalign\n",
    "            # data (e.g. duration column filled with service strings).\n",
    "            with open(fpath, \"r\", encoding=\"utf-8\", errors=\"replace\") as _fh:\n",
    "                for _ in range(6): next(_fh)          # skip lines 1-6\n",
    "                fields_line = next(_fh).strip()       # line 7 = #fields\n",
    "            if fields_line.startswith(\"#fields\"):\n",
    "                actual_cols = fields_line.split(\"\\t\")[1:]   # strip '#fields' token\n",
    "                # The compound col counts as 3 separate fields in the header\n",
    "                n_expected = len(IOT23_COLS) - 1 + 3  # replace compound with 3\n",
    "                if len(actual_cols) != n_expected:\n",
    "                    print(f\"\\n    âš ï¸  Column count mismatch in {fpath.name}: \"\n",
    "                          f\"expected {n_expected}, got {len(actual_cols)} â€” \"\n",
    "                          f\"skipping file to avoid silent data misalignment\")\n",
    "                    continue\n",
    "\n",
    "            reader = pd.read_csv(\n",
    "                fpath,\n",
    "                sep           = \"\\t\",\n",
    "                skiprows      = 8,        # skip Zeek metadata header\n",
    "                header        = None,\n",
    "                names         = IOT23_COLS,\n",
    "                chunksize     = CHUNK_SIZE,\n",
    "                low_memory    = False,\n",
    "                encoding      = \"utf-8\",\n",
    "                encoding_errors = \"replace\",\n",
    "                on_bad_lines  = \"skip\",\n",
    "            )\n",
    "\n",
    "            for chunk in reader:\n",
    "                # â”€â”€ Filter Zeek #close footer row â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                # Last line of every Zeek log: #close YYYY-MM-DD-HH-MM-SS\n",
    "                # After skiprows=8+fixed names, it appears as a data row\n",
    "                # where 'ts' (col 0) starts with '#'\n",
    "                chunk = chunk[~chunk[\"ts\"].astype(str).str.startswith(\"#\")].copy()\n",
    "                if chunk.empty:\n",
    "                    continue\n",
    "\n",
    "                aligned = transform_iot23_chunk(chunk)\n",
    "                table   = pa.Table.from_pandas(aligned, preserve_index=False)\n",
    "\n",
    "                if writer is None:\n",
    "                    pq_schema = table.schema\n",
    "                    writer = pq.ParquetWriter(\n",
    "                        str(staging_path),  # write to .tmp, rename when complete\n",
    "                        schema      = pq_schema,\n",
    "                        compression = \"snappy\",\n",
    "                    )\n",
    "                else:\n",
    "                    table = table.cast(pq_schema)\n",
    "\n",
    "                writer.write_table(table)\n",
    "                file_rows  += len(aligned)\n",
    "                total_rows += len(aligned)\n",
    "                del aligned, table\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n    âŒ  ERROR in {fpath.name}: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "        print(f\"  {file_rows:>10,} rows\")\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "    # Atomic rename: staging (.tmp) â†’ final name.\n",
    "    # Use .replace() not .rename() â€” on Windows, .replace() overwrites the\n",
    "    # destination atomically, handling the case where the old corrupt file\n",
    "    # could not be deleted due to an Explorer lock.\n",
    "    if staging_path.exists():\n",
    "        staging_path.replace(output_path)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    size_mb = output_path.stat().st_size / 1024**2 if output_path.exists() else 0\n",
    "\n",
    "    print(f\"\\n{'â”€'*65}\")\n",
    "    print(f\"âœ… IoT-23 complete\")\n",
    "    print(f\"   Total rows written  : {total_rows:,}\")\n",
    "    print(f\"   Parquet size        : {size_mb:.1f} MB\")\n",
    "    print(f\"   Elapsed             : {elapsed:.1f}s  ({elapsed/60:.1f} min)\")\n",
    "    print(f\"   Throughput          : {total_rows/max(elapsed,1)/1e6:.2f}M rows/sec\")\n",
    "\n",
    "    return {\n",
    "        \"dataset\": \"IoT-23\",\n",
    "        \"total_rows\": total_rows,\n",
    "        \"n_files\": len(iot23_files),\n",
    "        \"elapsed_sec\": round(elapsed, 1),\n",
    "        \"size_mb\": round(size_mb, 1),\n",
    "        \"output_path\": str(output_path),\n",
    "    }\n",
    "\n",
    "\n",
    "iot23_stats = stream_iot23_to_parquet(force_rerun=True)  # force rebuild â€” partial file detected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665235bc",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 â€” Bot-IoT Processing\n",
    "\n",
    "~3.7M rows across 4 CSV files. Argus format. 22 behavioral window features\n",
    "(Group B Bot-IoT-only). sport/dport = -1 for ARP â†’ `has_port` boolean derived.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb812c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Still locked: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\Users\\\\suhas\\\\OneDrive\\\\Desktop\\\\Capstone\\\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\\\main_folder\\\\data\\\\unified\\\\iot23_aligned.parquet'\n",
      "   â†’ Pause OneDrive sync or close Explorer preview and retry\n"
     ]
    }
   ],
   "source": [
    "import gc, os\n",
    "\n",
    "# Release any open PyArrow file handles pointing at the corrupt IoT-23 Parquet.\n",
    "# pf / pf_val are ParquetFile objects that keep an OS-level file descriptor open.\n",
    "import builtins\n",
    "_g = globals()\n",
    "for _var in [\"pf\", \"pf_val\", \"pf_existing\"]:\n",
    "    if _var in _g:\n",
    "        del _g[_var]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Delete from within the kernel process â€” avoids Windows cross-process lock\n",
    "iot23_partial = UNIFIED_DIR / \"iot23_aligned.parquet\"\n",
    "if iot23_partial.exists():\n",
    "    try:\n",
    "        os.remove(str(iot23_partial))\n",
    "        print(f\"âœ… Deleted corrupt file: {iot23_partial.name}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"âŒ Still locked: {e}\")\n",
    "        print(\"   â†’ Pause OneDrive sync or close Explorer preview and retry\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  File already gone â€” nothing to delete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d242d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 Bot-IoT files\n",
      "\n",
      "=================================================================\n",
      "ğŸš€ Processing Bot-IoT  (4 files)\n",
      "   â†’ Output: botiot_aligned.parquet\n",
      "=================================================================\n",
      "  [ 1/4] UNSW_2018_IoT_Botnet_Full5pc_1.csv                   1,000,000 rows\n",
      "  [ 2/4] UNSW_2018_IoT_Botnet_Full5pc_2.csv                   1,000,000 rows\n",
      "  [ 3/4] UNSW_2018_IoT_Botnet_Full5pc_3.csv                   1,000,000 rows\n",
      "  [ 4/4] UNSW_2018_IoT_Botnet_Full5pc_4.csv                     668,522 rows\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Bot-IoT complete\n",
      "   Total rows written  : 3,668,522\n",
      "   Parquet size        : 78.6 MB\n",
      "   Elapsed             : 20.4s\n",
      "   Throughput          : 0.18M rows/sec\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 10 | Execute Bot-IoT Alignment\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "botiot_files = sorted(BOTIOT_DATA_DIR.glob(\"UNSW_2018_IoT_Botnet_Full5pc_*.csv\"))\n",
    "print(f\"Found {len(botiot_files)} Bot-IoT files\")\n",
    "\n",
    "BOTIOT_READ_KW = {\n",
    "    \"encoding\": \"utf-8\",\n",
    "    \"on_bad_lines\": \"skip\",\n",
    "}\n",
    "\n",
    "botiot_stats = stream_to_parquet(\n",
    "    files         = botiot_files,\n",
    "    transform_fn  = transform_botiot_chunk,\n",
    "    output_path   = BOTIOT_PARQUET,\n",
    "    dataset_label = \"Bot-IoT\",\n",
    "    read_kwargs   = BOTIOT_READ_KW,\n",
    "    use_chunks    = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61963cb6",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 â€” Schema Validation & Alignment Summary\n",
    "\n",
    "Verify that all three Parquet files share the canonical schema.\n",
    "Spot-check dtypes, sentinel values, label distributions, and column completeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df623170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸ” SCHEMA VALIDATION\n",
      "=================================================================\n",
      "\n",
      "  âœ… TON-IoT  (236.8 MB)\n",
      "     Columns    : 48\n",
      "     Expected   : 48\n",
      "     Col order  : âœ… Exact match\n",
      "     Dtype check: âœ… All OK\n",
      "     NaN cols   : âš ï¸  ['dataset_source']\n",
      "     botiot_mean sentinel      : âœ… -1.0\n",
      "     Unmapped labels: 0 / 50,000 (0.00%)\n",
      "     Proto vocab    : âœ… Clean\n",
      "     State vocab    : âœ… Clean\n",
      "\n",
      "  âœ… IoT-23  (517.8 MB)\n",
      "     Columns    : 48\n",
      "     Expected   : 48\n",
      "     Col order  : âœ… Exact match\n",
      "     Dtype check: âœ… All OK\n",
      "     NaN cols   : âš ï¸  ['dataset_source']\n",
      "     toniot_dns_qclass sentinel: âœ… -1\n",
      "     Unmapped labels: 0 / 452 (0.00%)\n",
      "     Proto vocab    : âœ… Clean\n",
      "     State vocab    : âœ… Clean\n",
      "\n",
      "  âœ… Bot-IoT  (78.6 MB)\n",
      "     Columns    : 48\n",
      "     Expected   : 48\n",
      "     Col order  : âœ… Exact match\n",
      "     Dtype check: âœ… All OK\n",
      "     NaN cols   : âš ï¸  ['dataset_source']\n",
      "     zeek_missed_bytes sentinel: âœ… -1\n",
      "     zeek_service sentinel     : âœ… unknown\n",
      "     Unmapped labels: 0 / 50,000 (0.00%)\n",
      "     Proto vocab    : âœ… Clean\n",
      "     State vocab    : âœ… Clean\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ” Cross-dataset schema equivalence:\n",
      "   Common columns  : 48\n",
      "   Union columns   : 48\n",
      "   âœ… All three Parquet files share IDENTICAL schema\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 11 | Schema Validation\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "PARQUET_FILES = {\n",
    "    \"TON-IoT\": TONIOT_PARQUET,\n",
    "    \"IoT-23\":  IOT23_PARQUET,\n",
    "    \"Bot-IoT\": BOTIOT_PARQUET,\n",
    "}\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ” SCHEMA VALIDATION\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "validation_results = {}\n",
    "schemas_seen = []\n",
    "\n",
    "for label, pq_path in PARQUET_FILES.items():\n",
    "    if not pq_path.exists():\n",
    "        print(f\"\\n  âŒ {label}: Parquet file not found â€” {pq_path.name}\")\n",
    "        continue\n",
    "\n",
    "    # Read ONLY the first row-group (~100k rows) â€” never load the full file\n",
    "    # pq.read_table() on IoT-23 (325M rows) would exhaust all RAM\n",
    "    pf_val = pq.ParquetFile(str(pq_path))\n",
    "    sample = pf_val.read_row_group(0).to_pandas().head(50_000)\n",
    "    schemas_seen.append(set(sample.columns))\n",
    "\n",
    "    print(f\"\\n  âœ… {label}  ({pq_path.stat().st_size/1024**2:.1f} MB)\")\n",
    "    print(f\"     Columns    : {len(sample.columns)}\")\n",
    "    print(f\"     Expected   : {len(FINAL_COLUMNS)}\")\n",
    "\n",
    "    # Column order check\n",
    "    col_match = list(sample.columns) == FINAL_COLUMNS\n",
    "    print(f\"     Col order  : {'âœ… Exact match' if col_match else 'âŒ MISMATCH'}\")\n",
    "\n",
    "    # dtype spot-check\n",
    "    dtype_issues = []\n",
    "    for c in [\"univ_duration\", \"botiot_mean\", \"botiot_rate\"]:\n",
    "        if sample[c].dtype != \"float64\":\n",
    "            dtype_issues.append(f\"{c}={sample[c].dtype}\")\n",
    "    for c in [\"univ_src_bytes\", \"toniot_dns_qclass\", \"botiot_TnBPSrcIP\"]:\n",
    "        if sample[c].dtype != \"int64\":\n",
    "            dtype_issues.append(f\"{c}={sample[c].dtype}\")\n",
    "    for c in [\"univ_has_src_port\", \"univ_label_binary\"]:\n",
    "        if str(sample[c].dtype) not in (\"int8\",):\n",
    "            dtype_issues.append(f\"{c}={sample[c].dtype}\")\n",
    "    print(f\"     Dtype check: {'âœ… All OK' if not dtype_issues else 'âš ï¸  ' + str(dtype_issues)}\")\n",
    "\n",
    "    # Sentinel check â€” Group B should have no NaN\n",
    "    nan_cols = [c for c in FINAL_COLUMNS if sample[c].isna().any()]\n",
    "    print(f\"     NaN cols   : {'âœ… None' if not nan_cols else 'âš ï¸  ' + str(nan_cols[:5])}\")\n",
    "\n",
    "    # Sentinel value check â€” non-source columns should be -1 or \"unknown\"\n",
    "    if label == \"Bot-IoT\":\n",
    "        sentinel_ok = (sample[\"zeek_missed_bytes\"] == -1).all()\n",
    "        print(f\"     zeek_missed_bytes sentinel: {'âœ… -1' if sentinel_ok else 'âŒ unexpected values'}\")\n",
    "        sentinel_ok2 = (sample[\"zeek_service\"] == \"unknown\").all()\n",
    "        print(f\"     zeek_service sentinel     : {'âœ… unknown' if sentinel_ok2 else 'âŒ'}\")\n",
    "    if label == \"IoT-23\":\n",
    "        sentinel_ok = (sample[\"toniot_dns_qclass\"] == -1).all()\n",
    "        print(f\"     toniot_dns_qclass sentinel: {'âœ… -1' if sentinel_ok else 'âŒ'}\")\n",
    "    if label == \"TON-IoT\":\n",
    "        sentinel_ok = (sample[\"botiot_mean\"] == -1.0).all()\n",
    "        print(f\"     botiot_mean sentinel      : {'âœ… -1.0' if sentinel_ok else 'âŒ'}\")\n",
    "\n",
    "    # Label check â€” no unmapped labels\n",
    "    unmapped = (sample[\"univ_label_multiclass\"] == -1).sum()\n",
    "    pct_unmapped = unmapped / len(sample) * 100\n",
    "    print(f\"     Unmapped labels: {unmapped:,} / {len(sample):,} ({pct_unmapped:.2f}%)\")\n",
    "\n",
    "    # Protocol vocab check\n",
    "    unseen_proto = set(sample[\"univ_proto\"].unique()) - VALID_PROTO_VOCAB\n",
    "    print(f\"     Proto vocab    : {'âœ… Clean' if not unseen_proto else 'âš ï¸  unseen: ' + str(unseen_proto)}\")\n",
    "\n",
    "    # State vocab check\n",
    "    unseen_state = set(sample[\"univ_state\"].unique()) - VALID_STATE_VOCAB\n",
    "    print(f\"     State vocab    : {'âœ… Clean' if not unseen_state else 'âš ï¸  unseen: ' + str(unseen_state)}\")\n",
    "\n",
    "    validation_results[label] = {\n",
    "        \"columns\": len(sample.columns),\n",
    "        \"col_order_ok\": col_match,\n",
    "        \"nan_cols\": nan_cols,\n",
    "        \"dtype_issues\": dtype_issues,\n",
    "        \"pct_unmapped_labels\": round(pct_unmapped, 4),\n",
    "    }\n",
    "\n",
    "# Cross-dataset schema equivalence check\n",
    "if len(schemas_seen) == 3:\n",
    "    common = schemas_seen[0] & schemas_seen[1] & schemas_seen[2]\n",
    "    total  = schemas_seen[0] | schemas_seen[1] | schemas_seen[2]\n",
    "    print(f\"\\n{'â”€'*65}\")\n",
    "    print(f\"ğŸ” Cross-dataset schema equivalence:\")\n",
    "    print(f\"   Common columns  : {len(common)}\")\n",
    "    print(f\"   Union columns   : {len(total)}\")\n",
    "    if common == total:\n",
    "        print(\"   âœ… All three Parquet files share IDENTICAL schema\")\n",
    "    else:\n",
    "        diff = total - common\n",
    "\n",
    "        print(f\"   âš ï¸  Schema divergence in: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaa0180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ğŸ“Š LABEL DISTRIBUTION PER DATASET (from Parquet metadata sample)\n",
      "=================================================================\n",
      "\n",
      "  ğŸ“ TON-IoT  â€” 22,339,021 total rows\n",
      "  Class                               Count        %\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Normal                            207,924    41.58%\n",
      "  Reconnaissance                    292,076    58.42%\n",
      "\n",
      "  Binary: Normal=207,924 (41.6%)  Attack=292,076 (58.4%)\n",
      "  Proto : {'tcp': np.int64(402354), 'udp': np.int64(90185), 'icmp': np.int64(7461)}\n",
      "  State : {'attempt': np.int64(339861), 'rejected': np.int64(127439), 'other': np.int64(20914), 'established': np.int64(11786)}\n",
      "\n",
      "  ğŸ“ IoT-23  â€” 325,309,946 total rows\n",
      "  Class                               Count        %\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Normal                            229,348    45.87%\n",
      "  Reconnaissance                    270,646    54.13%\n",
      "  C2_Botnet                               6     0.00%\n",
      "\n",
      "  Binary: Normal=229,348 (45.9%)  Attack=270,652 (54.1%)\n",
      "  Proto : {'tcp': np.int64(293253), 'udp': np.int64(194001), 'icmp': np.int64(12746)}\n",
      "  State : {'attempt': np.int64(483717), 'established': np.int64(11334), 'rejected': np.int64(4829), 'other': np.int64(120)}\n",
      "\n",
      "  ğŸ“ Bot-IoT  â€” 3,668,522 total rows\n",
      "  Class                               Count        %\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Volumetric_Attack                 500,000   100.00%\n",
      "\n",
      "  Binary: Normal=0 (0.0%)  Attack=500,000 (100.0%)\n",
      "  Proto : {'tcp': np.int64(499980), 'arp': np.int64(20)}\n",
      "  State : {'attempt': np.int64(286538), 'rejected': np.int64(213442), 'established': np.int64(20)}\n",
      "\n",
      "=================================================================\n",
      "ğŸ“‹ PROCESSING RUN SUMMARY\n",
      "=================================================================\n",
      "ğŸ“‹ PROCESSING RUN SUMMARY\n",
      "=================================================================\n",
      "\\nâœ… Label distribution DataFrame: (6, 6)\n",
      "dataset  total_rows_written  parquet_size_mb\n",
      "TON-IoT            22339021            236.8\n",
      " IoT-23           325309946            517.8\n",
      "Bot-IoT             3668522             78.6\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 12 | Label Distribution & Run Summary\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ“Š LABEL DISTRIBUTION PER DATASET (from Parquet metadata sample)\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "SAMPLE_ROWS = 500_000   # read 500k rows from each file for distribution check\n",
    "\n",
    "dist_rows = []\n",
    "run_summary = []\n",
    "\n",
    "for label, pq_path in PARQUET_FILES.items():\n",
    "    if not pq_path.exists():\n",
    "        print(f\"  âš ï¸  {label}: file not found, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Read full Parquet for row count (metadata only â€” no data scan)\n",
    "    pf = pq.ParquetFile(str(pq_path))\n",
    "    n_rows_total = pf.metadata.num_rows\n",
    "\n",
    "    # Read sample for label distribution â€” use row groups only, NEVER read_table()\n",
    "    # Accumulate row groups until we reach SAMPLE_ROWS or exhaust all groups\n",
    "    frames = []\n",
    "    rows_collected = 0\n",
    "    for rg_idx in range(pf.metadata.num_row_groups):\n",
    "        rg_df = pf.read_row_group(rg_idx).to_pandas()\n",
    "        frames.append(rg_df)\n",
    "        rows_collected += len(rg_df)\n",
    "        if rows_collected >= SAMPLE_ROWS:\n",
    "            break\n",
    "    sample = pd.concat(frames, ignore_index=True).head(SAMPLE_ROWS)\n",
    "    del frames\n",
    "\n",
    "    print(f\"\\n  ğŸ“ {label}  â€” {n_rows_total:,} total rows\")\n",
    "    print(f\"  {'Class':<30s} {'Count':>10s}  {'%':>7s}\")\n",
    "    print(f\"  {'â”€'*50}\")\n",
    "\n",
    "    class_counts = sample[\"univ_label_multiclass\"].value_counts().sort_index()\n",
    "    sample_size  = len(sample)\n",
    "\n",
    "    for cls_id, cnt in class_counts.items():\n",
    "        cls_name = LABEL_CLASS_NAMES.get(int(cls_id), f\"Class_{cls_id}\")\n",
    "        row_pct  = cnt / sample_size * 100\n",
    "        print(f\"  {cls_name:<30s} {cnt:>10,}   {row_pct:>6.2f}%\")\n",
    "        dist_rows.append({\n",
    "            \"dataset\": label,\n",
    "            \"class_id\": int(cls_id),\n",
    "            \"class_name\": cls_name,\n",
    "            \"sample_count\": int(cnt),\n",
    "            \"sample_pct\": round(row_pct, 4),\n",
    "            \"total_rows_in_file\": n_rows_total,\n",
    "        })\n",
    "\n",
    "    # Binary label summary\n",
    "    binary_counts = sample[\"univ_label_binary\"].value_counts()\n",
    "    n_normal = binary_counts.get(0, 0)\n",
    "    n_attack = binary_counts.get(1, 0)\n",
    "    print(f\"\\n  Binary: Normal={n_normal:,} ({n_normal/sample_size*100:.1f}%)  \"\n",
    "          f\"Attack={n_attack:,} ({n_attack/sample_size*100:.1f}%)\")\n",
    "\n",
    "    # Protocol distribution\n",
    "    proto_counts = sample[\"univ_proto\"].value_counts().head(5)\n",
    "    print(f\"  Proto : {dict(proto_counts)}\")\n",
    "\n",
    "    # State distribution\n",
    "    state_counts = sample[\"univ_state\"].value_counts()\n",
    "    print(f\"  State : {dict(state_counts)}\")\n",
    "\n",
    "    run_summary.append({\n",
    "        \"dataset\": label,\n",
    "        \"total_rows_written\": n_rows_total,\n",
    "        \"parquet_size_mb\": round(pq_path.stat().st_size/1024**2, 1),\n",
    "    })\n",
    "\n",
    "# Overall run stats table\n",
    "print(f\"\\n{'='*65}\")\n",
    "\n",
    "print(\"ğŸ“‹ PROCESSING RUN SUMMARY\")\n",
    "\n",
    "print(f\"{'='*65}\")\n",
    "print(f\"ğŸ“‹ PROCESSING RUN SUMMARY\")\n",
    "print(f\"{'='*65}\")\n",
    "\n",
    "dist_df = pd.DataFrame(dist_rows)\n",
    "run_df  = pd.DataFrame(run_summary)\n",
    "\n",
    "print(f\"\\\\nâœ… Label distribution DataFrame: {dist_df.shape}\")\n",
    "print(run_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573c157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Schema manifest saved   : phase1_1_schema_manifest.json\n",
      "âœ… Label distribution saved: phase1_1_label_distribution.csv\n",
      "  â„¹ï¸  Processing stats not in scope â€” rebuilding from Parquet metadata\n",
      "âœ… Processing stats saved  : phase1_1_processing_stats.csv\n",
      "âœ… Column registry saved   : phase1_1_column_registry.csv\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“¦ Artifacts summary:\n",
      "   phase1_1_schema_manifest.json                               5.7 KB\n",
      "   phase1_1_label_distribution.csv                             0.3 KB\n",
      "   phase1_1_processing_stats.csv                               0.2 KB\n",
      "   phase1_1_column_registry.csv                                3.0 KB\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 13 | Save Artifacts â€” Schema Manifest + Alignment Report\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# â”€â”€ 1. Schema Manifest (JSON) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Machine-readable schema that Phase 1.2+ can load without re-running alignment\n",
    "manifest = {\n",
    "    \"phase\": \"Phase_1_1_Universal_Schema_Alignment\",\n",
    "    \"generated\": pd.Timestamp.now().isoformat(),\n",
    "    \"total_columns\": len(FINAL_COLUMNS),\n",
    "    \"columns\": FINAL_COLUMNS,\n",
    "    \"group_a_columns\": [c for c in FINAL_COLUMNS\n",
    "                        if c.startswith(\"univ_\") or c == \"dataset_source\"],\n",
    "    \"group_b_zeek_columns\":   [c for c in FINAL_COLUMNS if c.startswith(\"zeek_\")],\n",
    "    \"group_b_toniot_columns\": [c for c in FINAL_COLUMNS if c.startswith(\"toniot_\")],\n",
    "    \"group_b_botiot_columns\": [c for c in FINAL_COLUMNS if c.startswith(\"botiot_\")],\n",
    "    \"label_columns\": [c for c in FINAL_COLUMNS if \"label\" in c],\n",
    "    \"sentinel_rules\": {\n",
    "        \"categorical_absent_feature\": \"<absent>\",\n",
    "        \"categorical_observed_unknown\": \"unknown\",\n",
    "        \"numerical_float_missing\": -1.0,\n",
    "        \"numerical_int_missing\": -1,\n",
    "        \"port_absent_sentinel\": -1,\n",
    "        \"zeek_dash_sentinel\": \"-\",\n",
    "        \"zeek_question_sentinel\": \"?\",\n",
    "        \"note\": \"<absent> = feature not collected by this dataset; unknown = feature collected but value undetermined\",\n",
    "    },\n",
    "    \"state_vocabulary\": sorted(VALID_STATE_VOCAB),\n",
    "    \"proto_vocabulary\": sorted(VALID_PROTO_VOCAB),\n",
    "    \"label_taxonomy\": {str(k): v for k, v in LABEL_CLASS_NAMES.items()},\n",
    "    \"parquet_files\": {\n",
    "        \"toniot\": str(TONIOT_PARQUET),\n",
    "        \"iot23\":  str(IOT23_PARQUET),\n",
    "        \"botiot\": str(BOTIOT_PARQUET),\n",
    "    },\n",
    "    \"source_column_mappings\": {\n",
    "        \"group_a\": {\n",
    "            \"toniot\": GROUP_A_TONIOT,\n",
    "            \"iot23\":  GROUP_A_IOT23,\n",
    "            \"botiot\": GROUP_A_BOTIOT,\n",
    "        }\n",
    "    },\n",
    "    \"validation\": validation_results,\n",
    "}\n",
    "\n",
    "manifest_path = ARTIFACTS_DIR / \"phase1_1_schema_manifest.json\"\n",
    "with open(manifest_path, \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2, default=str)\n",
    "print(f\"âœ… Schema manifest saved   : {manifest_path.name}\")\n",
    "\n",
    "\n",
    "# â”€â”€ 2. Label Distribution CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "label_dist_path = ARTIFACTS_DIR / \"phase1_1_label_distribution.csv\"\n",
    "dist_df.to_csv(label_dist_path, index=False)\n",
    "print(f\"âœ… Label distribution saved: {label_dist_path.name}\")\n",
    "\n",
    "\n",
    "# â”€â”€ 3. Run Summary CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# If processing cells were skipped (kernel restart), rebuild stats from Parquet metadata\n",
    "def _pq_stub(dataset_label, pq_path):\n",
    "    pf = pq.ParquetFile(str(pq_path))\n",
    "    return {\n",
    "        \"dataset\": dataset_label,\n",
    "        \"rows_written\": pf.metadata.num_rows,\n",
    "        \"files_processed\": \"N/A (restored from Parquet metadata)\",\n",
    "        \"parquet_size_mb\": round(pq_path.stat().st_size / 1024**2, 1),\n",
    "    }\n",
    "\n",
    "try:\n",
    "    all_stats = [toniot_stats, iot23_stats, botiot_stats]\n",
    "except NameError:\n",
    "    print(\"  â„¹ï¸  Processing stats not in scope â€” rebuilding from Parquet metadata\")\n",
    "    all_stats = [\n",
    "        _pq_stub(\"TON-IoT\", TONIOT_PARQUET),\n",
    "        _pq_stub(\"IoT-23\",  IOT23_PARQUET),\n",
    "        _pq_stub(\"Bot-IoT\", BOTIOT_PARQUET),\n",
    "    ]\n",
    "stats_df  = pd.DataFrame(all_stats)\n",
    "stats_path = ARTIFACTS_DIR / \"phase1_1_processing_stats.csv\"\n",
    "stats_df.to_csv(stats_path, index=False)\n",
    "print(f\"âœ… Processing stats saved  : {stats_path.name}\")\n",
    "\n",
    "\n",
    "# â”€â”€ 4. Column Registry CSV (for Phase 1.2 preprocessing reference) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "col_registry_rows = []\n",
    "for col in FINAL_COLUMNS:\n",
    "    group = (\"Group A\"     if col.startswith(\"univ_\") or col == \"dataset_source\"\n",
    "             else \"Label\"  if \"label\" in col\n",
    "             else \"Group B / Zeek\"   if col.startswith(\"zeek_\")\n",
    "             else \"Group B / TON-IoT\" if col.startswith(\"toniot_\")\n",
    "             else \"Group B / Bot-IoT\")\n",
    "    sources = []\n",
    "    if col == \"dataset_source\": sources = [\"toniot\", \"iot23\", \"botiot\"]\n",
    "    elif col in [c for c in manifest[\"group_a_columns\"] if c != \"dataset_source\"]:\n",
    "        sources = [\"toniot\", \"iot23\", \"botiot\"]\n",
    "    elif col.startswith(\"zeek_service\") or col.startswith(\"zeek_missed\"):\n",
    "        sources = [\"toniot\", \"iot23\"]\n",
    "    elif col == \"zeek_history\":\n",
    "        sources = [\"iot23\"]\n",
    "    elif col in (\"zeek_src_ip_bytes\", \"zeek_dst_ip_bytes\"):\n",
    "        sources = [\"toniot\", \"iot23\"]\n",
    "    elif col.startswith(\"toniot_\"):\n",
    "        sources = [\"toniot\"]\n",
    "    elif col.startswith(\"botiot_\"):\n",
    "        sources = [\"botiot\"]\n",
    "\n",
    "    sentinel_cat = \"none\"\n",
    "    is_categorical = col in [\"dataset_source\", \"univ_proto\", \"univ_state\",\n",
    "                               \"univ_label_str\", \"zeek_service\", \"zeek_history\"]\n",
    "    if col.startswith(\"zeek_\") or col.startswith(\"toniot_\") or col.startswith(\"botiot_\"):\n",
    "        sentinel_cat = \"unknown\" if is_categorical else (\"-1.0\" if \"AR_P_Proto\" in col or \"rate\" in col or col in [\"botiot_mean\",\"botiot_stddev\",\"botiot_sum\",\"botiot_min\",\"botiot_max\",\"botiot_rate\",\"botiot_srate\",\"botiot_drate\"] else \"-1\")\n",
    "\n",
    "    col_registry_rows.append({\n",
    "        \"column\": col,\n",
    "        \"group\": group,\n",
    "        \"present_in_datasets\": str(sources),\n",
    "        \"sentinel_when_absent\": sentinel_cat,\n",
    "        \"is_categorical\": is_categorical,\n",
    "    })\n",
    "\n",
    "col_registry_df = pd.DataFrame(col_registry_rows)\n",
    "col_registry_path = ARTIFACTS_DIR / \"phase1_1_column_registry.csv\"\n",
    "col_registry_df.to_csv(col_registry_path, index=False)\n",
    "print(f\"âœ… Column registry saved   : {col_registry_path.name}\")\n",
    "\n",
    "\n",
    "print(f\"\\n{'â”€'*65}\")\n",
    "print(\"ğŸ“¦ Artifacts summary:\")\n",
    "for f in [manifest_path, label_dist_path, stats_path, col_registry_path]:\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"   {f.name:<55s}  {size_kb:>6.1f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd811a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Staging files...\n",
      "  âœ… staged: Phase_1_1_Universal_Schema_Alignment.ipynb\n",
      "  âœ… staged: phase1_1_schema_manifest.json\n",
      "  âœ… staged: phase1_1_label_distribution.csv\n",
      "  âœ… staged: phase1_1_processing_stats.csv\n",
      "  âœ… staged: phase1_1_column_registry.csv\n",
      "\n",
      "ğŸ“Œ Committing...\n",
      "[main c530040] Phase_1_1: Universal Schema Alignment Ã¢â‚¬â€ 48-col aligned Parquet (TON-IoT/IoT-23/Bot-IoT), Group A/B partition, univ_ prefix, 5-class taxonomy, schema manifest + label distribution artifacts\n",
      " 1 file changed, 155 insertions(+), 35 deletions(-)\n",
      "\n",
      "ğŸ“Œ Pushing to remote...\n",
      "\n",
      "âœ… Phase 1.1 complete â€” all artifacts committed and pushed.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 14 | Git Commit â€” Artifacts + Notebook\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import subprocess\n",
    "\n",
    "REPO_ROOT = MAIN_DIR.parent  # main_folder's parent = repo root\n",
    "\n",
    "def git(args: list, cwd=REPO_ROOT):\n",
    "    result = subprocess.run(\n",
    "        [\"git\"] + args,\n",
    "        cwd   = str(cwd),\n",
    "        capture_output = True,\n",
    "        text  = True,\n",
    "    )\n",
    "    if result.stdout.strip():\n",
    "        print(result.stdout.strip())\n",
    "    if result.stderr.strip() and result.returncode != 0:\n",
    "        print(\"STDERR:\", result.stderr.strip())\n",
    "    return result.returncode\n",
    "\n",
    "print(\"ğŸ“Œ Staging files...\")\n",
    "files_to_stage = [\n",
    "    \"main_folder/Phase_1/Phase_1_1_Universal_Schema_Alignment.ipynb\",\n",
    "    \"main_folder/artifacts/phase1_1_schema_manifest.json\",\n",
    "    \"main_folder/artifacts/phase1_1_label_distribution.csv\",\n",
    "    \"main_folder/artifacts/phase1_1_processing_stats.csv\",\n",
    "    \"main_folder/artifacts/phase1_1_column_registry.csv\",\n",
    "]\n",
    "\n",
    "for f in files_to_stage:\n",
    "    rc = git([\"add\", f])\n",
    "    if rc == 0:\n",
    "        print(f\"  âœ… staged: {f.split('/')[-1]}\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸  could not stage: {f}\")\n",
    "\n",
    "print(\"\\nğŸ“Œ Committing...\")\n",
    "commit_msg = (\n",
    "    f\"Phase_1_1: Universal Schema Alignment â€” \"\n",
    "    f\"{len(FINAL_COLUMNS)}-col aligned Parquet (TON-IoT/IoT-23/Bot-IoT), \"\n",
    "    f\"Group A/B partition, univ_ prefix, 5-class taxonomy, \"\n",
    "    f\"schema manifest + label distribution artifacts\"\n",
    ")\n",
    "git([\"commit\", \"-m\", commit_msg])\n",
    "\n",
    "print(\"\\nğŸ“Œ Pushing to remote...\")\n",
    "git([\"push\"])\n",
    "\n",
    "print(\"\\nâœ… Phase 1.1 complete â€” all artifacts committed and pushed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ea5dbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“‹  COMPLETENESS AUDIT â€” Source rows vs Parquet rows\n",
      "======================================================================\n",
      "\n",
      "â–¶  TON-IoT\n",
      "   Source CSVs (23 files) :      22,339,021 rows\n",
      "   Parquet                   :      22,339,021 rows\n",
      "   Coverage                  : 100.0000%  âœ…\n",
      "\n",
      "â–¶  IoT-23  (counting source rows â€” may take ~30s for 23 files)\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowInvalid\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m iot23_log_files = \u001b[38;5;28msorted\u001b[39m(IOT23_DATA_DIR.rglob(\u001b[33m\"\u001b[39m\u001b[33mconn.log.labeled\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     50\u001b[39m src_rows_iot23  = \u001b[38;5;28msum\u001b[39m(count_iot23_rows(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m iot23_log_files)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m pq_rows_iot23   = \u001b[43mpq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mParquetFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIOT23_PARQUET\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.metadata.num_rows\n\u001b[32m     52\u001b[39m pct = pq_rows_iot23 / src_rows_iot23 * \u001b[32m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m src_rows_iot23 \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     53\u001b[39m status = \u001b[33m\"\u001b[39m\u001b[33mâœ…\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pq_rows_iot23 == src_rows_iot23 \u001b[38;5;28;01melse\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mâš ï¸ \u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pct > \u001b[32m99\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mâŒ\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyarrow\\parquet\\core.py:328\u001b[39m, in \u001b[36mParquetFile.__init__\u001b[39m\u001b[34m(self, source, metadata, common_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, filesystem, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_source = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# We opened it here, ensure we close it.\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28mself\u001b[39m.reader = ParquetReader()\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_memory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[38;5;28mself\u001b[39m.common_metadata = common_metadata\n\u001b[32m    341\u001b[39m \u001b[38;5;28mself\u001b[39m._nested_paths_by_prefix = \u001b[38;5;28mself\u001b[39m._build_nested_paths()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyarrow\\_parquet.pyx:1656\u001b[39m, in \u001b[36mpyarrow._parquet.ParquetReader.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowInvalid\u001b[39m: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COMPLETENESS AUDIT â€” Parquet row counts vs raw source file row counts\n",
    "# Counts raw source rows WITHOUT loading full data into memory:\n",
    "#   - TON-IoT / Bot-IoT: wc-style line count minus header per CSV\n",
    "#   - IoT-23: line count minus 9 (8 Zeek header lines + 1 #close footer) per log\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def count_csv_rows(path, has_header=True):\n",
    "    \"\"\"Count data rows in a CSV without loading into pandas.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        n = sum(1 for _ in f)\n",
    "    return n - (1 if has_header else 0)\n",
    "\n",
    "def count_iot23_rows(path):\n",
    "    \"\"\"Count data rows in a Zeek log (skip 8-line header + #close footer).\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        lines = f.readlines()\n",
    "    data_lines = [l for l in lines[8:] if not l.startswith(\"#\")]\n",
    "    return len(data_lines)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“‹  COMPLETENESS AUDIT â€” Source rows vs Parquet rows\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "grand_source = 0\n",
    "grand_parquet = 0\n",
    "all_ok = True\n",
    "\n",
    "# â”€â”€ TON-IoT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nâ–¶  TON-IoT\")\n",
    "toniot_csv_files = sorted(TONIOT_DATA_DIR.glob(\"Network_dataset_*.csv\"))\n",
    "src_rows_toniot  = sum(count_csv_rows(f) for f in toniot_csv_files)\n",
    "pq_rows_toniot   = pq.ParquetFile(str(TONIOT_PARQUET)).metadata.num_rows\n",
    "pct = pq_rows_toniot / src_rows_toniot * 100\n",
    "status = \"âœ…\" if pq_rows_toniot == src_rows_toniot else (\"âš ï¸ \" if pct > 99 else \"âŒ\")\n",
    "print(f\"   Source CSVs ({len(toniot_csv_files)} files) : {src_rows_toniot:>15,} rows\")\n",
    "print(f\"   Parquet                   : {pq_rows_toniot:>15,} rows\")\n",
    "print(f\"   Coverage                  : {pct:.4f}%  {status}\")\n",
    "if pq_rows_toniot != src_rows_toniot:\n",
    "    all_ok = False\n",
    "    print(f\"   Delta                     : {pq_rows_toniot - src_rows_toniot:+,} rows\")\n",
    "grand_source  += src_rows_toniot\n",
    "grand_parquet += pq_rows_toniot\n",
    "\n",
    "# â”€â”€ IoT-23 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nâ–¶  IoT-23  (counting source rows â€” may take ~30s for 23 files)\")\n",
    "iot23_log_files = sorted(IOT23_DATA_DIR.rglob(\"conn.log.labeled\"))\n",
    "src_rows_iot23  = sum(count_iot23_rows(f) for f in iot23_log_files)\n",
    "pq_rows_iot23   = pq.ParquetFile(str(IOT23_PARQUET)).metadata.num_rows\n",
    "pct = pq_rows_iot23 / src_rows_iot23 * 100 if src_rows_iot23 else 0\n",
    "status = \"âœ…\" if pq_rows_iot23 == src_rows_iot23 else (\"âš ï¸ \" if pct > 99 else \"âŒ\")\n",
    "print(f\"   Source logs ({len(iot23_log_files)} files) : {src_rows_iot23:>15,} rows\")\n",
    "print(f\"   Parquet                   : {pq_rows_iot23:>15,} rows\")\n",
    "print(f\"   Coverage                  : {pct:.4f}%  {status}\")\n",
    "if pq_rows_iot23 != src_rows_iot23:\n",
    "    # Small discrepancy is expected: our filter drops #close footer rows\n",
    "    # that a naive line-count includes. Anything >99% and delta explained\n",
    "    # by footer lines (=23, one per file) is acceptable.\n",
    "    delta = src_rows_iot23 - pq_rows_iot23\n",
    "    print(f\"   Delta                     : {delta:+,} rows  \"\n",
    "          f\"(expected â‰¤{len(iot23_log_files)} â€” one #close footer line per file)\")\n",
    "    if delta > len(iot23_log_files):\n",
    "        all_ok = False\n",
    "grand_source  += src_rows_iot23\n",
    "grand_parquet += pq_rows_iot23\n",
    "\n",
    "# â”€â”€ Bot-IoT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nâ–¶  Bot-IoT\")\n",
    "botiot_csv_files = sorted(BOTIOT_DATA_DIR.glob(\"UNSW_2018_IoT_Botnet_Full5pc_*.csv\"))\n",
    "src_rows_botiot  = sum(count_csv_rows(f) for f in botiot_csv_files)\n",
    "pq_rows_botiot   = pq.ParquetFile(str(BOTIOT_PARQUET)).metadata.num_rows\n",
    "pct = pq_rows_botiot / src_rows_botiot * 100\n",
    "status = \"âœ…\" if pq_rows_botiot == src_rows_botiot else (\"âš ï¸ \" if pct > 99 else \"âŒ\")\n",
    "print(f\"   Source CSVs ({len(botiot_csv_files)} files) : {src_rows_botiot:>15,} rows\")\n",
    "print(f\"   Parquet                   : {pq_rows_botiot:>15,} rows\")\n",
    "print(f\"   Coverage                  : {pct:.4f}%  {status}\")\n",
    "if pq_rows_botiot != src_rows_botiot:\n",
    "    all_ok = False\n",
    "    print(f\"   Delta                     : {pq_rows_botiot - src_rows_botiot:+,} rows\")\n",
    "grand_source  += src_rows_botiot\n",
    "grand_parquet += pq_rows_botiot\n",
    "\n",
    "# â”€â”€ Grand Total â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"{'GRAND TOTAL':<30s} {'Source':>15s}   {'Parquet':>15s}\")\n",
    "print(f\"{'â”€'*70}\")\n",
    "print(f\"{'All datasets combined':<30s} {grand_source:>15,}   {grand_parquet:>15,}\")\n",
    "overall_pct = grand_parquet / grand_source * 100 if grand_source else 0\n",
    "print(f\"\\n  Overall coverage: {overall_pct:.4f}%\")\n",
    "print(f\"\\n{'  âœ… ALL DATA ACCOUNTED FOR' if all_ok else '  âš ï¸  INVESTIGATE DELTAS ABOVE'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf931b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 files\n",
      "\n",
      "Test file: c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\iot_23\\CTU-Honeypot-Capture-4-1\\bro\\conn.log.labeled\n",
      "  Line 1: #separator \\x09\n",
      "  Line 2: #set_separator\t,\n",
      "  Line 3: #empty_field\t(empty)\n",
      "  Line 4: #unset_field\t-\n",
      "  Line 5: #path\tconn\n",
      "  Line 6: #open\t2019-12-05-15-46-36\n",
      "  Line 7: #fields\tts\tuid\tid.orig_h\tid.orig_p\tid.resp_h\tid.resp_p\tproto\tservice\tduration\torig_bytes\tresp_bytes\t\n",
      "  Line 8: #types\ttime\tstring\taddr\tport\taddr\tport\tenum\tstring\tinterval\tcount\tcount\tstring\tbool\tbool\tcount\tstrin\n",
      "  Line 9: 1540469302.538640\tCGm6jB4dXK71ZDWUDh\t192.168.1.132\t58687\t216.239.35.4\t123\tudp\t-\t0.114184\t48\t48\tSF\t-\t\n",
      "\n",
      "#fields line: #fields\tts\tuid\tid.orig_h\tid.orig_p\tid.resp_h\tid.resp_p\tproto\tservice\tduration\torig_bytes\tresp_bytes\tconn_state\tlocal_ori\n",
      "Actual cols : 21\n",
      "Expected    : 23\n",
      "Match       : False\n",
      "Actual list : ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', 'local_orig', 'local_resp', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'tunnel_parents   label   detailed-label']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Diagnose: test header validation against first IoT-23 file\n",
    "iot23_log_files = sorted(IOT23_DATA_DIR.rglob(\"conn.log.labeled\"))\n",
    "print(f\"Found {len(iot23_log_files)} files\\n\")\n",
    "\n",
    "test_file = iot23_log_files[0]\n",
    "print(f\"Test file: {test_file}\")\n",
    "\n",
    "with open(test_file, \"r\", encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "    lines = [next(fh).rstrip() for _ in range(9)]\n",
    "\n",
    "for i, l in enumerate(lines, 1):\n",
    "    print(f\"  Line {i}: {l[:100]}\")\n",
    "\n",
    "# Replicate the validation logic\n",
    "with open(test_file, \"r\", encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "    for _ in range(6): next(fh)\n",
    "    fields_line = next(fh).strip()\n",
    "\n",
    "print(f\"\\n#fields line: {fields_line[:120]}\")\n",
    "if fields_line.startswith(\"#fields\"):\n",
    "    actual_cols = fields_line.split(\"\\t\")[1:]\n",
    "    n_expected = len(IOT23_COLS) - 1 + 3\n",
    "    print(f\"Actual cols : {len(actual_cols)}\")\n",
    "    print(f\"Expected    : {n_expected}\")\n",
    "    print(f\"Match       : {len(actual_cols) == n_expected}\")\n",
    "    print(f\"Actual list : {actual_cols}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
