{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63117cc",
   "metadata": {},
   "source": [
    "# Phase 1.4 â€” Knowledge Ingestion into ChromaDB\n",
    "\n",
    "**Objective:** Ingest the verified Phase 1.2 vectors into ChromaDB to build the RAG Knowledge Base,\n",
    "while strictly reserving a 20% \"Live Fire\" dataset for future testing.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "```\n",
    "X_knowledge_vectors_v4.npy  (9,152,557 Ã— 50)\n",
    "X_knowledge_meta_v4.parquet (9,152,557 Ã— 8)\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        Stratified Split  (sklearn)          â”‚\n",
    "â”‚   80% Ingest Set  |  20% Live Set           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚                      â”‚\n",
    "           â–¼                      â–¼\n",
    "    ChromaDB Ingest        live_simulation_data_v4.npy\n",
    "    (rag_ids_knowledge_base)   live_simulation_meta_v4.parquet\n",
    "    cosine / hnsw\n",
    "```\n",
    "\n",
    "| Step | Action | Output |\n",
    "|------|--------|--------|\n",
    "| 1 | Stratified 80/20 split | Ingest Set + Live Set arrays |\n",
    "| 2 | Save Live Set | `live_simulation_data_v4.npy`, `live_simulation_meta_v4.parquet` |\n",
    "| 3 | Verify split disjoint | Assertion pass |\n",
    "| 4 | Init ChromaDB | Persistent collection at `chromadb_store/` |\n",
    "| 5 | Batch ingest (40 K/batch) | `rag_ids_knowledge_base` collection |\n",
    "| 6 | Sanity-check query | Nearest-neighbour from Live Set |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c115b157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… chromadb       1.4.1\n",
      "âœ… scikit-learn   1.7.0\n",
      "âœ… tqdm\n",
      "âœ… numpy          2.1.3\n",
      "âœ… pandas         2.2.3\n",
      "âœ… Python         3.13.9\n",
      "\n",
      "ğŸ“ Path verification:\n",
      "   âœ… VECTORS      c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\processed\\X_knowledge_vectors_v4.npy\n",
      "   âœ… META         c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\processed\\X_knowledge_meta_v4.parquet\n",
      "   âœ… CHROMA_DIR   c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\chromadb_store_v4\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 1 | Imports & Path Configuration\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import sys, os, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "try:\n",
    "    import chromadb\n",
    "    print(f\"âœ… chromadb       {chromadb.__version__}\")\n",
    "except ImportError:\n",
    "    raise SystemExit(\"pip install chromadb\")\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import sklearn\n",
    "    print(f\"âœ… scikit-learn   {sklearn.__version__}\")\n",
    "except ImportError:\n",
    "    raise SystemExit(\"pip install scikit-learn\")\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "    print(f\"âœ… tqdm\")\n",
    "except ImportError:\n",
    "    raise SystemExit(\"pip install tqdm\")\n",
    "\n",
    "print(f\"âœ… numpy          {np.__version__}\")\n",
    "print(f\"âœ… pandas         {pd.__version__}\")\n",
    "print(f\"âœ… Python         {sys.version.split()[0]}\")\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "NOTEBOOK_DIR  = Path.cwd()\n",
    "MAIN_DIR      = NOTEBOOK_DIR.parent\n",
    "PROCESSED_DIR = MAIN_DIR / \"data\" / \"processed\"\n",
    "CHROMA_DIR    = MAIN_DIR / \"chromadb_store_v4\"\n",
    "\n",
    "VECTORS_PATH          = PROCESSED_DIR / \"X_knowledge_vectors_v4.npy\"\n",
    "META_PATH             = PROCESSED_DIR / \"X_knowledge_meta_v4.parquet\"\n",
    "LIVE_VECTORS_PATH     = PROCESSED_DIR / \"live_simulation_data_v4.npy\"\n",
    "LIVE_META_PATH        = PROCESSED_DIR / \"live_simulation_meta_v4.parquet\"\n",
    "\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "COLLECTION_NAME  = \"rag_ids_knowledge_base\"\n",
    "# ChromaDB hard limit is 5461 per batch â€” use 5000 to stay safely under it\n",
    "BATCH_SIZE       = 5_000\n",
    "TEST_SIZE        = 0.20      # 20% live simulation\n",
    "RANDOM_STATE     = 42\n",
    "LABEL_COL        = \"univ_label_multiclass\"\n",
    "\n",
    "LABEL_NAMES = {\n",
    "    0: \"Normal\",\n",
    "    1: \"Reconnaissance\",\n",
    "    2: \"Volumetric_Attack\",\n",
    "    3: \"C2_Botnet\",\n",
    "    4: \"Exploit_and_Theft\",\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“ Path verification:\")\n",
    "for name, p in [\n",
    "    (\"VECTORS\",    VECTORS_PATH),\n",
    "    (\"META\",       META_PATH),\n",
    "    (\"CHROMA_DIR\", CHROMA_DIR),\n",
    "]:\n",
    "    print(f\"   {'âœ…' if p.exists() else 'âŒ'} {name:<12} {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e922a",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Reset â€” Clean ChromaDB Build for v4\n",
    "\n",
    "**Objective:** Delete the existing ChromaDB store (built from v2 vectors - 6.75M vectors) to ensure we build a clean v4 knowledge base from **9.15M v4 vectors**.\n",
    "\n",
    "The existing `chromadb_store/` directory must be removed before ingesting the new v4 vectors to prevent contamination or version conflicts.\n",
    "\n",
    "**Note:** For this v4 rebuild, we will use a new directory `chromadb_store_v4/` to avoid file lock conflicts with the existing v2 store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02999714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‘ï¸  Checking for existing ChromaDB v4 store â€¦\n",
      "   Found: c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\chromadb_store_v4  (0.000 GB)\n",
      "   âš ï¸  Unable to delete (file locks): [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\Users\\\\suhas\\\\OneDrive\\\\Desktop\\\\Capstone\\\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\\\main_folder\\\\chromadb_store_v4\\\\chroma.sqlite3'\n",
      "   â„¹ï¸  Will create new directory - collection delete in Step 5 will ensure clean state\n",
      "   âœ… Directory ready: c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\chromadb_store_v4\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 1.5 | Delete Existing ChromaDB Store\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import shutil\n",
    "\n",
    "print(\"ğŸ—‘ï¸  Checking for existing ChromaDB v4 store â€¦\")\n",
    "\n",
    "# Using chromadb_store_v4 to avoid file locks from v2 store\n",
    "if CHROMA_DIR.exists():\n",
    "    total_size = sum(f.stat().st_size for f in CHROMA_DIR.rglob(\"*\") if f.is_file())\n",
    "    print(f\"   Found: {CHROMA_DIR}  ({total_size / 1e9:.3f} GB)\")\n",
    "    try:\n",
    "        shutil.rmtree(CHROMA_DIR)\n",
    "        print(f\"   âœ… Deleted â€” clean slate for v4\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"   âš ï¸  Unable to delete (file locks): {e}\")\n",
    "        print(f\"   â„¹ï¸  Will create new directory - collection delete in Step 5 will ensure clean state\")\n",
    "else:\n",
    "    print(f\"   â­ï¸  No existing v4 store found â€” clean start\")\n",
    "\n",
    "# Ensure directory exists\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"   âœ… Directory ready: {CHROMA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b0933",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 â€” Load Input Artifacts\n",
    "\n",
    "Load `X_knowledge_vectors_v4.npy` and `X_knowledge_meta_v4.parquet` produced by Phase 1.2 v4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d85993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Loading artifacts â€¦\n",
      "   âœ… X_knowledge       shape=(9152557, 50)  dtype=float32  size=1.831 GB\n",
      "   âœ… meta_knowledge    shape=(9152557, 8)  size=70.9 MB\n",
      "\n",
      "   âœ… Row alignment confirmed: 9,152,557 rows\n",
      "   âœ… Loaded in 8.2s\n",
      "\n",
      "   Metadata columns : ['univ_label_multiclass', 'univ_label_str', 'univ_specific_attack', 'meta_src_ip', 'meta_dst_ip', 'meta_timestamp', 'dataset_source', 'univ_proto']\n",
      "\n",
      "   dtype overview   :\n",
      "univ_label_multiclass       int8\n",
      "univ_label_str            object\n",
      "univ_specific_attack      object\n",
      "meta_src_ip               object\n",
      "meta_dst_ip               object\n",
      "meta_timestamp           float64\n",
      "dataset_source            object\n",
      "univ_proto                object\n",
      "\n",
      "ğŸ“Š Full class distribution:\n",
      "   0 Normal                    1,018,128  (11.12%)\n",
      "   1 Reconnaissance            2,526,173  (27.60%)\n",
      "   2 Volumetric_Attack           307,229  ( 3.36%)\n",
      "   3 C2_Botnet                   438,786  ( 4.79%)\n",
      "   4 Exploit_and_Theft         4,862,241  (53.12%)\n",
      "\n",
      "   Sample metadata (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>univ_label_multiclass</th>\n",
       "      <th>univ_label_str</th>\n",
       "      <th>univ_specific_attack</th>\n",
       "      <th>meta_src_ip</th>\n",
       "      <th>meta_dst_ip</th>\n",
       "      <th>meta_timestamp</th>\n",
       "      <th>dataset_source</th>\n",
       "      <th>univ_proto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>3.122.49.24</td>\n",
       "      <td>192.168.1.152</td>\n",
       "      <td>1.554198e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>tcp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>3.122.49.24</td>\n",
       "      <td>192.168.1.152</td>\n",
       "      <td>1.554220e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>tcp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>3.122.49.24</td>\n",
       "      <td>192.168.1.152</td>\n",
       "      <td>1.554241e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>tcp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   univ_label_multiclass univ_label_str univ_specific_attack  meta_src_ip  \\\n",
       "0                      0         Normal               normal  3.122.49.24   \n",
       "1                      0         Normal               normal  3.122.49.24   \n",
       "2                      0         Normal               normal  3.122.49.24   \n",
       "\n",
       "     meta_dst_ip  meta_timestamp dataset_source univ_proto  \n",
       "0  192.168.1.152    1.554198e+09           None        tcp  \n",
       "1  192.168.1.152    1.554220e+09           None        tcp  \n",
       "2  192.168.1.152    1.554241e+09           None        tcp  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 2 | Load Input Artifacts\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ“¦ Loading artifacts â€¦\")\n",
    "t0 = time.time()\n",
    "\n",
    "# â”€â”€ Vectors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_knowledge = np.load(str(VECTORS_PATH))\n",
    "print(f\"   âœ… X_knowledge       shape={X_knowledge.shape}  \"\n",
    "      f\"dtype={X_knowledge.dtype}  \"\n",
    "      f\"size={VECTORS_PATH.stat().st_size / 1e9:.3f} GB\")\n",
    "\n",
    "# â”€â”€ Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "meta_knowledge = pd.read_parquet(str(META_PATH))\n",
    "print(f\"   âœ… meta_knowledge    shape={meta_knowledge.shape}  \"\n",
    "      f\"size={META_PATH.stat().st_size / 1e6:.1f} MB\")\n",
    "\n",
    "# â”€â”€ Alignment check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "assert X_knowledge.shape[0] == len(meta_knowledge), (\n",
    "    f\"ALIGNMENT ERROR: vectors={X_knowledge.shape[0]} meta={len(meta_knowledge)}\"\n",
    ")\n",
    "print(f\"\\n   âœ… Row alignment confirmed: {X_knowledge.shape[0]:,} rows\")\n",
    "print(f\"   âœ… Loaded in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# â”€â”€ Preview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n   Metadata columns : {list(meta_knowledge.columns)}\")\n",
    "print(f\"\\n   dtype overview   :\")\n",
    "print(meta_knowledge.dtypes.to_string())\n",
    "\n",
    "print(f\"\\nğŸ“Š Full class distribution:\")\n",
    "total = len(meta_knowledge)\n",
    "for lbl, cnt in meta_knowledge[LABEL_COL].value_counts().sort_index().items():\n",
    "    name = LABEL_NAMES.get(int(lbl), str(lbl))\n",
    "    print(f\"   {int(lbl)} {name:<24} {cnt:>10,}  ({cnt/total*100:5.2f}%)\")\n",
    "\n",
    "print(f\"\\n   Sample metadata (first 3 rows):\")\n",
    "display(meta_knowledge.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1d67c",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 â€” Stratified Split: 80% Ingest Set / 20% Live Simulation Set\n",
    "\n",
    "`train_test_split` with `stratify=univ_label_multiclass` guarantees each class is\n",
    "proportionally represented in both halves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f847d79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸  Performing stratified 80/20 split â€¦\n",
      "\n",
      "   Ingest Set  :  7,322,045 vectors  (80.0%)\n",
      "   Live Set    :  1,830,512 vectors  (20.0%)\n",
      "   Total       :  9,152,557 == 9,152,557  âœ…\n",
      "\n",
      "   Split completed in 6.2s\n",
      "\n",
      "ğŸ“Š Ingest Set class distribution:\n",
      "   0 Normal                      814,502  (11.12%)\n",
      "   1 Reconnaissance            2,020,938  (27.60%)\n",
      "   2 Volumetric_Attack           245,783  ( 3.36%)\n",
      "   3 C2_Botnet                   351,029  ( 4.79%)\n",
      "   4 Exploit_and_Theft         3,889,793  (53.12%)\n",
      "\n",
      "ğŸ“Š Live Set class distribution:\n",
      "   0 Normal                      203,626  (11.12%)\n",
      "   1 Reconnaissance              505,235  (27.60%)\n",
      "   2 Volumetric_Attack            61,446  ( 3.36%)\n",
      "   3 C2_Botnet                    87,757  ( 4.79%)\n",
      "   4 Exploit_and_Theft           972,448  (53.12%)\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 3 | Stratified Split\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"âœ‚ï¸  Performing stratified 80/20 split â€¦\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Generate indices for the full dataset\n",
    "all_indices = np.arange(len(meta_knowledge))\n",
    "labels      = meta_knowledge[LABEL_COL].values.astype(int)\n",
    "\n",
    "ingest_idx, live_idx = train_test_split(\n",
    "    all_indices,\n",
    "    test_size    = TEST_SIZE,\n",
    "    stratify     = labels,\n",
    "    random_state = RANDOM_STATE,\n",
    "    shuffle      = True,\n",
    ")\n",
    "\n",
    "# â”€â”€ Slice vectors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_ingest = X_knowledge[ingest_idx]\n",
    "X_live   = X_knowledge[live_idx]\n",
    "\n",
    "# â”€â”€ Slice metadata â€” preserve original DataFrame index for traceability â”€â”€â”€â”€â”€â”€â”€\n",
    "meta_ingest = meta_knowledge.iloc[ingest_idx].reset_index(drop=True)\n",
    "meta_live   = meta_knowledge.iloc[live_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n   Ingest Set  : {len(X_ingest):>10,} vectors  ({len(X_ingest)/len(X_knowledge)*100:.1f}%)\")\n",
    "print(f\"   Live Set    : {len(X_live):>10,} vectors  ({len(X_live)/len(X_knowledge)*100:.1f}%)\")\n",
    "print(f\"   Total       : {len(X_ingest)+len(X_live):>10,} == {len(X_knowledge):,}  âœ…\")\n",
    "print(f\"\\n   Split completed in {time.time()-t0:.1f}s\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Ingest Set class distribution:\")\n",
    "total_i = len(meta_ingest)\n",
    "for lbl, cnt in meta_ingest[LABEL_COL].value_counts().sort_index().items():\n",
    "    name = LABEL_NAMES.get(int(lbl), str(lbl))\n",
    "    print(f\"   {int(lbl)} {name:<24} {cnt:>10,}  ({cnt/total_i*100:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Live Set class distribution:\")\n",
    "total_l = len(meta_live)\n",
    "for lbl, cnt in meta_live[LABEL_COL].value_counts().sort_index().items():\n",
    "    name = LABEL_NAMES.get(int(lbl), str(lbl))\n",
    "    print(f\"   {int(lbl)} {name:<24} {cnt:>10,}  ({cnt/total_l*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648dcf25",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 â€” Save Live Simulation Set\n",
    "\n",
    "The 20% Live Set is written to disk **immediately** and is never touched again during ingestion.\n",
    "It will serve as unseen \"live traffic\" for Phase 2 RAG retrieval evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58159ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving Live Simulation Set â€¦\n",
      "   âœ… live_simulation_data_v4.npy          shape=(1830512, 50)  (366.1 MB)\n",
      "   âœ… live_simulation_meta_v4.parquet      shape=(1830512, 8)  (29.2 MB)\n",
      "\n",
      "   âœ… Live Set saved in 1.5s\n",
      "\n",
      "   ğŸ”’ Live Set is now ON DISK â€” will not be referenced again during ingestion.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 4 | Save Live Simulation Set\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ’¾ Saving Live Simulation Set â€¦\")\n",
    "t0 = time.time()\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Vectors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.save(str(LIVE_VECTORS_PATH), X_live)\n",
    "live_vec_size = LIVE_VECTORS_PATH.stat().st_size\n",
    "print(f\"   âœ… {LIVE_VECTORS_PATH.name:<35}  \"\n",
    "      f\"shape={X_live.shape}  \"\n",
    "      f\"({live_vec_size / 1e6:,.1f} MB)\")\n",
    "\n",
    "# â”€â”€ Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "meta_live.to_parquet(str(LIVE_META_PATH), index=False)\n",
    "live_meta_size = LIVE_META_PATH.stat().st_size\n",
    "print(f\"   âœ… {LIVE_META_PATH.name:<35}  \"\n",
    "      f\"shape={meta_live.shape}  \"\n",
    "      f\"({live_meta_size / 1e6:,.1f} MB)\")\n",
    "\n",
    "print(f\"\\n   âœ… Live Set saved in {time.time()-t0:.1f}s\")\n",
    "print(f\"\\n   ğŸ”’ Live Set is now ON DISK â€” will not be referenced again during ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ee01a",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 â€” Verify Split Integrity\n",
    "\n",
    "Three hard assertions:\n",
    "1. Ingest âˆ© Live = âˆ… (sets are disjoint)\n",
    "2. |Ingest| + |Live| = |Total|\n",
    "3. Both splits reproduce the expected class proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17bb39ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Verifying split integrity â€¦\n",
      "   âœ… Assertion 1 â€” Disjoint  : Ingest âˆ© Live = âˆ…  (overlap=0)\n",
      "   âœ… Assertion 2 â€” Totals    : 7,322,045 + 1,830,512 = 9,152,557\n",
      "\n",
      "   âœ… Assertion 3 â€” Stratification proportions:\n",
      "   Class                           Base%   Ingest%    Live%    Max Î”\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€\n",
      "   âœ… 0 Normal                     11.12%    11.12%   11.12%  0.0000\n",
      "   âœ… 1 Reconnaissance             27.60%    27.60%   27.60%  0.0000\n",
      "   âœ… 2 Volumetric_Attack           3.36%     3.36%    3.36%  0.0000\n",
      "   âœ… 3 C2_Botnet                   4.79%     4.79%    4.79%  0.0000\n",
      "   âœ… 4 Exploit_and_Theft          53.12%    53.12%   53.12%  0.0000\n",
      "\n",
      "   âœ… Max stratification drift: 0.0000%  (< 2.0% threshold)\n",
      "\n",
      "âœ… All integrity checks passed â€” safe to proceed with ingestion.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 5 | Verify Split Integrity\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ” Verifying split integrity â€¦\")\n",
    "\n",
    "ingest_set = set(ingest_idx.tolist())\n",
    "live_set   = set(live_idx.tolist())\n",
    "\n",
    "# â”€â”€ Assertion 1: Disjoint sets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "overlap = ingest_set & live_set\n",
    "assert len(overlap) == 0, f\"CRITICAL: {len(overlap)} overlapping indices found!\"\n",
    "print(f\"   âœ… Assertion 1 â€” Disjoint  : Ingest âˆ© Live = âˆ…  (overlap={len(overlap)})\")\n",
    "\n",
    "# â”€â”€ Assertion 2: Total count â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "assert len(ingest_set) + len(live_set) == len(X_knowledge), (\n",
    "    f\"Count mismatch: {len(ingest_set)}+{len(live_set)} != {len(X_knowledge)}\"\n",
    ")\n",
    "print(f\"   âœ… Assertion 2 â€” Totals    : {len(ingest_set):,} + {len(live_set):,} = {len(X_knowledge):,}\")\n",
    "\n",
    "# â”€â”€ Assertion 3: Approximate class proportions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n   âœ… Assertion 3 â€” Stratification proportions:\")\n",
    "print(f\"   {'Class':<28}  {'Base%':>7}  {'Ingest%':>8}  {'Live%':>7}  {'Max Î”':>7}\")\n",
    "print(f\"   {'â”€'*28}  {'â”€'*7}  {'â”€'*8}  {'â”€'*7}  {'â”€'*7}\")\n",
    "\n",
    "max_delta = 0.0\n",
    "for lbl in sorted(LABEL_NAMES.keys()):\n",
    "    base_pct   = (labels == lbl).mean() * 100\n",
    "    ingest_pct = (meta_ingest[LABEL_COL] == lbl).mean() * 100\n",
    "    live_pct   = (meta_live[LABEL_COL]   == lbl).mean() * 100\n",
    "    delta      = max(abs(ingest_pct - base_pct), abs(live_pct - base_pct))\n",
    "    max_delta  = max(max_delta, delta)\n",
    "    flag       = \"âœ…\" if delta < 0.5 else \"âš ï¸ \"\n",
    "    name       = LABEL_NAMES[lbl]\n",
    "    print(f\"   {flag} {lbl} {name:<24}  {base_pct:>6.2f}%  {ingest_pct:>7.2f}%  {live_pct:>6.2f}%  {delta:>6.4f}\")\n",
    "\n",
    "assert max_delta < 2.0, f\"Stratification drift too large: {max_delta:.4f}%\"\n",
    "print(f\"\\n   âœ… Max stratification drift: {max_delta:.4f}%  (< 2.0% threshold)\")\n",
    "print(f\"\\nâœ… All integrity checks passed â€” safe to proceed with ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf981f2",
   "metadata": {},
   "source": [
    "---\n",
    "## Minimum Viable Standard (MVS) â€” Acceptance Criteria\n",
    "\n",
    "**Context:** This v4 rebuild establishes a **new performance baseline** measured against **absolute thresholds**, not relative improvement over v2.\n",
    "\n",
    "### Acceptance Thresholds (Phase 2 RAG Retrieval):\n",
    "\n",
    "| Metric | Threshold | Rationale |\n",
    "|--------|-----------|-----------|\n",
    "| **Retrieval Latency** | < 50ms per query | Must support real-time IDS (industrial standard: sub-100ms) |\n",
    "| **Normal Traffic Accuracy (Class 0)** | â‰¥ **85%** | Avoid false alarm fatigue; v2's 55% is categorized as **FAILURE** |\n",
    "| **Attack Recall (Class 1-4)** | â‰¥ **95%** | Mission-critical: missed attacks = security breach |\n",
    "| **Rare Variant Retention** | 100% | Verified in Phase 1.3 v4 (all 9 Exploit attack types present) |\n",
    "\n",
    "### Key Changes from v2:\n",
    "\n",
    "- **Normal Traffic Boost:** 422k â†’ 1.02M vectors (11.1% of KB) â€” addresses v2's \"Normal Traffic Starvation\"\n",
    "- **Volumetric Fix:** 13.88M (v3 explosion) â†’ 307k vectors (3.4% of KB) â€” `src_ip` grouping restored\n",
    "- **Total KB Size:** 9.15M vectors (vs 8.4M v2, 22.7M v3)\n",
    "\n",
    "**Acceptance Decision:**  \n",
    "If Phase 2 testing shows Normal Accuracy < 85% or Attack Recall < 95%, the v4 KB is rejected and Phase 1.2 must be re-engineered with adjusted reduction hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96991c27",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 â€” Initialize ChromaDB Persistent Client & Collection\n",
    "\n",
    "- **Store path:** `main_folder/chromadb_store_v4/`\n",
    "- **Collection:** `rag_ids_knowledge_base`\n",
    "- **Distance metric:** `cosine` (set via `hnsw:space`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4438173b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—„ï¸  Initializing ChromaDB â€¦\n",
      "   Store path : c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\chromadb_store_v4\n",
      "   âš ï¸  Existing collection 'rag_ids_knowledge_base' deleted â€” fresh ingest.\n",
      "\n",
      "   âœ… Collection created : 'rag_ids_knowledge_base'\n",
      "   Distance metric      : cosine  (hnsw:space)\n",
      "   Embedding dimension  : 50\n",
      "   Current item count   : 0\n",
      "\n",
      "   ChromaDB store contents:\n",
      "     chroma.sqlite3\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 6 | Initialize ChromaDB Persistent Client & Collection\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ—„ï¸  Initializing ChromaDB â€¦\")\n",
    "print(f\"   Store path : {CHROMA_DIR}\")\n",
    "\n",
    "client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "\n",
    "# â”€â”€ Drop existing collection if re-running (idempotent) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "existing = [c.name for c in client.list_collections()]\n",
    "if COLLECTION_NAME in existing:\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "    print(f\"   âš ï¸  Existing collection '{COLLECTION_NAME}' deleted â€” fresh ingest.\")\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name     = COLLECTION_NAME,\n",
    "    metadata = {\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "print(f\"\\n   âœ… Collection created : '{collection.name}'\")\n",
    "print(f\"   Distance metric      : cosine  (hnsw:space)\")\n",
    "print(f\"   Embedding dimension  : {X_ingest.shape[1]}\")\n",
    "print(f\"   Current item count   : {collection.count()}\")\n",
    "\n",
    "# â”€â”€ Collection info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n   ChromaDB store contents:\")\n",
    "for fname in sorted(CHROMA_DIR.iterdir()):\n",
    "    print(f\"     {fname.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd199367",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6 â€” Batch Ingestion into ChromaDB\n",
    "\n",
    "Ingest the **80% Ingest Set** in batches of 40,000 vectors.  \n",
    "Each document stores 5 metadata fields: `label`, `attack_type`, `src_ip`, `proto`, `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b20db0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting ChromaDB ingestion â€¦\n",
      "   Total vectors :  7,322,045\n",
      "   Batch size    :      5,000\n",
      "   Num batches   :      1,465\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5a02d776ec489d84b7a8e909311a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ingesting batches:   0%|          | 0/1465 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Ingestion complete!\n",
      "   Vectors ingested :  7,322,045\n",
      "   Elapsed          :   165.6 min  (9939s)\n",
      "   Throughput       :        737 vectors/s\n",
      "   Collection count :  7,322,045\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 7 | Batch Ingestion (The Heavy Lift)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "n_ingest     = len(X_ingest)\n",
    "n_batches    = (n_ingest + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "print(f\"ğŸš€ Starting ChromaDB ingestion â€¦\")\n",
    "print(f\"   Total vectors : {n_ingest:>10,}\")\n",
    "print(f\"   Batch size    : {BATCH_SIZE:>10,}\")\n",
    "print(f\"   Num batches   : {n_batches:>10,}\")\n",
    "print()\n",
    "\n",
    "# â”€â”€ Pre-cache metadata columns as numpy arrays for fast slicing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "labels_arr   = meta_ingest[LABEL_COL].values.astype(int)\n",
    "attack_arr   = meta_ingest[\"univ_specific_attack\"].fillna(\"unknown\").astype(str).values\n",
    "src_ip_arr   = meta_ingest[\"meta_src_ip\"].fillna(\"0.0.0.0\").astype(str).values\n",
    "proto_arr    = meta_ingest[\"univ_proto\"].fillna(\"unknown\").astype(str).values\n",
    "dataset_arr  = meta_ingest[\"dataset_source\"].fillna(\"unknown\").astype(str).values\n",
    "\n",
    "# â”€â”€ Sanity check: none of the string fields have nulls left â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for arr_name, arr in [(\"attack_arr\", attack_arr), (\"src_ip_arr\", src_ip_arr),\n",
    "                      (\"proto_arr\", proto_arr), (\"dataset_arr\", dataset_arr)]:\n",
    "    nones = int(np.sum(arr == \"None\") + np.sum(arr == \"nan\"))\n",
    "    if nones > 0:\n",
    "        print(f\"   âš ï¸  {arr_name}: {nones} 'None'/'nan' strings found â€” will be kept as 'unknown'\")\n",
    "\n",
    "t0 = time.time()\n",
    "total_ingested = 0\n",
    "\n",
    "for batch_num in tqdm(range(n_batches), desc=\"Ingesting batches\", unit=\"batch\"):\n",
    "    start = batch_num * BATCH_SIZE\n",
    "    end   = min(start + BATCH_SIZE, n_ingest)\n",
    "\n",
    "    batch_vectors = X_ingest[start:end].astype(np.float32)\n",
    "\n",
    "    # â”€â”€ IDs: unique string per vector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    batch_ids = [f\"v{ingest_idx[i]}\" for i in range(start, end)]\n",
    "\n",
    "    # â”€â”€ Metadata dicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    batch_meta = [\n",
    "        {\n",
    "            \"label\":       int(labels_arr[i]),\n",
    "            \"attack_type\": str(attack_arr[i]),\n",
    "            \"src_ip\":      str(src_ip_arr[i]),\n",
    "            \"proto\":       str(proto_arr[i]),\n",
    "            \"dataset\":     str(dataset_arr[i]),\n",
    "        }\n",
    "        for i in range(start, end)\n",
    "    ]\n",
    "\n",
    "    # â”€â”€ Convert vectors to Python lists (ChromaDB requirement) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    batch_embeddings = batch_vectors.tolist()\n",
    "\n",
    "    collection.add(\n",
    "        embeddings = batch_embeddings,\n",
    "        metadatas  = batch_meta,\n",
    "        ids        = batch_ids,\n",
    "    )\n",
    "    total_ingested += (end - start)\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "rate    = total_ingested / elapsed\n",
    "\n",
    "print(f\"\\nâœ… Ingestion complete!\")\n",
    "print(f\"   Vectors ingested : {total_ingested:>10,}\")\n",
    "print(f\"   Elapsed          : {elapsed/60:>7.1f} min  ({elapsed:.0f}s)\")\n",
    "print(f\"   Throughput       : {rate:>10,.0f} vectors/s\")\n",
    "print(f\"   Collection count : {collection.count():>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7323393e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7 â€” Verification: Collection Count & Sanity-Check Query\n",
    "\n",
    "1. Confirm the final ChromaDB item count matches the Ingest Set size.  \n",
    "2. Pick **5 random vectors** from the **v4 Live Set** (never ingested) and query for their nearest neighbours.  \n",
    "   Small cosine distances confirm the collection is correctly indexed and searchable.\n",
    "3. Flag any distance > 0.1 for same-class matches as potential indexing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23f5104b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "  VERIFICATION\n",
      "=================================================================\n",
      "\n",
      "  [A] Collection item count:\n",
      "      Expected :  7,322,045\n",
      "      Actual   :  7,322,045\n",
      "      âœ… Count matches!\n",
      "\n",
      "  [B] ChromaDB store size: 6.860 GB  (c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\chromadb_store_v4)\n",
      "\n",
      "  [C] Sanity-check query (5 random Live Set vectors â†’ nearest neighbours):\n",
      "\n",
      "      Query 1/5:\n",
      "        Index (in Live Set) : 1,144,242\n",
      "        True label          : 4 (Exploit_and_Theft)\n",
      "        Attack type         : xss\n",
      "        Distance            : -0.000000  (cosine; 0=identical, 2=opposite)\n",
      "        Latency             : 2.4 ms\n",
      "        NN class            : Exploit_and_Theft\n",
      "        Label match         : âœ… YES\n",
      "\n",
      "      Query 2/5:\n",
      "        Index (in Live Set) : 1,252,398\n",
      "        True label          : 4 (Exploit_and_Theft)\n",
      "        Attack type         : xss\n",
      "        Distance            : 0.000000  (cosine; 0=identical, 2=opposite)\n",
      "        Latency             : 1.5 ms\n",
      "        NN class            : Exploit_and_Theft\n",
      "        Label match         : âœ… YES\n",
      "\n",
      "      Query 3/5:\n",
      "        Index (in Live Set) : 1,729,656\n",
      "        True label          : 4 (Exploit_and_Theft)\n",
      "        Attack type         : password\n",
      "        Distance            : -0.000000  (cosine; 0=identical, 2=opposite)\n",
      "        Latency             : 1.1 ms\n",
      "        NN class            : Exploit_and_Theft\n",
      "        Label match         : âœ… YES\n",
      "\n",
      "      Query 4/5:\n",
      "        Index (in Live Set) : 1,058,571\n",
      "        True label          : 0 (Normal)\n",
      "        Attack type         : <absent>\n",
      "        Distance            : 0.000000  (cosine; 0=identical, 2=opposite)\n",
      "        Latency             : 0.7 ms\n",
      "        NN class            : Normal\n",
      "        Label match         : âœ… YES\n",
      "\n",
      "      Query 5/5:\n",
      "        Index (in Live Set) : 1,642,359\n",
      "        True label          : 3 (C2_Botnet)\n",
      "        Attack type         : Okiru\n",
      "        Distance            : -0.000000  (cosine; 0=identical, 2=opposite)\n",
      "        Latency             : 0.6 ms\n",
      "        NN class            : C2_Botnet\n",
      "        Label match         : âœ… YES\n",
      "\n",
      "      â”€â”€ Summary Statistics â”€â”€\n",
      "      Avg distance    : -0.000000\n",
      "      Avg latency     : 1.3 ms\n",
      "      Distance range  : [-0.000000, 0.000000]\n",
      "\n",
      "=================================================================\n",
      "  PHASE 1.4 SUMMARY\n",
      "=================================================================\n",
      "  Total vectors                       :    9,152,557\n",
      "  Ingest Set (80%)                    :    7,322,045\n",
      "  Live Set (20%)                      :    1,830,512\n",
      "  ChromaDB collection count           :    7,322,045\n",
      "  ChromaDB store size                 :       6.860 GB\n",
      "  Avg sanity-check distance           :    -0.000000\n",
      "  Avg sanity-check latency            :         1.3 ms\n",
      "=================================================================\n",
      "\n",
      "âœ… Phase 1.4 complete â€” ChromaDB knowledge base is ready for RAG!\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 8 | Verification: Collection Count & Sanity-Check Query\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"  VERIFICATION\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# â”€â”€ A: Collection count â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "final_count = collection.count()\n",
    "expected    = len(X_ingest)\n",
    "\n",
    "print(f\"\\n  [A] Collection item count:\")\n",
    "print(f\"      Expected : {expected:>10,}\")\n",
    "print(f\"      Actual   : {final_count:>10,}\")\n",
    "assert final_count == expected, (\n",
    "    f\"COUNT MISMATCH: expected {expected}, got {final_count}\"\n",
    ")\n",
    "print(f\"      âœ… Count matches!\")\n",
    "\n",
    "# â”€â”€ B: ChromaDB store size on disk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "total_bytes = sum(f.stat().st_size for f in CHROMA_DIR.rglob(\"*\") if f.is_file())\n",
    "print(f\"\\n  [B] ChromaDB store size: {total_bytes / 1e9:.3f} GB  ({CHROMA_DIR})\")\n",
    "\n",
    "# â”€â”€ C: Sanity-check query with 5 random Live Set vectors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n  [C] Sanity-check query (5 random Live Set vectors â†’ nearest neighbours):\")\n",
    "\n",
    "rng = np.random.default_rng(seed=7)\n",
    "n_test_queries = 5\n",
    "query_indices = rng.choice(len(X_live), size=n_test_queries, replace=False)\n",
    "\n",
    "avg_latency = 0.0\n",
    "all_distances = []\n",
    "\n",
    "for i, query_idx in enumerate(query_indices):\n",
    "    query_vec  = X_live[query_idx].astype(np.float32).tolist()\n",
    "    query_meta = meta_live.iloc[query_idx]\n",
    "    \n",
    "    print(f\"\\n      Query {i+1}/{n_test_queries}:\")\n",
    "    print(f\"        Index (in Live Set) : {query_idx:,}\")\n",
    "    print(f\"        True label          : {int(query_meta[LABEL_COL])} \"\n",
    "          f\"({LABEL_NAMES.get(int(query_meta[LABEL_COL]), '?')})\")\n",
    "    print(f\"        Attack type         : {query_meta['univ_specific_attack']}\")\n",
    "    \n",
    "    # â”€â”€ Run query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    t_q = time.time()\n",
    "    results = collection.query(\n",
    "        query_embeddings = [query_vec],\n",
    "        n_results        = 1,\n",
    "        include          = [\"metadatas\", \"distances\"],\n",
    "    )\n",
    "    latency_ms = (time.time() - t_q) * 1000\n",
    "    avg_latency += latency_ms\n",
    "    \n",
    "    distance = results['distances'][0][0]\n",
    "    all_distances.append(distance)\n",
    "    nn_meta = results[\"metadatas\"][0][0]\n",
    "    nn_label = nn_meta.get(\"label\", -1)\n",
    "    label_match = nn_label == int(query_meta[LABEL_COL])\n",
    "    \n",
    "    print(f\"        Distance            : {distance:.6f}  (cosine; 0=identical, 2=opposite)\")\n",
    "    print(f\"        Latency             : {latency_ms:.1f} ms\")\n",
    "    print(f\"        NN class            : {LABEL_NAMES.get(nn_label, '?')}\")\n",
    "    print(f\"        Label match         : {'âœ… YES' if label_match else 'âš ï¸  NO (cross-class NN)'}\")\n",
    "    \n",
    "    # â”€â”€ Flag potential indexing errors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if distance > 0.1 and label_match:\n",
    "        print(f\"        âš ï¸  HIGH DISTANCE for same-class match â€” potential indexing issue!\")\n",
    "\n",
    "avg_latency /= n_test_queries\n",
    "avg_distance = sum(all_distances) / len(all_distances)\n",
    "\n",
    "print(f\"\\n      â”€â”€ Summary Statistics â”€â”€\")\n",
    "print(f\"      Avg distance    : {avg_distance:.6f}\")\n",
    "print(f\"      Avg latency     : {avg_latency:.1f} ms\")\n",
    "print(f\"      Distance range  : [{min(all_distances):.6f}, {max(all_distances):.6f}]\")\n",
    "\n",
    "# â”€â”€ Summary table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n{'='*65}\")\n",
    "print(f\"  PHASE 1.4 SUMMARY\")\n",
    "print(f\"{'='*65}\")\n",
    "print(f\"  {'Total vectors':<35} : {len(X_knowledge):>12,}\")\n",
    "print(f\"  {'Ingest Set (80%)':<35} : {len(X_ingest):>12,}\")\n",
    "print(f\"  {'Live Set (20%)':<35} : {len(X_live):>12,}\")\n",
    "print(f\"  {'ChromaDB collection count':<35} : {final_count:>12,}\")\n",
    "print(f\"  {'ChromaDB store size':<35} : {total_bytes/1e9:>11.3f} GB\")\n",
    "print(f\"  {'Avg sanity-check distance':<35} : {avg_distance:>12.6f}\")\n",
    "print(f\"  {'Avg sanity-check latency':<35} : {avg_latency:>11.1f} ms\")\n",
    "print(f\"{'='*65}\")\n",
    "print(f\"\\nâœ… Phase 1.4 complete â€” ChromaDB knowledge base is ready for RAG!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768f8dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â­ï¸  Skipping large binary (>100 MB): live_simulation_data_v4.npy\n",
      "ğŸ“‹ Staging Phase 1.4 artifacts â€¦\n",
      "   âœ… staged  main_folder/Phase_1/Phase_1_4_Knowledge_Ingestion.ipynb\n",
      "   âœ… staged  main_folder/data/processed/live_simulation_meta_v4.parquet\n",
      "âš ï¸  git commit -m returned 1\n",
      "\n",
      "\n",
      "=================================================================\n",
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 13 commits.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add/rm <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   main_folder/Phase_0/Phase_0_1_TON_IoT_Data_Understanding.ipynb\n",
      "\tmodified:   main_folder/Phase_0/Phase_0_2_IoT23_Data_Understanding.ipynb\n",
      "\tmodified:   main_folder/Phase_1/Phase_1_1_Universal_Schema_Alignment.ipynb\n",
      "\tdeleted:    main_folder/Phase_1/Phase_1_2_Preprocessing_Normalization.ipynb\n",
      "\tmodified:   main_folder/Phase_1/Phase_1_2_Vectorization_and_Smart_Reduction.ipynb\n",
      "\tmodified:   main_folder/Phase_1/Phase_1_3_Deep_Validation.ipynb\n",
      "\tmodified:   main_folder/Phase_2/Phase_2_2_Adaptive_Window.ipynb\n",
      "\tdeleted:    main_folder/artifacts/Phase_0_1_TON_IoT_Data_Understanding_Report.md\n",
      "\tdeleted:    main_folder/artifacts/Phase_0_2_IoT23_Data_Understanding_Report.md\n",
      "\tdeleted:    main_folder/artifacts/Phase_0_3_Bot_IoT_Data_Understanding_Report.md\n",
      "\tdeleted:    main_folder/artifacts/botiot_fullscan_label_distribution.csv\n",
      "\tdeleted:    main_folder/artifacts/botiot_fullscan_numerical_stats.csv\n",
      "\tdeleted:    main_folder/artifacts/botiot_fullscan_row_counts.csv\n",
      "\tdeleted:    main_folder/artifacts/botiot_phase0_column_inventory.csv\n",
      "\tdeleted:    main_folder/artifacts/botiot_phase0_feature_meanings.csv\n",
      "\tdeleted:    main_folder/artifacts/botiot_phase0_file_inventory.csv\n",
      "\tdeleted:    main_folder/artifacts/botiot_phase0_numerical_stats.csv\n",
      "\tdeleted:    main_folder/artifacts/botiot_phase0_role_classification.csv\n",
      "\tdeleted:    main_folder/artifacts/botiot_phase0_sentinel_analysis.csv\n",
      "\tdeleted:    main_folder/artifacts/iot23_fullscan_label_distribution.csv\n",
      "\tdeleted:    main_folder/artifacts/iot23_fullscan_numerical_stats.csv\n",
      "\tdeleted:    main_folder/artifacts/iot23_fullscan_row_counts.csv\n",
      "\tdeleted:    main_folder/artifacts/iot23_phase0_column_inventory.csv\n",
      "\tdeleted:    main_folder/artifacts/iot23_phase0_feature_meanings.csv\n",
      "\tdeleted:    main_folder/artifacts/iot23_phase0_file_inventory.csv\n",
      "\tdeleted:    main_folder/artifacts/iot23_phase0_numerical_stats.csv\n",
      "\tdeleted:    main_folder/artifacts/iot23_phase0_role_classification.csv\n",
      "\tdeleted:    main_folder/artifacts/iot23_phase0_sentinel_analysis.csv\n",
      "\tdeleted:    main_folder/artifacts/phase1_1_column_registry.csv\n",
      "\tdeleted:    main_folder/artifacts/phase1_1_label_distribution.csv\n",
      "\tdeleted:    main_folder/artifacts/phase1_1_processing_stats.csv\n",
      "\tdeleted:    main_folder/artifacts/phase1_1_schema_manifest.json\n",
      "\tdeleted:    main_folder/artifacts/phase1_2_feature_names.json\n",
      "\tdeleted:    main_folder/artifacts/toniot_phase0_column_inventory.csv\n",
      "\tdeleted:    main_folder/artifacts/toniot_phase0_feature_meanings.csv\n",
      "\tdeleted:    main_folder/artifacts/toniot_phase0_files_summary.csv\n",
      "\tdeleted:    main_folder/artifacts/toniot_phase0_placeholder_analysis.csv\n",
      "\tdeleted:    main_folder/artifacts/toniot_phase0_role_classification.csv\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\tmain_folder/Phase_0/Phase_0_1_TON_IoT_Data_Understanding.md\n",
      "\tmain_folder/Phase_0/Phase_0_2_IoT23_Data_Understanding.md\n",
      "\tmain_folder/Phase_0/Phase_0_3_Bot_IoT_Data_Understanding.md\n",
      "\tmain_folder/Phase_1/Phase_1_1_Universal_Schema_Alignment.md\n",
      "\tmain_folder/Phase_1/Phase_1_2_Vectorization_and_Smart_Reduction.md\n",
      "\tmain_folder/chromadb_store_v4/\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "\n",
      "  Latest commit: ce6ed41 Phase 1.4 v4: ChromaDB Knowledge Ingestion complete\n",
      "\n",
      "âœ… Phase 1.4 committed!\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CELL 9 | Git Commit\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess\n",
    "\n",
    "REPO_ROOT = MAIN_DIR.parent\n",
    "\n",
    "def run_git(args, cwd=REPO_ROOT):\n",
    "    result = subprocess.run(\n",
    "        [\"git\"] + args, cwd=str(cwd), capture_output=True, text=True\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        print(f\"âš ï¸  git {' '.join(args[:2])} returned {result.returncode}\")\n",
    "        print(result.stderr.strip())\n",
    "    return result.stdout.strip()\n",
    "\n",
    "files_to_stage = [\n",
    "    \"main_folder/Phase_1/Phase_1_4_Knowledge_Ingestion.ipynb\",\n",
    "    \"main_folder/data/processed/live_simulation_meta_v4.parquet\",\n",
    "    # live_simulation_data_v4.npy may be large â€” use LFS or skip if needed\n",
    "]\n",
    "\n",
    "# Track live_simulation_data_v4.npy only if under 100 MB (git limit)\n",
    "live_npy_rel = \"main_folder/data/processed/live_simulation_data_v4.npy\"\n",
    "if LIVE_VECTORS_PATH.exists() and LIVE_VECTORS_PATH.stat().st_size < 100 * 1e6:\n",
    "    files_to_stage.append(live_npy_rel)\n",
    "else:\n",
    "    print(f\"   â­ï¸  Skipping large binary (>{100} MB): {LIVE_VECTORS_PATH.name}\")\n",
    "\n",
    "print(\"ğŸ“‹ Staging Phase 1.4 artifacts â€¦\")\n",
    "for f in files_to_stage:\n",
    "    abs_f = REPO_ROOT / f\n",
    "    if abs_f.exists():\n",
    "        run_git([\"add\", \"-f\", f])\n",
    "        print(f\"   âœ… staged  {f}\")\n",
    "    else:\n",
    "        print(f\"   â­ï¸  skip (not found): {f}\")\n",
    "\n",
    "commit_msg = (\n",
    "    f\"Phase 1.4 v4: ChromaDB Knowledge Ingestion complete\\n\\n\"\n",
    "    f\"- Built from Phase 1.2 v4 vectors (9.15M Ã— 50)\\n\"\n",
    "    f\"- Stratified 80/20 split of {len(X_knowledge):,} vectors\\n\"\n",
    "    f\"- Ingest Set: {len(X_ingest):,} vectors â†’ rag_ids_knowledge_base (cosine/hnsw)\\n\"\n",
    "    f\"- Live Set  : {len(X_live):,} vectors â†’ live_simulation_data_v4.npy + meta_v4.parquet\\n\"\n",
    "    f\"- ChromaDB collection count: {final_count:,}\\n\"\n",
    "    f\"- Avg sanity-check distance : {avg_distance:.6f}\\n\"\n",
    "    f\"- Avg sanity-check latency  : {avg_latency:.1f} ms\"\n",
    ")\n",
    "\n",
    "out = run_git([\"commit\", \"-m\", commit_msg])\n",
    "print(f\"\\n{'='*65}\")\n",
    "print(out if out else \"(nothing to commit / already up-to-date)\")\n",
    "\n",
    "log = run_git([\"log\", \"--oneline\", \"-1\"])\n",
    "\n",
    "print(f\"\\n  Latest commit: {log}\")\n",
    "print(f\"\\nâœ… Phase 1.4 committed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
