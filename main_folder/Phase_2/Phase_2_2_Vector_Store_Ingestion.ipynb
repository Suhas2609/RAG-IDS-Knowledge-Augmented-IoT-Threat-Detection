{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b8d724",
   "metadata": {},
   "source": [
    "# Phase 2.2 — Vector Store Ingestion  `[v5.1 — 102,505-row Knowledge Base]`\n",
    "\n",
    "**Objective:** Ingest the distilled Knowledge Vector Base into a persistent ChromaDB vector store  \n",
    "optimised for **sub-10 ms** cosine-similarity retrieval at inference time.\n",
    "\n",
    "**Input:**\n",
    "- `data/vectors/X_knowledge_vectors_v51.npy` — float32, shape *(102505, 114)*\n",
    "- `data/vectors/y_knowledge_metadata_v51.parquet` — archetype · specific_attack · dataset_source\n",
    "\n",
    "**Output:** `chromadb_store_v51/` — persistent HNSW collection `ids_knowledge_base_v51`\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Strategy\n",
    "\n",
    "| Step | Action | Rationale |\n",
    "|------|--------|----------|\n",
    "| **1. L2 Normalisation** | Unit-normalise every 114-dim vector | Neutralises the 169.3 Global Max outlier — prevents high-magnitude features (e.g. packet bytes) from dominating cosine distance |\n",
    "| **2. Cosine HNSW** | `hnsw:space = 'cosine'` | After L2 normalisation, cosine ≡ dot-product — maximally captures *behavioural direction* rather than raw magnitude |\n",
    "| **3. Batch = 5,000** | `collection.add()` in 5K-row batches | Keeps per-batch RAM below ~12 MB (5000 × 114 × float32) — avoids the OOM crashes from Phase 2.1 |\n",
    "| **4. HNSW index** | Hierarchical Navigable Small World graph | O(log N) ANN search → sub-10 ms on 102K vectors at 114 dims |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03e47f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromadb : 1.4.1\n",
      "Python   : 3.13.9\n",
      "numpy    : 2.1.3\n",
      "pandas   : 2.2.3\n",
      "Schema   : v5.1  (114-dim)\n",
      "Imports OK.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 1: Imports & Dependency Gate ─────────────────────────────────────────\n",
    "import sys, os, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ChromaDB — install if missing\n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb import PersistentClient\n",
    "    print(f'chromadb : {chromadb.__version__}')\n",
    "except ImportError:\n",
    "    print('chromadb not found — installing …')\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'chromadb', '-q'])\n",
    "    import chromadb\n",
    "    from chromadb import PersistentClient\n",
    "    print(f'chromadb : {chromadb.__version__}  (just installed)')\n",
    "\n",
    "SCHEMA_VERSION = 'v5.1'\n",
    "TOTAL_DIMS     = 114\n",
    "\n",
    "print(f'Python   : {sys.version.split()[0]}')\n",
    "print(f'numpy    : {np.__version__}')\n",
    "print(f'pandas   : {pd.__version__}')\n",
    "print(f'Schema   : {SCHEMA_VERSION}  ({TOTAL_DIMS}-dim)')\n",
    "print('Imports OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8005f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors  : c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\vectors\\X_knowledge_vectors_v51.npy\n",
      "Metadata : c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\vectors\\y_knowledge_metadata_v51.parquet\n",
      "ChromaDB : c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\chromadb_store_v51\n",
      "Collection : ids_knowledge_base_v51\n",
      "Batch size : 5,000\n",
      "Paths OK.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 2: Paths ──────────────────────────────────────────────────────────────\n",
    "NOTEBOOK_DIR  = Path.cwd()\n",
    "MAIN_DIR      = NOTEBOOK_DIR.parent\n",
    "DATA_DIR      = MAIN_DIR / 'data'\n",
    "VECTORS_DIR   = DATA_DIR / 'vectors'\n",
    "CHROMA_DIR    = MAIN_DIR / 'chromadb_store_v51'\n",
    "\n",
    "VECTORS_PATH  = VECTORS_DIR / 'X_knowledge_vectors_v51.npy'\n",
    "META_PATH     = VECTORS_DIR / 'y_knowledge_metadata_v51.parquet'\n",
    "\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "COLLECTION_NAME = 'ids_knowledge_base_v51'\n",
    "INGEST_BATCH    = 5_000   # rows per collection.add() call\n",
    "\n",
    "print(f'Vectors  : {VECTORS_PATH}')\n",
    "print(f'Metadata : {META_PATH}')\n",
    "print(f'ChromaDB : {CHROMA_DIR}')\n",
    "print(f'Collection : {COLLECTION_NAME}')\n",
    "print(f'Batch size : {INGEST_BATCH:,}')\n",
    "assert VECTORS_PATH.exists(), f'MISSING: {VECTORS_PATH}'\n",
    "assert META_PATH.exists(),    f'MISSING: {META_PATH}'\n",
    "print('Paths OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b980536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading knowledge base …\n",
      "  X_raw  : (102505, 114)  dtype=float32\n",
      "  y_meta : (102505, 3)  cols=['ubt_archetype', 'univ_specific_attack', 'dataset_source']\n",
      "  Global Min / Max (raw) : -2.8248 / 169.3271\n",
      "\n",
      "Applying L2 normalisation …\n",
      "  X_norm : (102505, 114)  dtype=float32\n",
      "  Post-norm range        : [-0.6364, 0.9993]\n",
      "  Max deviation from |v|=1  : 1.79e-07  (should be < 1e-6)\n",
      "  Zero-norm vectors (raw)   : 0\n",
      "  Unique archetypes  : 7\n",
      "  Unique attacks     : 33\n",
      "\n",
      "L2 normalisation verified. ✅\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 3: Data Loading & L2 Normalisation ────────────────────────────────────\n",
    "#\n",
    "# WHY L2 NORMALISE?\n",
    "#   Phase 2.1 produced vectors with Global Max = 169.3 (bytes / packet features).\n",
    "#   Raw Euclidean / cosine search on un-normalised vectors gives those high-magnitude\n",
    "#   features 169× the voting weight of binary mask bits.  L2 normalisation collapses\n",
    "#   every vector onto the unit sphere so that cosine similarity measures *direction*\n",
    "#   (i.e. behavioural pattern) rather than magnitude.  After normalisation, cosine\n",
    "#   distance == dot product, which HNSW is built to optimise.\n",
    "# ─────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "print('Loading knowledge base …')\n",
    "X_raw  = np.load(str(VECTORS_PATH))          # (102505, 114) float32\n",
    "y_meta = pd.read_parquet(str(META_PATH))      # (102505, 3)\n",
    "\n",
    "print(f'  X_raw  : {X_raw.shape}  dtype={X_raw.dtype}')\n",
    "print(f'  y_meta : {y_meta.shape}  cols={list(y_meta.columns)}')\n",
    "print(f'  Global Min / Max (raw) : {X_raw.min():.4f} / {X_raw.max():.4f}')\n",
    "\n",
    "print('\\nApplying L2 normalisation …')\n",
    "_norms = np.linalg.norm(X_raw, axis=1, keepdims=True)   # (102505, 1)\n",
    "_norms = np.where(_norms == 0, 1.0, _norms)              # guard zero-norm vectors\n",
    "X_norm = (X_raw / _norms).astype(np.float32)             # unit sphere\n",
    "\n",
    "# Sanity checks\n",
    "_magnitudes       = np.linalg.norm(X_norm, axis=1)\n",
    "_max_deviation    = np.abs(_magnitudes - 1.0).max()\n",
    "_zero_norm_count  = int(np.sum(np.linalg.norm(X_raw, axis=1) == 0))\n",
    "\n",
    "print(f'  X_norm : {X_norm.shape}  dtype={X_norm.dtype}')\n",
    "print(f'  Post-norm range        : [{X_norm.min():.4f}, {X_norm.max():.4f}]')\n",
    "print(f'  Max deviation from |v|=1  : {_max_deviation:.2e}  (should be < 1e-6)')\n",
    "print(f'  Zero-norm vectors (raw)   : {_zero_norm_count}')\n",
    "print(f'  Unique archetypes  : {y_meta[\"ubt_archetype\"].nunique()}')\n",
    "print(f'  Unique attacks     : {y_meta[\"univ_specific_attack\"].nunique()}')\n",
    "\n",
    "assert _max_deviation < 1e-5, f'L2 normalisation failed — max deviation: {_max_deviation}'\n",
    "assert len(X_norm) == len(y_meta), 'Vector / metadata alignment error'\n",
    "print('\\nL2 normalisation verified. ✅')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0cc551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising ChromaDB at: c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\chromadb_store_v51\n",
      "  Collection created : ids_knowledge_base_v51\n",
      "  HNSW space         : cosine\n",
      "  HNSW M             : 32   (graph edges per node)\n",
      "  HNSW ef_construct  : 200  (build-time beam width)\n",
      "  HNSW search_ef     : 100  (query-time beam width)\n",
      "ChromaDB initialised. ✅\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 4: ChromaDB Initialisation ───────────────────────────────────────────\n",
    "#\n",
    "# HNSW space = 'cosine':\n",
    "#   After L2 normalisation every vector has magnitude 1.  Cosine similarity then\n",
    "#   equals the dot product, which HNSW evaluates in O(dim) at each graph edge\n",
    "#   traversal.  This gives the fastest possible ANN performance for semantic\n",
    "#   behavioural matching on our 114-dim schema.\n",
    "# ─────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "print(f'Initialising ChromaDB at: {CHROMA_DIR}')\n",
    "client = PersistentClient(path=str(CHROMA_DIR))\n",
    "\n",
    "# Delete existing collection to allow clean re-ingest\n",
    "existing = [c.name for c in client.list_collections()]\n",
    "if COLLECTION_NAME in existing:\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "    print(f'  Deleted existing collection: {COLLECTION_NAME}')\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name     = COLLECTION_NAME,\n",
    "    metadata = {\n",
    "        'hnsw:space'          : 'cosine',   # dot-product ANN after L2 norm\n",
    "        'hnsw:construction_ef': 200,         # higher = better recall during build\n",
    "        'hnsw:M'              : 32,          # edges/node — balances RAM vs recall\n",
    "        'hnsw:search_ef'      : 100,         # candidates examined per query\n",
    "        'description'         : 'RAG-IDS Knowledge Base v5.1 — 102505 medoids, 114-dim, L2-normalised',\n",
    "        'schema_version'      : SCHEMA_VERSION,\n",
    "        'total_dims'          : str(TOTAL_DIMS),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f'  Collection created : {collection.name}')\n",
    "print(f'  HNSW space         : cosine')\n",
    "print(f'  HNSW M             : 32   (graph edges per node)')\n",
    "print(f'  HNSW ef_construct  : 200  (build-time beam width)')\n",
    "print(f'  HNSW search_ef     : 100  (query-time beam width)')\n",
    "print('ChromaDB initialised. ✅')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37620155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting 102,505 vectors in 21 batches of 5,000 …\n",
      "  Per-batch RAM (vectors only) : 2.28 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff51f5eac2f4eb79693f8f58f422299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ingesting:   0%|          | 0/21 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete.\n",
      "  Rows added           : 102,505\n",
      "  Collection count     : 102,505\n",
      "  Total time           : 26.5s  (0.4 min)\n",
      "  Throughput           : 3,870 rows/s\n",
      "Count verified. ✅\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 5: Resource-Safe Batch Ingestion ──────────────────────────────────────\n",
    "#\n",
    "# Each batch of 5,000 × 114 float32 = 2.28 MB of raw vector data.\n",
    "# ChromaDB serialises + indexes each batch before requesting the next,\n",
    "# so peak RAM per batch is bounded — avoids the OOM pattern from Phase 2.1.\n",
    "# Total ingestion memory footprint is dominated by the HNSW graph (~200 MB)\n",
    "# not by the batch buffer.\n",
    "# ─────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(it, **kw): return it\n",
    "\n",
    "N_TOTAL  = len(X_norm)\n",
    "n_batches = (N_TOTAL + INGEST_BATCH - 1) // INGEST_BATCH\n",
    "\n",
    "print(f'Ingesting {N_TOTAL:,} vectors in {n_batches} batches of {INGEST_BATCH:,} …')\n",
    "print(f'  Per-batch RAM (vectors only) : {INGEST_BATCH * TOTAL_DIMS * 4 / 1e6:.2f} MB')\n",
    "\n",
    "t_ingest = time.time()\n",
    "rows_added = 0\n",
    "\n",
    "for batch_idx in tqdm(range(n_batches), desc='Ingesting', unit='batch'):\n",
    "    lo = batch_idx * INGEST_BATCH\n",
    "    hi = min(lo + INGEST_BATCH, N_TOTAL)\n",
    "\n",
    "    batch_vecs  = X_norm[lo:hi].tolist()          # ChromaDB expects list[list[float]]\n",
    "    batch_meta  = y_meta.iloc[lo:hi]\n",
    "\n",
    "    batch_ids = [f'kb_{i}' for i in range(lo, hi)]\n",
    "\n",
    "    batch_documents = [\n",
    "        f\"{row['ubt_archetype']}|{row['univ_specific_attack']}|{row['dataset_source']}\"\n",
    "        for _, row in batch_meta.iterrows()\n",
    "    ]\n",
    "\n",
    "    batch_metadatas = [\n",
    "        {\n",
    "            'ubt_archetype'        : str(row['ubt_archetype']),\n",
    "            'univ_specific_attack' : str(row['univ_specific_attack']),\n",
    "            'dataset_source'       : str(row['dataset_source']),\n",
    "            'kb_index'             : int(lo + j),\n",
    "        }\n",
    "        for j, (_, row) in enumerate(batch_meta.iterrows())\n",
    "    ]\n",
    "\n",
    "    collection.add(\n",
    "        ids        = batch_ids,\n",
    "        embeddings = batch_vecs,\n",
    "        documents  = batch_documents,\n",
    "        metadatas  = batch_metadatas,\n",
    "    )\n",
    "    rows_added += (hi - lo)\n",
    "\n",
    "t_elapsed = time.time() - t_ingest\n",
    "final_count = collection.count()\n",
    "\n",
    "print(f'\\nIngestion complete.')\n",
    "print(f'  Rows added           : {rows_added:,}')\n",
    "print(f'  Collection count     : {final_count:,}')\n",
    "print(f'  Total time           : {t_elapsed:.1f}s  ({t_elapsed/60:.1f} min)')\n",
    "print(f'  Throughput           : {rows_added/t_elapsed:,.0f} rows/s')\n",
    "assert final_count == N_TOTAL, f'COUNT MISMATCH: {final_count} != {N_TOTAL}'\n",
    "print('Count verified. ✅')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37ad3885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LATENCY BENCHMARK & FIDELITY CHECK\n",
      "=================================================================\n",
      "\n",
      "Probe vector index : 9,148\n",
      "  Archetype  : EXPLOIT\n",
      "  Attack     : xss\n",
      "  Source     : toniot\n",
      "\n",
      "  Cold query latency   : 55.10 ms\n",
      "  Warm query latency   : 0.94 ms (mean over 20 queries)\n",
      "    p50                : 0.91 ms\n",
      "    p95                : 1.21 ms\n",
      "    p99                : 1.42 ms\n",
      "    min / max          : 0.77 / 1.47 ms\n",
      "\n",
      "  Target (<10 ms warm)  : ✅ PASS\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "FIDELITY CHECK — Top-3 Nearest Neighbours\n",
      "-----------------------------------------------------------------\n",
      "  Probe    →  [EXPLOIT] xss  (src: toniot)\n",
      "\n",
      "  Rank 1 ✅  dist=0.000000\n",
      "          ubt_archetype        : EXPLOIT\n",
      "          univ_specific_attack : xss\n",
      "          dataset_source       : toniot\n",
      "\n",
      "  Rank 2 ✅  dist=0.000002\n",
      "          ubt_archetype        : EXPLOIT\n",
      "          univ_specific_attack : xss\n",
      "          dataset_source       : toniot\n",
      "\n",
      "  Rank 3 ✅  dist=0.000005\n",
      "          ubt_archetype        : EXPLOIT\n",
      "          univ_specific_attack : xss\n",
      "          dataset_source       : toniot\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "  Archetype Precision@3 : 3/3  (100.0%)\n",
      "=================================================================\n",
      "PHASE 2.2 — COMPLETE\n",
      "=================================================================\n",
      "  Collection : ids_knowledge_base_v51\n",
      "  Vectors    : 102,505  ×  114-dim  (L2-normalised, cosine HNSW)\n",
      "  Store path : c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\chromadb_store_v51\n",
      "  Warm p50   : 0.91 ms\n",
      "  Real-time ready: YES\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 6: Latency Benchmarking & Fidelity Check ─────────────────────────────\n",
    "#\n",
    "# Simulates a real-time IDS detection query:\n",
    "#   1. Pick a random probe vector from the knowledge base (warm path)\n",
    "#   2. Query collection.query() — HNSW ANN search — for top-10 nearest neighbours\n",
    "#   3. Measure wall-clock latency (cold + warm runs)\n",
    "#   4. Inspect top-3 returned metadata for semantic fidelity\n",
    "# ─────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "SEP   = '=' * 65\n",
    "SEP2  = '-' * 65\n",
    "N_TOP = 10\n",
    "\n",
    "rng   = np.random.default_rng(seed=42)\n",
    "probe_idx   = int(rng.integers(0, len(X_norm)))\n",
    "probe_vec   = X_norm[probe_idx].tolist()          # already L2-normalised\n",
    "probe_meta  = y_meta.iloc[probe_idx]\n",
    "\n",
    "print(SEP)\n",
    "print('LATENCY BENCHMARK & FIDELITY CHECK')\n",
    "print(SEP)\n",
    "print(f'\\nProbe vector index : {probe_idx:,}')\n",
    "print(f'  Archetype  : {probe_meta[\"ubt_archetype\"]}')\n",
    "print(f'  Attack     : {probe_meta[\"univ_specific_attack\"]}')\n",
    "print(f'  Source     : {probe_meta[\"dataset_source\"]}')\n",
    "\n",
    "# ── Cold run (first query — HNSW graph not yet in CPU cache) ──────────────────\n",
    "t0_cold = time.perf_counter()\n",
    "_res_cold = collection.query(\n",
    "    query_embeddings=[probe_vec],\n",
    "    n_results=N_TOP,\n",
    "    include=['metadatas', 'distances', 'documents'],\n",
    ")\n",
    "latency_cold_ms = (time.perf_counter() - t0_cold) * 1000\n",
    "\n",
    "# ── Warm runs (HNSW graph resident in CPU cache) ──────────────────────────────\n",
    "N_WARM = 20\n",
    "warm_probes = rng.integers(0, len(X_norm), size=N_WARM)\n",
    "warm_latencies = []\n",
    "\n",
    "for wi in warm_probes:\n",
    "    _wv = X_norm[int(wi)].tolist()\n",
    "    t0w = time.perf_counter()\n",
    "    collection.query(\n",
    "        query_embeddings=[_wv],\n",
    "        n_results=N_TOP,\n",
    "        include=['metadatas', 'distances'],\n",
    "    )\n",
    "    warm_latencies.append((time.perf_counter() - t0w) * 1000)\n",
    "\n",
    "warm_latencies = np.array(warm_latencies)\n",
    "\n",
    "print(f'\\n  Cold query latency   : {latency_cold_ms:.2f} ms')\n",
    "print(f'  Warm query latency   : {warm_latencies.mean():.2f} ms (mean over {N_WARM} queries)')\n",
    "print(f'    p50                : {np.percentile(warm_latencies, 50):.2f} ms')\n",
    "print(f'    p95                : {np.percentile(warm_latencies, 95):.2f} ms')\n",
    "print(f'    p99                : {np.percentile(warm_latencies, 99):.2f} ms')\n",
    "print(f'    min / max          : {warm_latencies.min():.2f} / {warm_latencies.max():.2f} ms')\n",
    "\n",
    "TARGET_MS = 10.0\n",
    "meets_target = warm_latencies.mean() < TARGET_MS\n",
    "print(f'\\n  Target (<{TARGET_MS:.0f} ms warm)  : {\"✅ PASS\" if meets_target else \"⚠️  MISS\"}')\n",
    "\n",
    "# ── Fidelity Check — top-3 results ────────────────────────────────────────────\n",
    "top_metas     = _res_cold['metadatas'][0][:3]\n",
    "top_distances = _res_cold['distances'][0][:3]\n",
    "\n",
    "print(f'\\n{SEP2}')\n",
    "print(f'FIDELITY CHECK — Top-3 Nearest Neighbours')\n",
    "print(f'{SEP2}')\n",
    "print(f'  Probe    →  [{probe_meta[\"ubt_archetype\"]}] {probe_meta[\"univ_specific_attack\"]}  (src: {probe_meta[\"dataset_source\"]})')\n",
    "print()\n",
    "\n",
    "for rank, (m, d) in enumerate(zip(top_metas, top_distances), 1):\n",
    "    archetype = m.get('ubt_archetype', 'N/A')\n",
    "    attack    = m.get('univ_specific_attack', 'N/A')\n",
    "    source    = m.get('dataset_source', 'N/A')\n",
    "    match_arch = '✅' if archetype == probe_meta['ubt_archetype'] else '❌'\n",
    "    print(f'  Rank {rank} {match_arch}  dist={d:.6f}')\n",
    "    print(f'          ubt_archetype        : {archetype}')\n",
    "    print(f'          univ_specific_attack : {attack}')\n",
    "    print(f'          dataset_source       : {source}')\n",
    "    print()\n",
    "\n",
    "# Archetype precision@3\n",
    "correct_arch = sum(\n",
    "    1 for m in top_metas if m.get('ubt_archetype') == probe_meta['ubt_archetype']\n",
    ")\n",
    "precision_at_3 = correct_arch / 3\n",
    "\n",
    "print(f'{SEP2}')\n",
    "print(f'  Archetype Precision@3 : {correct_arch}/3  ({precision_at_3*100:.1f}%)')\n",
    "print(f'{SEP}')\n",
    "print('PHASE 2.2 — COMPLETE')\n",
    "print(f'{SEP}')\n",
    "print(f'  Collection : {collection.name}')\n",
    "print(f'  Vectors    : {final_count:,}  ×  {TOTAL_DIMS}-dim  (L2-normalised, cosine HNSW)')\n",
    "print(f'  Store path : {CHROMA_DIR}')\n",
    "print(f'  Warm p50   : {np.percentile(warm_latencies, 50):.2f} ms')\n",
    "print(f'  Real-time ready: {\"YES\" if meets_target else \"REVIEW NEEDED\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b285720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare Attack Test: THEFT_EXFIL\n",
      "Distance: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Rarity Validation: Can the system still find the 97 Theft/Exfil rows?\n",
    "rare_probe = y_meta[y_meta['ubt_archetype'] == 'THEFT_EXFIL'].index[0]\n",
    "res_rare = collection.query(query_embeddings=[X_norm[rare_probe].tolist()], n_results=1)\n",
    "\n",
    "print(f\"Rare Attack Test: {res_rare['metadatas'][0][0]['ubt_archetype']}\")\n",
    "print(f\"Distance: {res_rare['distances'][0][0]:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
