{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2047e2",
   "metadata": {},
   "source": [
    "# Phase 2.1 — Master Medoid Distillation  `[v5.1 — 351M-row IDS Ocean]`\n",
    "\n",
    "**Objective:** Distil the 351M-row IDS Ocean into a compact, semantically representative **Knowledge Vector Base**  \n",
    "via Serial Archetype Distillation — MiniBatchKMeans + Incremental Global Medoid Tracking.\n",
    "\n",
    "**Input:** `data/unified/ocean_v51/` (7 Hive partitions) + `artifacts/preprocessors_v51.pkl`  \n",
    "**Outputs:**\n",
    "- `data/vectors/X_knowledge_vectors_v51.npy` — float32, shape *(N_medoids, 114)*\n",
    "- `data/vectors/y_knowledge_metadata_v51.parquet` — archetype · specific_attack · dataset_source\n",
    "\n",
    "---\n",
    "\n",
    "## Universal Behavioral Schema v5.1 — 114 Dimensions\n",
    "\n",
    "| Block | Dims | Offset | Contents |\n",
    "|-------|------|--------|----------|\n",
    "| **B1 Core**     | 5  | 0   | duration(RS·log1p) · bytes_in/out(QT) · pkts_in/out(QT) |\n",
    "| **B2 Protocol** | 18 | 5   | proto-OHE(6: tcp udp icmp arp ipv6 other) · svc-OHE(12, gated by has_svc) |\n",
    "| **B3 State**    | 5  | 23  | PENDING · ESTABLISHED · REJECTED · RESET · OTHER |\n",
    "| **B4 Port**     | 16 | 28  | sport_func-OHE(7) · dport_func-OHE(7) · rarity(2, PowerTransformer) |\n",
    "| **B5a DNS**     | 15 | 44  | qtype-OHE(10) · qclass-OHE(3) · rcode(2); gated by has_dns |\n",
    "| **B5b HTTP**    | 21 | 59  | method-OHE(8) · status-class(6) · body(2) · flags(4) · depth(1); gated by has_http |\n",
    "| **B5c SSL**     | 15 | 80  | cipher-OHE(12) · version(2) · established(1); gated by has_ssl |\n",
    "| **B6 Momentum** | 14 | 95  | 14 UNSW window features (RS·log1p+shift); gated by has_unsw |\n",
    "| **Mask Bits**   | 5  | 109 | has_svc · has_dns · has_http · has_ssl · has_unsw |\n",
    "| **TOTAL**       | **114** | | |\n",
    "\n",
    "## Medoid Allocation\n",
    "\n",
    "| Archetype    | Source Rows | Centers | Strategy |\n",
    "|--------------|-------------|---------|----------|\n",
    "| EXPLOIT      | 2,635,460   | 60,000  | MiniBatchKMeans |\n",
    "| BOTNET_C2    | 61,556,313  | 50,000  | MiniBatchKMeans |\n",
    "| BRUTE_FORCE  | 1,718,568   | 50,000  | MiniBatchKMeans |\n",
    "| SCAN         | 221,084,172 | 30,000  | MiniBatchKMeans |\n",
    "| DOS_DDOS     | 32,665,331  | 30,000  | MiniBatchKMeans |\n",
    "| NORMAL       | 31,657,548  | 30,000  | MiniBatchKMeans |\n",
    "| THEFT_EXFIL  | 97          | 97      | 100% capture (no clustering) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "107b67c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python      : 3.13.9\n",
      "pandas      : 2.2.3\n",
      "numpy       : 2.1.3\n",
      "pyarrow     : 23.0.0\n",
      "Schema      : v5.1  (114-dim)\n",
      "Imports OK.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, gc, json, time, pickle, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ads\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    from tqdm.auto import tqdm as tqdm_auto\n",
    "except ImportError:\n",
    "    def tqdm(it, **kw): return it\n",
    "    tqdm_auto = tqdm\n",
    "\n",
    "SCHEMA_VERSION = 'v5.1'\n",
    "TOTAL_DIMS     = 114\n",
    "\n",
    "print(f'Python      : {sys.version.split()[0]}')\n",
    "print(f'pandas      : {pd.__version__}')\n",
    "print(f'numpy       : {np.__version__}')\n",
    "print(f'pyarrow     : {pa.__version__}')\n",
    "print(f'Schema      : {SCHEMA_VERSION}  ({TOTAL_DIMS}-dim)')\n",
    "print('Imports OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e5a5935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessors_v51.pkl …\n",
      "  preprocessors_v51: loaded 15 keys\n",
      "  total_rows_ocean : 351,317,489\n",
      "  block1_scalers   : 5 cols\n",
      "  block6_scalers   : 14 cols\n",
      "  qt_byte_pkt      : 4 cols\n",
      "  pt_sport_rarity  : OK\n",
      "  pt_dport_rarity  : OK\n",
      "\n",
      "Partition inventory:\n",
      "  Archetype       #Files   Centers  Strategy\n",
      "  ──────────────────────────────────────────────────────\n",
      "  ✅ EXPLOIT             15     60000  MiniBatchKMeans(k=60,000)\n",
      "  ✅ BOTNET_C2         1041     50000  MiniBatchKMeans(k=50,000)\n",
      "  ✅ BRUTE_FORCE          8     50000  MiniBatchKMeans(k=50,000)\n",
      "  ✅ SCAN              1182     30000  MiniBatchKMeans(k=30,000)\n",
      "  ✅ DOS_DDOS           537     30000  MiniBatchKMeans(k=30,000)\n",
      "  ✅ NORMAL            1408     30000  MiniBatchKMeans(k=30,000)\n",
      "  ✅ THEFT_EXFIL          4       all  100% capture\n",
      "Config OK.\n"
     ]
    }
   ],
   "source": [
    "# ── Paths ─────────────────────────────────────────────────────────────────────\n",
    "NOTEBOOK_DIR    = Path.cwd()\n",
    "MAIN_DIR        = NOTEBOOK_DIR.parent\n",
    "DATA_DIR        = MAIN_DIR / 'data'\n",
    "OCEAN_V51_DIR   = DATA_DIR / 'unified' / 'ocean_v51'\n",
    "ARTIFACTS_DIR   = MAIN_DIR / 'artifacts'\n",
    "VECTORS_DIR     = DATA_DIR / 'vectors'\n",
    "TMP_DIR         = VECTORS_DIR / '_tmp_medoids'\n",
    "\n",
    "for d in [VECTORS_DIR, TMP_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PREPROCESSORS_PATH = ARTIFACTS_DIR / 'preprocessors_v51.pkl'\n",
    "PORT_MAP_PATH      = ARTIFACTS_DIR / 'scalers' / 'global_port_map.json'\n",
    "VECTORS_OUT_PATH   = VECTORS_DIR / 'X_knowledge_vectors_v51.npy'\n",
    "META_OUT_PATH      = VECTORS_DIR / 'y_knowledge_metadata_v51.parquet'\n",
    "\n",
    "# ── Distillation Config ────────────────────────────────────────────────────────\n",
    "CHUNK_ROWS = 200_000   # rows per streaming batch\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ARCHETYPE_ALLOCATION = {\n",
    "    'EXPLOIT'    : 60_000,\n",
    "    'BOTNET_C2'  : 50_000,\n",
    "    'BRUTE_FORCE': 50_000,\n",
    "    'SCAN'       : 30_000,\n",
    "    'DOS_DDOS'   : 30_000,\n",
    "    'NORMAL'     : 30_000,\n",
    "    'THEFT_EXFIL': None,    # 100% capture\n",
    "}\n",
    "\n",
    "# ── Load Preprocessors ─────────────────────────────────────────────────────────\n",
    "print(f'Loading {PREPROCESSORS_PATH.name} …')\n",
    "with open(PREPROCESSORS_PATH, 'rb') as f:\n",
    "    PP = pickle.load(f)\n",
    "\n",
    "block1_scalers = PP['block1_scalers']\n",
    "block6_scalers = PP['block6_scalers']\n",
    "qt_byte_pkt    = PP['qt_byte_pkt']\n",
    "pt_sport       = PP['pt_sport_rarity']\n",
    "pt_dport       = PP['pt_dport_rarity']\n",
    "sport_rarity   = PP['sport_rarity_map']   # {str(port) -> freq}\n",
    "dport_rarity   = PP['dport_rarity_map']\n",
    "TOTAL_ROWS_OCEAN = PP['total_rows_ocean']\n",
    "\n",
    "print(f'  preprocessors_v51: loaded {len(PP)} keys')\n",
    "print(f'  total_rows_ocean : {TOTAL_ROWS_OCEAN:,}')\n",
    "print(f'  block1_scalers   : {len(block1_scalers)} cols')\n",
    "print(f'  block6_scalers   : {len(block6_scalers)} cols')\n",
    "print(f'  qt_byte_pkt      : {len(qt_byte_pkt)} cols')\n",
    "print(f'  pt_sport_rarity  : {\"OK\" if pt_sport else \"MISSING\"}')\n",
    "print(f'  pt_dport_rarity  : {\"OK\" if pt_dport else \"MISSING\"}')\n",
    "\n",
    "# ── Discover Partition Dirs ────────────────────────────────────────────────────\n",
    "part_dirs = {d.name.split('=', 1)[1]: d\n",
    "             for d in sorted(OCEAN_V51_DIR.iterdir()) if d.is_dir() and '=' in d.name}\n",
    "\n",
    "print(f'\\nPartition inventory:')\n",
    "print(f'  {\"Archetype\":<14} {\"#Files\":>7}  {\"Centers\":>8}  {\"Strategy\"}')\n",
    "print('  ' + '─' * 54)\n",
    "for arch, n_c in ARCHETYPE_ALLOCATION.items():\n",
    "    pdir = part_dirs.get(arch)\n",
    "    n_f  = len(list(pdir.glob('*.parquet'))) if pdir else 0\n",
    "    strat = f'MiniBatchKMeans(k={n_c:,})' if n_c else '100% capture'\n",
    "    status = '✅' if pdir else '❌ MISSING'\n",
    "    print(f'  {status} {arch:<14} {n_f:>7}  {str(n_c) if n_c else \"all\":>8}  {strat}')\n",
    "print('Config OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "129d087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying vectorize_v51 output dimensions …\n",
      "  Shape: (1, 114)  dtype=float32  ✅\n",
      "  Non-zero dims: 20 / 114\n",
      "  B1[0:5]   = [1.153  0.9516 0.7869 0.7741 0.7191]\n",
      "  B2a[5:11] = [1. 0. 0. 0. 0. 0.]  ← tcp should be 1.0\n",
      "  B2b[11:23]= [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]  ← http(idx1) = 1.0\n",
      "  B3[23:28] = [0. 1. 0. 0. 0.]  ← ESTABLISHED(idx1) = 1.0\n",
      "  B4a[28:35]= [0. 0. 0. 0. 0. 1. 0.]  ← WEB_SERVICES(idx2) = 1.0 (sport eph)\n",
      "  B4b[35:42]= [0. 0. 1. 0. 0. 0. 0.]  ← WEB_SERVICES(idx2) = 1.0 (dport 80)\n",
      "  Mask[109:]= [1. 0. 1. 0. 0.]\n",
      "vectorize_v51 verified. ✅\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 1 — vectorize_v51(df)\n",
    "# Maps one chunk of v5.1-aligned ocean rows → (N, 114) float32 array.\n",
    "#\n",
    "# Block layout (must match schema header):\n",
    "#   [0 :5 ] B1  Core              5 dims\n",
    "#   [5 :11] B2a Proto OHE         6 dims\n",
    "#   [11:23] B2b Service OHE      12 dims  (gated by has_svc)\n",
    "#   [23:28] B3  State OHE         5 dims\n",
    "#   [28:35] B4a sport_func OHE    7 dims\n",
    "#   [35:42] B4b dport_func OHE    7 dims\n",
    "#   [42:44] B4c Port rarity       2 dims  (PowerTransformer)\n",
    "#   [44:54] B5a DNS qtype OHE    10 dims  (gated by has_dns)\n",
    "#   [54:57] B5a DNS qclass OHE    3 dims\n",
    "#   [57:59] B5a DNS rcode         2 dims\n",
    "#   [59:67] B5b HTTP method OHE   8 dims  (gated by has_http)\n",
    "#   [67:73] B5b HTTP status cls   6 dims\n",
    "#   [73:80] B5b HTTP body/flags   7 dims\n",
    "#   [80:92] B5c SSL cipher OHE   12 dims  (gated by has_ssl)\n",
    "#   [92:95] B5c SSL ver+estab     3 dims\n",
    "#   [95:109] B6  Momentum        14 dims  (gated by has_unsw)\n",
    "#   [109:114] Mask Bits           5 dims\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# ── OHE vocabularies (must match Phase 1.1 alignment) ─────────────────────────\n",
    "PROTO_TOKENS   = ['tcp', 'udp', 'icmp', 'arp', 'ipv6', 'other']          # 6\n",
    "SERVICE_TOKENS = ['dns', 'http', 'ssl', 'ftp', 'ssh', 'smtp',             # 12\n",
    "                  'dhcp', 'quic', 'ntp', 'rdp', 'pop3', 'other']\n",
    "STATE_TOKENS   = ['PENDING', 'ESTABLISHED', 'REJECTED', 'RESET', 'OTHER'] # 5\n",
    "PORT_FUNC_TOKENS = [\n",
    "    'SCADA_CONTROL', 'IOT_MANAGEMENT', 'WEB_SERVICES',\n",
    "    'NETWORK_CORE',  'REMOTE_ACCESS',  'FUNC_EPHEMERAL', 'FUNC_UNKNOWN',  # 7\n",
    "]\n",
    "HTTP_METHOD_TOKENS = ['GET', 'POST', 'PUT', 'DELETE', 'HEAD', 'OPTIONS', 'PATCH', 'OTHER']  # 8\n",
    "SSL_CIPHER_TOKENS = [\n",
    "    'TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256',\n",
    "    'TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384',\n",
    "    'TLS_RSA_WITH_AES_128_GCM_SHA256',\n",
    "    'TLS_RSA_WITH_AES_256_GCM_SHA384',\n",
    "    'TLS_RSA_WITH_AES_128_CBC_SHA',\n",
    "    'TLS_RSA_WITH_AES_256_CBC_SHA',\n",
    "    'TLS_RSA_WITH_RC4_128_SHA',\n",
    "    'TLS_RSA_WITH_RC4_128_MD5',\n",
    "    'TLS_RSA_WITH_3DES_EDE_CBC_SHA',\n",
    "    'TLS_DHE_RSA_WITH_AES_128_CBC_SHA',\n",
    "    'TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256',\n",
    "    'other',  # 12\n",
    "]\n",
    "\n",
    "# Port function classifier (7-way)\n",
    "SCADA_PORTS    = frozenset({502, 102, 44818})\n",
    "IOT_MGMT_PORTS = frozenset({1883, 5683, 8883})\n",
    "WEB_PORTS      = frozenset({80, 443, 8080})\n",
    "NET_CORE_PORTS = frozenset({53, 67, 68, 123})\n",
    "REMOTE_PORTS   = frozenset({22, 23, 3389})\n",
    "\n",
    "def _classify_port_vec(port_series):\n",
    "    \"\"\"Vectorised 7-way port → index (0-6). Input: int Series or array.\"\"\"\n",
    "    p = pd.to_numeric(port_series, errors='coerce').fillna(-1).astype(int)\n",
    "    result = np.full(len(p), 6, dtype=np.int8)   # default FUNC_UNKNOWN\n",
    "    for idx_port, func_idx, pset in [\n",
    "        (0, 0, SCADA_PORTS),\n",
    "        (1, 1, IOT_MGMT_PORTS),\n",
    "        (2, 2, WEB_PORTS),\n",
    "        (3, 3, NET_CORE_PORTS),\n",
    "        (4, 4, REMOTE_PORTS),\n",
    "    ]:\n",
    "        mask_set = p.isin(pset).values\n",
    "        result[mask_set] = func_idx\n",
    "    # ephemeral\n",
    "    ephemeral = (p.values > 49152) & (result == 6)\n",
    "    result[ephemeral] = 5\n",
    "    return result\n",
    "\n",
    "# DNS qtype canonical map: code → index 0-9 (10 = other)\n",
    "_DNS_QTYPE_MAP = {1: 0, 2: 1, 5: 2, 6: 3, 12: 4, 15: 5, 16: 6, 28: 7, 33: 8, 255: 9}\n",
    "_DNS_QCLASS_MAP = {1: 0, 3: 1}   # IN=0, CH=1, other=2\n",
    "_WEAK_SSL_VER   = frozenset({'sslv2', 'sslv3', 'tlsv1', 'tlsv10', 'tlsv1.0', 'tls1.0'})\n",
    "_STRONG_SSL_VER = frozenset({'tlsv12', 'tlsv13', 'tlsv1.2', 'tlsv1.3', 'tls1.2', 'tls1.3'})\n",
    "\n",
    "# Pre-build index dicts\n",
    "_PROTO_IDX    = {t: i for i, t in enumerate(PROTO_TOKENS)}\n",
    "_SVC_IDX      = {t: i for i, t in enumerate(SERVICE_TOKENS)}\n",
    "_STATE_IDX    = {t: i for i, t in enumerate(STATE_TOKENS)}\n",
    "_METHOD_IDX   = {t: i for i, t in enumerate(HTTP_METHOD_TOKENS)}\n",
    "_CIPHER_IDX   = {t: i for i, t in enumerate(SSL_CIPHER_TOKENS)}\n",
    "_ABSENT_SVCS  = frozenset({'<absent>', '-', 'unknown', '', 'none', '(empty)', 'nan'})\n",
    "\n",
    "\n",
    "def vectorize_v51(df):\n",
    "    \"\"\"\n",
    "    Map a DataFrame of v5.1-aligned ocean rows → (N, 114) float32 array.\n",
    "    All OHE uses strict vocabulary; unknowns mapped to 'other' index.\n",
    "    Gated blocks (B2b svc, B5a DNS, B5b HTTP, B5c SSL, B6 momentum)\n",
    "    are zeroed when the corresponding mask bit = 0.\n",
    "    \"\"\"\n",
    "    n  = len(df)\n",
    "    X  = np.zeros((n, TOTAL_DIMS), dtype=np.float32)\n",
    "    idx = df.index\n",
    "\n",
    "    def _col(name, fill=0.0):\n",
    "        if name in df.columns:\n",
    "            return df[name].fillna(fill)\n",
    "        return pd.Series(fill, index=idx)\n",
    "\n",
    "    def _str_col(name, fill=''):\n",
    "        if name in df.columns:\n",
    "            return df[name].fillna(fill).astype(str).str.lower().str.strip()\n",
    "        return pd.Series(fill, index=idx)\n",
    "\n",
    "    # ── B1: Core (dims 0-4) ───────────────────────────────────────────────────\n",
    "    b1_cols = [\n",
    "        ('univ_duration',  'rs',  0),\n",
    "        ('univ_bytes_in',  'qt',  1),\n",
    "        ('univ_bytes_out', 'qt',  2),\n",
    "        ('univ_pkts_in',   'qt',  3),\n",
    "        ('univ_pkts_out',  'qt',  4),\n",
    "    ]\n",
    "    for col, mode, out_i in b1_cols:\n",
    "        vals = _col(col, 0.0).values.astype(np.float64)\n",
    "        vals = np.clip(vals, 0.0, None)\n",
    "        if mode == 'qt' and col in qt_byte_pkt:\n",
    "            X[:, out_i] = qt_byte_pkt[col].transform(vals.reshape(-1, 1)).ravel().astype(np.float32)\n",
    "        elif mode == 'rs' and col in block1_scalers:\n",
    "            X[:, out_i] = block1_scalers[col].transform(np.log1p(vals).reshape(-1, 1)).ravel().astype(np.float32)\n",
    "\n",
    "    # ── B2a: Proto OHE (dims 5-10) ────────────────────────────────────────────\n",
    "    proto = _str_col('raw_proto', 'other')\n",
    "    proto_idx = proto.map(lambda p: _PROTO_IDX.get(p, _PROTO_IDX['other'])).values\n",
    "    X[np.arange(n), 5 + proto_idx] = 1.0\n",
    "\n",
    "    # ── B2b: Service OHE (dims 11-22) gated by has_svc ───────────────────────\n",
    "    has_svc = _col('has_svc', 0).values.astype(np.float32)\n",
    "    svc = _str_col('raw_service', 'other')\n",
    "    # Map absent / unknown → 'other' (last index)\n",
    "    other_svc_idx = _SVC_IDX['other']\n",
    "    svc_idx = svc.map(\n",
    "        lambda s: _SVC_IDX.get(s, other_svc_idx) if s not in _ABSENT_SVCS else other_svc_idx\n",
    "    ).values\n",
    "    svc_one_hot = np.zeros((n, 12), dtype=np.float32)\n",
    "    svc_one_hot[np.arange(n), svc_idx] = 1.0\n",
    "    X[:, 11:23] = svc_one_hot * has_svc[:, np.newaxis]\n",
    "\n",
    "    # ── B3: State OHE (dims 23-27) ────────────────────────────────────────────\n",
    "    state = df['raw_state_v51'].fillna('OTHER').astype(str).str.upper() if 'raw_state_v51' in df.columns else pd.Series('OTHER', index=idx)\n",
    "    state_idx = state.map(lambda s: _STATE_IDX.get(s, _STATE_IDX['OTHER'])).values\n",
    "    X[np.arange(n), 23 + state_idx] = 1.0\n",
    "\n",
    "    # ── B4a: sport_func OHE (dims 28-34) ─────────────────────────────────────\n",
    "    sport_func = _classify_port_vec(_col('raw_sport', -1))\n",
    "    X[np.arange(n), 28 + sport_func] = 1.0\n",
    "\n",
    "    # ── B4b: dport_func OHE (dims 35-41) ─────────────────────────────────────\n",
    "    dport_func = _classify_port_vec(_col('raw_dport', -1))\n",
    "    X[np.arange(n), 35 + dport_func] = 1.0\n",
    "\n",
    "    # ── B4c: Port rarity via PowerTransformer (dims 42-43) ───────────────────\n",
    "    DEFAULT_RARITY = 1.0 / max(TOTAL_ROWS_OCEAN, 1)\n",
    "    sport_str = _col('raw_sport', -1).values.astype(int).astype(str)\n",
    "    dport_str = _col('raw_dport', -1).values.astype(int).astype(str)\n",
    "    sport_r = np.array([sport_rarity.get(p, DEFAULT_RARITY) for p in sport_str], dtype=np.float64)\n",
    "    dport_r = np.array([dport_rarity.get(p, DEFAULT_RARITY) for p in dport_str], dtype=np.float64)\n",
    "    if pt_sport is not None:\n",
    "        X[:, 42] = pt_sport.transform(sport_r.reshape(-1, 1)).ravel().astype(np.float32)\n",
    "    if pt_dport is not None:\n",
    "        X[:, 43] = pt_dport.transform(dport_r.reshape(-1, 1)).ravel().astype(np.float32)\n",
    "\n",
    "    # ── B5a: DNS (dims 44-58) gated by has_dns ────────────────────────────────\n",
    "    has_dns = _col('has_dns', 0).values.astype(np.float32)\n",
    "    qtype  = _col('dns_qtype', -1).values.astype(int)\n",
    "    qclass = _col('dns_qclass', -1).values.astype(int)\n",
    "    rcode  = _col('dns_rcode', -1).values.astype(int)\n",
    "    # qtype OHE: 10 dims (dims 44-53)\n",
    "    qtype_arr = np.zeros((n, 10), dtype=np.float32)\n",
    "    for code, idx_q in _DNS_QTYPE_MAP.items():\n",
    "        qtype_arr[qtype == code, idx_q] = 1.0\n",
    "    unknown_qt = (qtype > 0) & ~np.isin(qtype, list(_DNS_QTYPE_MAP.keys()))\n",
    "    qtype_arr[unknown_qt, 9] = 1.0  # 'other' bucket\n",
    "    X[:, 44:54] = qtype_arr * has_dns[:, np.newaxis]\n",
    "    # qclass OHE: 3 dims (dims 54-56)\n",
    "    qclass_arr = np.zeros((n, 3), dtype=np.float32)\n",
    "    qclass_arr[qclass == 1, 0] = 1.0   # IN\n",
    "    qclass_arr[qclass == 3, 1] = 1.0   # CH\n",
    "    qclass_arr[(qclass >= 0) & (qclass != 1) & (qclass != 3), 2] = 1.0  # other\n",
    "    X[:, 54:57] = qclass_arr * has_dns[:, np.newaxis]\n",
    "    # rcode 2 dims (dims 57-58)\n",
    "    X[:, 57] = ((rcode == 0) & (has_dns > 0)).astype(np.float32)\n",
    "    X[:, 58] = ((rcode > 0)  & (has_dns > 0)).astype(np.float32)\n",
    "\n",
    "    # ── B5b: HTTP (dims 59-79) gated by has_http ─────────────────────────────\n",
    "    has_http = _col('has_http', 0).values.astype(np.float32)\n",
    "    # method OHE: 8 dims (dims 59-66)\n",
    "    http_m = df['raw_http_method'].fillna('-').astype(str).str.strip().str.upper() if 'raw_http_method' in df.columns else pd.Series('-', index=idx)\n",
    "    m_idx = http_m.map(lambda m: _METHOD_IDX.get(m, _METHOD_IDX['OTHER'])).values\n",
    "    method_arr = np.zeros((n, 8), dtype=np.float32)\n",
    "    valid_m = (http_m != '-') & (http_m != '') & (http_m != 'NAN')\n",
    "    method_arr[valid_m.values, m_idx[valid_m.values]] = 1.0\n",
    "    X[:, 59:67] = method_arr * has_http[:, np.newaxis]\n",
    "    # status class OHE: 6 dims (dims 67-72): 1xx, 2xx, 3xx, 4xx, 5xx, absent\n",
    "    http_s = _col('http_status_code', -1).values.astype(int)\n",
    "    status_arr = np.zeros((n, 6), dtype=np.float32)\n",
    "    status_arr[(http_s >= 100) & (http_s < 200), 0] = 1.0\n",
    "    status_arr[(http_s >= 200) & (http_s < 300), 1] = 1.0\n",
    "    status_arr[(http_s >= 300) & (http_s < 400), 2] = 1.0\n",
    "    status_arr[(http_s >= 400) & (http_s < 500), 3] = 1.0\n",
    "    status_arr[(http_s >= 500) & (http_s < 600), 4] = 1.0\n",
    "    status_arr[http_s < 0,                        5] = 1.0  # absent\n",
    "    X[:, 67:73] = status_arr * has_http[:, np.newaxis]\n",
    "    # body lens (dims 73-74): log1p normalised to [0,1] with cap 1e7\n",
    "    req_b  = np.clip(_col('http_req_body_len', 0).values.astype(np.float64), 0, 1e7)\n",
    "    resp_b = np.clip(_col('http_resp_body_len', 0).values.astype(np.float64), 0, 1e7)\n",
    "    X[:, 73] = (np.log1p(req_b)  / np.log1p(1e7)).astype(np.float32) * has_http\n",
    "    X[:, 74] = (np.log1p(resp_b) / np.log1p(1e7)).astype(np.float32) * has_http\n",
    "    # flags (dims 75-78)\n",
    "    X[:, 75] = (valid_m.values).astype(np.float32) * has_http\n",
    "    X[:, 76] = (http_s >= 100).astype(np.float32) * has_http\n",
    "    X[:, 77] = (req_b  > 0).astype(np.float32) * has_http\n",
    "    X[:, 78] = (resp_b > 0).astype(np.float32) * has_http\n",
    "    # trans_depth norm (dim 79): not in v5.1 ocean schema → 0\n",
    "    # (dim 79 stays zero)\n",
    "\n",
    "    # ── B5c: SSL (dims 80-94) gated by has_ssl ───────────────────────────────\n",
    "    has_ssl = _col('has_ssl', 0).values.astype(np.float32)\n",
    "    # cipher OHE: 12 dims (dims 80-91)\n",
    "    ssl_c = df['raw_ssl_cipher'].fillna('').astype(str).str.strip() if 'raw_ssl_cipher' in df.columns else pd.Series('', index=idx)\n",
    "    c_idx = ssl_c.map(lambda c: _CIPHER_IDX.get(c, _CIPHER_IDX['other'])).values\n",
    "    cipher_arr = np.zeros((n, 12), dtype=np.float32)\n",
    "    cipher_arr[np.arange(n), c_idx] = 1.0\n",
    "    X[:, 80:92] = cipher_arr * has_ssl[:, np.newaxis]\n",
    "    # version (dims 92-93)\n",
    "    ssl_v = df['raw_ssl_version'].fillna('').astype(str).str.strip().str.lower().str.replace(' ', '').str.replace('.', '') if 'raw_ssl_version' in df.columns else pd.Series('', index=idx)\n",
    "    X[:, 92] = ssl_v.isin(_WEAK_SSL_VER).values.astype(np.float32) * has_ssl\n",
    "    X[:, 93] = ssl_v.isin(_STRONG_SSL_VER).values.astype(np.float32) * has_ssl\n",
    "    # established (dim 94)\n",
    "    X[:, 94] = _col('ssl_established', 0).values.astype(np.float32) * has_ssl\n",
    "\n",
    "    # ── B6: Momentum (dims 95-108) gated by has_unsw ─────────────────────────\n",
    "    has_unsw = _col('has_unsw', 0).values.astype(np.float32)\n",
    "    BLOCK6_COLS = [\n",
    "        'mom_mean', 'mom_stddev', 'mom_sum', 'mom_min', 'mom_max',\n",
    "        'mom_rate', 'mom_srate', 'mom_drate',\n",
    "        'mom_TnBPSrcIP', 'mom_TnBPDstIP',\n",
    "        'mom_TnP_PSrcIP', 'mom_TnP_PDstIP',\n",
    "        'mom_TnP_PerProto', 'mom_TnP_Per_Dport',\n",
    "    ]\n",
    "    for i, col in enumerate(BLOCK6_COLS):\n",
    "        if col in block6_scalers:\n",
    "            info  = block6_scalers[col]\n",
    "            rs    = info['scaler']\n",
    "            shift = info['shift']\n",
    "            vals  = _col(col, -1.0).values.astype(np.float64)\n",
    "            valid = vals != -1.0\n",
    "            out   = np.zeros(n, dtype=np.float32)\n",
    "            if valid.any():\n",
    "                out[valid] = rs.transform(\n",
    "                    np.log1p(vals[valid] + shift).reshape(-1, 1)\n",
    "                ).ravel().astype(np.float32)\n",
    "            X[:, 95 + i] = out * has_unsw\n",
    "\n",
    "    # ── Mask Bits (dims 109-113) ──────────────────────────────────────────────\n",
    "    X[:, 109] = has_svc\n",
    "    X[:, 110] = has_dns\n",
    "    X[:, 111] = has_http\n",
    "    X[:, 112] = has_ssl\n",
    "    X[:, 113] = has_unsw\n",
    "\n",
    "    # Replace NaN / inf that may arise from scaler edge cases\n",
    "    np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0, copy=False)\n",
    "    return X\n",
    "\n",
    "\n",
    "# ── Dimension verification ─────────────────────────────────────────────────────\n",
    "print('Verifying vectorize_v51 output dimensions …')\n",
    "\n",
    "# Build a minimal test row with all columns\n",
    "_test_row = {\n",
    "    'univ_duration': [1.5], 'univ_bytes_in': [1000.0], 'univ_bytes_out': [500.0],\n",
    "    'univ_pkts_in': [5.0], 'univ_pkts_out': [3.0],\n",
    "    'raw_proto': ['tcp'], 'raw_service': ['http'],\n",
    "    'raw_state_v51': ['ESTABLISHED'],\n",
    "    'raw_sport': [54321], 'raw_dport': [80],\n",
    "    'dns_qtype': [-1], 'dns_qclass': [-1], 'dns_rcode': [-1],\n",
    "    'raw_http_method': ['GET'], 'http_status_code': [200],\n",
    "    'http_req_body_len': [0], 'http_resp_body_len': [1024],\n",
    "    'raw_ssl_cipher': ['-'], 'raw_ssl_version': ['-'], 'ssl_established': [0],\n",
    "    'mom_mean': [-1.0], 'mom_stddev': [-1.0], 'mom_sum': [-1.0],\n",
    "    'mom_min': [-1.0], 'mom_max': [-1.0], 'mom_rate': [-1.0],\n",
    "    'mom_srate': [-1.0], 'mom_drate': [-1.0],\n",
    "    'mom_TnBPSrcIP': [-1.0], 'mom_TnBPDstIP': [-1.0],\n",
    "    'mom_TnP_PSrcIP': [-1.0], 'mom_TnP_PDstIP': [-1.0],\n",
    "    'mom_TnP_PerProto': [-1.0], 'mom_TnP_Per_Dport': [-1.0],\n",
    "    'has_svc': [1], 'has_dns': [0], 'has_http': [1], 'has_ssl': [0], 'has_unsw': [0],\n",
    "}\n",
    "_test_df = pd.DataFrame(_test_row)\n",
    "_test_X  = vectorize_v51(_test_df)\n",
    "\n",
    "assert _test_X.shape == (1, 114), f'SHAPE MISMATCH: {_test_X.shape} ≠ (1, 114)'\n",
    "assert _test_X.dtype == np.float32, f'DTYPE MISMATCH: {_test_X.dtype}'\n",
    "assert not np.any(np.isnan(_test_X)), 'NaN found in test output'\n",
    "\n",
    "print(f'  Shape: {_test_X.shape}  dtype={_test_X.dtype}  ✅')\n",
    "print(f'  Non-zero dims: {np.sum(_test_X != 0)} / 114')\n",
    "print(f'  B1[0:5]   = {_test_X[0,  0: 5].round(4)}')\n",
    "print(f'  B2a[5:11] = {_test_X[0,  5:11].round(4)}  ← tcp should be 1.0')\n",
    "print(f'  B2b[11:23]= {_test_X[0, 11:23].round(4)}  ← http(idx1) = 1.0')\n",
    "print(f'  B3[23:28] = {_test_X[0, 23:28].round(4)}  ← ESTABLISHED(idx1) = 1.0')\n",
    "print(f'  B4a[28:35]= {_test_X[0, 28:35].round(4)}  ← WEB_SERVICES(idx2) = 1.0 (sport eph)')\n",
    "print(f'  B4b[35:42]= {_test_X[0, 35:42].round(4)}  ← WEB_SERVICES(idx2) = 1.0 (dport 80)')\n",
    "print(f'  Mask[109:]= {_test_X[0, 109:].round(4)}')\n",
    "print('vectorize_v51 verified. ✅')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c2ff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "STEP 2 — SERIAL ARCHETYPE DISTILLATION\n",
      "  init='random'  n_init=1  batch_size=100,000\n",
      "=================================================================\n",
      "\n",
      "  ┌─ [EXPLOIT] ──────────────────────────────────────\n",
      "  │  Files     : 15     Rows: 2,635,460\n",
      "  │  Clusters  : 60000     Batches≈13\n",
      "  │  Pass 1 — init=random  n_init=1  batch=100,000 …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 1 done — 2,635,460 rows in 85s\n",
      "  │  Pass 2 — predict + dist-to-center (K=60,000) …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 2 done — 2,635,460 rows in 109s\n",
      "  │  Medoids   : 50,059 valid  (9941 empty clusters)\n",
      "  └─ [EXPLOIT] DONE  (194s total) → saved to disk\n",
      "\n",
      "  ┌─ [BOTNET_C2] ──────────────────────────────────────\n",
      "  │  Files     : 1,041     Rows: 61,556,313\n",
      "  │  Clusters  : 50000     Batches≈307\n",
      "  │  Pass 1 — init=random  n_init=1  batch=100,000 …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 1 done — 61,556,313 rows in 4020s\n",
      "  │  Pass 2 — predict + dist-to-center (K=50,000) …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 2 done — 61,556,313 rows in 2613s\n",
      "  │  Medoids   : 4,382 valid  (45618 empty clusters)\n",
      "  └─ [BOTNET_C2] DONE  (6633s total) → saved to disk\n",
      "\n",
      "  ┌─ [BRUTE_FORCE] ──────────────────────────────────────\n",
      "  │  Files     : 8     Rows: 1,718,568\n",
      "  │  Clusters  : 50000     Batches≈8\n",
      "  │  Pass 1 — init=random  n_init=1  batch=100,000 …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 1 done — 1,718,568 rows in 46s\n",
      "  │  Pass 2 — predict + dist-to-center (K=50,000) …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 2 done — 1,718,568 rows in 49s\n",
      "  │  Medoids   : 24,202 valid  (25798 empty clusters)\n",
      "  └─ [BRUTE_FORCE] DONE  (95s total) → saved to disk\n",
      "\n",
      "  ┌─ [SCAN] ──────────────────────────────────────\n",
      "  │  Files     : 1,182     Rows: 221,084,172\n",
      "  │  Clusters  : 30000     Batches≈1,105\n",
      "  │  Pass 1 — init=random  n_init=1  batch=100,000 …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 1 done — 221,084,172 rows in 4311s\n",
      "  │  Pass 2 — predict + dist-to-center (K=30,000) …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 2 done — 221,084,172 rows in 4034s\n",
      "  │  Medoids   : 6,572 valid  (23428 empty clusters)\n",
      "  └─ [SCAN] DONE  (8345s total) → saved to disk\n",
      "\n",
      "  ┌─ [DOS_DDOS] ──────────────────────────────────────\n",
      "  │  Files     : 537     Rows: 32,665,331\n",
      "  │  Clusters  : 30000     Batches≈163\n",
      "  │  Pass 1 — init=random  n_init=1  batch=100,000 …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 1 done — 32,665,331 rows in 624s\n",
      "  │  Pass 2 — predict + dist-to-center (K=30,000) …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 2 done — 32,665,331 rows in 605s\n",
      "  │  Medoids   : 12,840 valid  (17160 empty clusters)\n",
      "  └─ [DOS_DDOS] DONE  (1229s total) → saved to disk\n",
      "\n",
      "  ┌─ [NORMAL] ──────────────────────────────────────\n",
      "  │  Files     : 1,408     Rows: 31,657,548\n",
      "  │  Clusters  : 30000     Batches≈158\n",
      "  │  Pass 1 — init=random  n_init=1  batch=100,000 …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 1 done — 31,657,548 rows in 733s\n",
      "  │  Pass 2 — predict + dist-to-center (K=30,000) …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Pass 2 done — 31,657,548 rows in 812s\n",
      "  │  Medoids   : 4,353 valid  (25647 empty clusters)\n",
      "  └─ [NORMAL] DONE  (1545s total) → saved to disk\n",
      "\n",
      "  ┌─ [THEFT_EXFIL] ──────────────────────────────────────\n",
      "  │  Files     : 4     Rows: 97\n",
      "  │  Clusters  : all     Batches≈1\n",
      "  │  Strategy  : 100% capture (no clustering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  └  THEFT_EXFIL: 5it [00:00, 108.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  │  Captured  : 97 vectors → saved\n",
      "\n",
      "All archetypes distilled in 18069s\n",
      "\n",
      "  Archetype         Medoids  Shape\n",
      "  ──────────────────────────────────────────\n",
      "  EXPLOIT            50,059  (50059, 114)\n",
      "  BOTNET_C2           4,382  (4382, 114)\n",
      "  BRUTE_FORCE        24,202  (24202, 114)\n",
      "  SCAN                6,572  (6572, 114)\n",
      "  DOS_DDOS           12,840  (12840, 114)\n",
      "  NORMAL              4,353  (4353, 114)\n",
      "  THEFT_EXFIL            97  (97, 114)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 2 — Serial Archetype Distillation (RAM-Safe, Lightweight Init)\n",
    "#\n",
    "# For each archetype (strictly one-at-a-time):\n",
    "#   1. Init MiniBatchKMeans(init='random', n_init=1, batch_size=100_000)\n",
    "#      → replaces heavy k-means++ seeding with fast random initialization\n",
    "#   2. Pass 1 — stream all files in chunks → partial_fit(X)\n",
    "#      Warm-up guard: stack chunks until ≥ n_clusters rows, then first fit.\n",
    "#   3. Pass 2 — stream again → predict(X) → incremental medoid tracking\n",
    "#      (distance computed ONLY to each row's assigned center — O(N×114))\n",
    "#   4. Save tmp medoids → gc.collect() before next archetype\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "_READ_COLS = [\n",
    "    'univ_duration', 'univ_bytes_in', 'univ_bytes_out', 'univ_pkts_in', 'univ_pkts_out',\n",
    "    'raw_proto', 'raw_service', 'raw_state_v51', 'raw_sport', 'raw_dport',\n",
    "    'dns_qtype', 'dns_qclass', 'dns_rcode',\n",
    "    'raw_http_method', 'http_status_code', 'http_req_body_len', 'http_resp_body_len',\n",
    "    'raw_ssl_cipher', 'raw_ssl_version', 'ssl_established',\n",
    "    'mom_mean', 'mom_stddev', 'mom_sum', 'mom_min', 'mom_max',\n",
    "    'mom_rate', 'mom_srate', 'mom_drate',\n",
    "    'mom_TnBPSrcIP', 'mom_TnBPDstIP', 'mom_TnP_PSrcIP', 'mom_TnP_PDstIP',\n",
    "    'mom_TnP_PerProto', 'mom_TnP_Per_Dport',\n",
    "    'has_svc', 'has_dns', 'has_http', 'has_ssl', 'has_unsw',\n",
    "    'univ_specific_attack', 'dataset_source',\n",
    "]\n",
    "\n",
    "# ── Config (step 1 updates) ────────────────────────────────────────────────────\n",
    "MBKM_INIT        = 'random'   # fast random seeding — replaces heavy k-means++\n",
    "MBKM_N_INIT      = 1          # single init run — 3x faster than n_init=3\n",
    "MBKM_BATCH_SIZE  = 100_000    # standardized batch for stability\n",
    "\n",
    "\n",
    "def _stream_partition(part_dir, read_cols, chunk_rows=CHUNK_ROWS):\n",
    "    \"\"\"Yield (chunk_df, arch) lazily over the partition using pyarrow.dataset.\"\"\"\n",
    "    arch  = part_dir.name.split('=', 1)[1] if '=' in part_dir.name else part_dir.name\n",
    "    ds    = ads.dataset(str(part_dir), format='parquet')\n",
    "    avail = set(ds.schema.names)\n",
    "    cols  = [c for c in read_cols if c in avail]\n",
    "    for batch in ds.to_batches(batch_size=chunk_rows, columns=cols):\n",
    "        df = batch.to_pandas()\n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        df['ubt_archetype'] = arch\n",
    "        yield df, arch\n",
    "\n",
    "\n",
    "def _medoid_update(best_dist, best_vec, best_meta, X, assignments, centers, meta_df, arch):\n",
    "    \"\"\"\n",
    "    Incremental medoid tracking.\n",
    "    Computes Euclidean distance ONLY to each row's assigned center — O(N×114).\n",
    "    \"\"\"\n",
    "    center_vecs = centers[assignments]                              # (N, 114)\n",
    "    dists = np.linalg.norm(\n",
    "        X.astype(np.float64) - center_vecs.astype(np.float64), axis=1\n",
    "    )                                                               # (N,)\n",
    "    for c in np.unique(assignments):\n",
    "        mask       = assignments == c\n",
    "        local_d    = dists[mask]\n",
    "        best_local = int(np.argmin(local_d))\n",
    "        if local_d[best_local] < best_dist[c]:\n",
    "            best_dist[c] = float(local_d[best_local])\n",
    "            best_vec[c]  = X[np.where(mask)[0][best_local]].copy()\n",
    "            row = meta_df.iloc[np.where(mask)[0][best_local]]\n",
    "            best_meta[c] = {\n",
    "                'ubt_archetype'       : arch,\n",
    "                'univ_specific_attack': str(row.get('univ_specific_attack', '')),\n",
    "                'dataset_source'      : str(row.get('dataset_source', '')),\n",
    "            }\n",
    "\n",
    "\n",
    "def distil_archetype(arch, part_dir, n_clusters):\n",
    "    \"\"\"\n",
    "    Full distillation for one archetype.\n",
    "    Returns (medoid_vectors_float32, medoid_meta_df).\n",
    "    Cache-aware: skips both passes if tmp files already exist on disk.\n",
    "    \"\"\"\n",
    "    tmp_vec  = TMP_DIR / f'medoids_{arch}.npy'\n",
    "    tmp_meta = TMP_DIR / f'medoids_meta_{arch}.parquet'\n",
    "\n",
    "    if tmp_vec.exists() and tmp_meta.exists():\n",
    "        print(f'  [{arch}] CACHE HIT — loading from disk')\n",
    "        vecs = np.load(str(tmp_vec))\n",
    "        meta = pd.read_parquet(str(tmp_meta))\n",
    "        print(f'    → {len(vecs):,} medoids loaded')\n",
    "        return vecs, meta\n",
    "\n",
    "    files      = list(part_dir.glob('*.parquet'))\n",
    "    total_rows = sum(pq.read_metadata(str(f)).num_rows for f in files)\n",
    "    n_batches  = max(1, total_rows // CHUNK_ROWS)\n",
    "\n",
    "    print(f'\\n  ┌─ [{arch}] ──────────────────────────────────────')\n",
    "    print(f'  │  Files     : {len(files):,}     Rows: {total_rows:,}')\n",
    "    print(f'  │  Clusters  : {n_clusters if n_clusters else \"all\"}     '\n",
    "          f'Batches≈{n_batches:,}')\n",
    "\n",
    "    # ── THEFT_EXFIL: 100% capture ─────────────────────────────────────────────\n",
    "    if n_clusters is None:\n",
    "        print(f'  │  Strategy  : 100% capture (no clustering)')\n",
    "        chunks = []\n",
    "        for df, _ in tqdm(_stream_partition(part_dir, _READ_COLS),\n",
    "                          desc=f'  └  {arch}', leave=True):\n",
    "            X          = vectorize_v51(df)\n",
    "            meta_chunk = df[['ubt_archetype', 'univ_specific_attack', 'dataset_source']].copy()\n",
    "            meta_chunk['ubt_archetype'] = arch\n",
    "            chunks.append((X, meta_chunk))\n",
    "        if not chunks:\n",
    "            return np.zeros((0, 114), dtype=np.float32), pd.DataFrame()\n",
    "        vecs = np.vstack([c[0] for c in chunks]).astype(np.float32)\n",
    "        meta = pd.concat([c[1] for c in chunks], ignore_index=True)\n",
    "        np.save(str(tmp_vec), vecs)\n",
    "        meta.to_parquet(str(tmp_meta), index=False)\n",
    "        print(f'  │  Captured  : {len(vecs):,} vectors → saved')\n",
    "        return vecs, meta\n",
    "\n",
    "    # ── Cap clusters at total row count ───────────────────────────────────────\n",
    "    if total_rows < n_clusters:\n",
    "        print(f'  │  [WARN] total_rows={total_rows:,} < n_clusters={n_clusters:,} '\n",
    "              f'→ capping to {total_rows:,}')\n",
    "        n_clusters = total_rows\n",
    "\n",
    "    # ── MiniBatchKMeans — lightweight init ────────────────────────────────────\n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters        = n_clusters,\n",
    "        init              = MBKM_INIT,       # 'random' — no expensive k-means++ search\n",
    "        n_init            = MBKM_N_INIT,     # 1 — single initialization run\n",
    "        batch_size        = MBKM_BATCH_SIZE, # 100_000 — stable, memory-safe\n",
    "        max_no_improvement= 10,\n",
    "        max_iter          = 100,\n",
    "        random_state      = RANDOM_STATE,\n",
    "        compute_labels    = False,\n",
    "        verbose           = 0,\n",
    "    )\n",
    "\n",
    "    # ── Pass 1: partial_fit with warm-up guard ─────────────────────────────────\n",
    "    # sklearn requires n_samples >= n_clusters on every call.\n",
    "    # pyarrow to_batches() may yield small per-file batches (< n_clusters).\n",
    "    # Guard: accumulate stream chunks until buf has >= n_clusters rows,\n",
    "    # then call the first partial_fit.  All subsequent chunks feed directly.\n",
    "    t0        = time.time()\n",
    "    rows_seen = 0\n",
    "    _buf_X    = []   # vectorized chunks while warming up\n",
    "    _buf_rows = 0\n",
    "    _warmed   = False\n",
    "\n",
    "    print(f'  │  Pass 1 — init=random  n_init=1  batch={MBKM_BATCH_SIZE:,} …')\n",
    "    pbar1 = tqdm(total=total_rows, desc=f'  │  {arch} P1', unit='row',\n",
    "                 unit_scale=True, leave=False)\n",
    "\n",
    "    for df, _ in _stream_partition(part_dir, _READ_COLS):\n",
    "        X = vectorize_v51(df)\n",
    "\n",
    "        if not _warmed:\n",
    "            _buf_X.append(X)\n",
    "            _buf_rows += len(X)\n",
    "            rows_seen += len(X)\n",
    "            pbar1.update(len(X))\n",
    "\n",
    "            if _buf_rows >= n_clusters:\n",
    "                # Enough rows accumulated — fire the first partial_fit\n",
    "                X_seed = np.vstack(_buf_X)\n",
    "                kmeans.partial_fit(X_seed)\n",
    "                _warmed = True\n",
    "                del X_seed, _buf_X\n",
    "                _buf_X = []\n",
    "        else:\n",
    "            kmeans.partial_fit(X)\n",
    "            rows_seen += len(X)\n",
    "            pbar1.update(len(X))\n",
    "\n",
    "    # End-of-stream flush: total_rows < n_clusters was already capped above,\n",
    "    # but guard against edge cases where buf never reached the threshold.\n",
    "    if not _warmed and _buf_X:\n",
    "        X_seed   = np.vstack(_buf_X)\n",
    "        actual_k = min(n_clusters, len(X_seed))\n",
    "        if actual_k < n_clusters:\n",
    "            print(f'\\n  │  [WARN] Flush: {len(X_seed):,} rows → capping K '\n",
    "                  f'{n_clusters:,}→{actual_k:,}')\n",
    "            kmeans.set_params(n_clusters=actual_k)\n",
    "            n_clusters = actual_k\n",
    "        kmeans.partial_fit(X_seed)\n",
    "        del X_seed, _buf_X\n",
    "\n",
    "    pbar1.close()\n",
    "    print(f'  │  Pass 1 done — {rows_seen:,} rows in {time.time()-t0:.0f}s')\n",
    "    gc.collect()\n",
    "\n",
    "    # ── Pass 2: medoid extraction via predict + dist-to-assigned-center ────────\n",
    "    actual_k  = kmeans.cluster_centers_.shape[0]\n",
    "    centers   = kmeans.cluster_centers_.astype(np.float32)   # (K, 114)\n",
    "    best_dist = np.full(actual_k, np.inf, dtype=np.float64)\n",
    "    best_vec  = np.zeros((actual_k, 114), dtype=np.float32)\n",
    "    best_meta = [None] * actual_k\n",
    "\n",
    "    t1         = time.time()\n",
    "    rows_seen2 = 0\n",
    "    print(f'  │  Pass 2 — predict + dist-to-center (K={actual_k:,}) …')\n",
    "    pbar2 = tqdm(total=total_rows, desc=f'  │  {arch} P2', unit='row',\n",
    "                 unit_scale=True, leave=False)\n",
    "    for df, _ in _stream_partition(part_dir, _READ_COLS):\n",
    "        X           = vectorize_v51(df)\n",
    "        assignments = kmeans.predict(X)           # (N,) — fast lookup, no full transform\n",
    "        _medoid_update(best_dist, best_vec, best_meta, X, assignments, centers, df, arch)\n",
    "        rows_seen2 += len(X)\n",
    "        pbar2.update(len(X))\n",
    "    pbar2.close()\n",
    "    print(f'  │  Pass 2 done — {rows_seen2:,} rows in {time.time()-t1:.0f}s')\n",
    "\n",
    "    # ── Collect valid medoids ──────────────────────────────────────────────────\n",
    "    valid_mask = best_dist < np.inf\n",
    "    valid_vecs = best_vec[valid_mask].astype(np.float32)\n",
    "    valid_meta = [m for m, v in zip(best_meta, valid_mask) if v]\n",
    "    meta_df    = pd.DataFrame(valid_meta)\n",
    "\n",
    "    np.save(str(tmp_vec), valid_vecs)\n",
    "    meta_df.to_parquet(str(tmp_meta), index=False)\n",
    "\n",
    "    n_empty = int(np.sum(~valid_mask))\n",
    "    print(f'  │  Medoids   : {len(valid_vecs):,} valid  ({n_empty} empty clusters)')\n",
    "    print(f'  └─ [{arch}] DONE  ({time.time()-t0:.0f}s total) → saved to disk')\n",
    "    return valid_vecs, meta_df\n",
    "\n",
    "\n",
    "# ── MAIN Distillation Loop ─────────────────────────────────────────────────────\n",
    "print('=' * 65)\n",
    "print('STEP 2 — SERIAL ARCHETYPE DISTILLATION')\n",
    "print(f'  init={MBKM_INIT!r}  n_init={MBKM_N_INIT}  batch_size={MBKM_BATCH_SIZE:,}')\n",
    "print('=' * 65)\n",
    "\n",
    "archetype_results = {}\n",
    "t_total = time.time()\n",
    "\n",
    "for arch, n_clusters in ARCHETYPE_ALLOCATION.items():\n",
    "    part_dir = part_dirs.get(arch)\n",
    "    if part_dir is None:\n",
    "        print(f'\\n  [{arch}] SKIPPED — partition dir not found')\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        vecs, meta = distil_archetype(arch, part_dir, n_clusters)\n",
    "        archetype_results[arch] = (vecs, meta)\n",
    "    except Exception as e:\n",
    "        print(f'\\n  [{arch}] ERROR: {e}')\n",
    "        import traceback; traceback.print_exc()\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print(f'\\nAll archetypes distilled in {time.time()-t_total:.0f}s')\n",
    "print(f'\\n  {\"Archetype\":<14} {\"Medoids\":>10}  {\"Shape\"}')\n",
    "print('  ' + '─' * 42)\n",
    "for arch, (vecs, _) in archetype_results.items():\n",
    "    print(f'  {arch:<14} {len(vecs):>10,}  {vecs.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba79f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "STEP 3 — CONSOLIDATION\n",
      "=================================================================\n",
      "\n",
      "Loading archetype results from tmp …\n",
      "  Archetype         Medoids  Unique attacks\n",
      "  --------------------------------------------------\n",
      "  EXPLOIT            50,059  4\n",
      "  BOTNET_C2           4,382  10\n",
      "  BRUTE_FORCE        24,202  1\n",
      "  SCAN                6,572  4\n",
      "  DOS_DDOS           12,840  7\n",
      "  NORMAL              4,353  4\n",
      "  THEFT_EXFIL            97  3\n",
      "\n",
      "Saving outputs …\n",
      "  X_knowledge_vectors_v51.npy      -> c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\vectors\\X_knowledge_vectors_v51.npy\n",
      "    shape=(102505, 114)  dtype=float32  46.7 MB\n",
      "  y_knowledge_metadata_v51.parquet -> c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\main_folder\\data\\vectors\\y_knowledge_metadata_v51.parquet\n",
      "    shape=(102505, 3)  0.0 MB\n",
      "\n",
      "=================================================================\n",
      "MASTER MEDOID DISTILLATION — COMPLETE\n",
      "=================================================================\n",
      "  Ocean rows consumed  : 351,317,489\n",
      "  Total medoids        : 102,505\n",
      "  Vector dimensions    : 114\n",
      "  Compression ratio    : 3,427x\n",
      "\n",
      "  Archetype         Medoids    % of KB   Top attack variant\n",
      "  ------------------------------------------------------------------------\n",
      "  EXPLOIT            50,059     48.84%   xss [toniot]\n",
      "  BOTNET_C2           4,382      4.27%   backdoor [iot23]\n",
      "  BRUTE_FORCE        24,202     23.61%   password [toniot]\n",
      "  SCAN                6,572      6.41%   Service_Scan [botiot]\n",
      "  DOS_DDOS           12,840     12.53%   UDP [botiot]\n",
      "  NORMAL              4,353      4.25%   Benign [iot23]\n",
      "  THEFT_EXFIL            97      0.09%   Keylogging [botiot]\n",
      "  ------------------------------------------------------------------------\n",
      "  TOTAL             102,505   100.00%\n",
      "\n",
      "  Artifacts:\n",
      "    X_knowledge_vectors_v51.npy          46.7 MB\n",
      "    y_knowledge_metadata_v51.parquet      0.0 MB\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# STEP 3 — Consolidation\n",
    "#   Merge all archetype medoids → X_knowledge_vectors_v51.npy (float32)\n",
    "#   Merge metadata              → y_knowledge_metadata_v51.parquet\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "SEP = '=' * 65   # reusable separator (avoids backslash-in-f-string)\n",
    "\n",
    "print(SEP)\n",
    "print('STEP 3 — CONSOLIDATION')\n",
    "print(SEP)\n",
    "\n",
    "all_vecs  = []\n",
    "all_metas = []\n",
    "\n",
    "print('\\nLoading archetype results from tmp …')\n",
    "print(f'  {\"Archetype\":<14} {\"Medoids\":>10}  {\"Unique attacks\"}')\n",
    "print('  ' + '-' * 50)\n",
    "\n",
    "for arch in ARCHETYPE_ALLOCATION.keys():\n",
    "    tmp_vec  = TMP_DIR / f'medoids_{arch}.npy'\n",
    "    tmp_meta = TMP_DIR / f'medoids_meta_{arch}.parquet'\n",
    "\n",
    "    if not tmp_vec.exists() or not tmp_meta.exists():\n",
    "        print(f'  {arch:<14} <- MISSING tmp files — skipped')\n",
    "        continue\n",
    "\n",
    "    vecs = np.load(str(tmp_vec)).astype(np.float32)\n",
    "    meta = pd.read_parquet(str(tmp_meta))\n",
    "\n",
    "    for col in ['ubt_archetype', 'univ_specific_attack', 'dataset_source']:\n",
    "        if col not in meta.columns:\n",
    "            meta[col] = arch if col == 'ubt_archetype' else ''\n",
    "    meta['ubt_archetype'] = arch  # guarantee correct label\n",
    "\n",
    "    n_attacks = meta['univ_specific_attack'].nunique() if 'univ_specific_attack' in meta.columns else 0\n",
    "    print(f'  {arch:<14} {len(vecs):>10,}  {n_attacks}')\n",
    "\n",
    "    all_vecs.append(vecs)\n",
    "    all_metas.append(meta[['ubt_archetype', 'univ_specific_attack', 'dataset_source']])\n",
    "\n",
    "assert all_vecs, 'No archetype results to consolidate — check distillation step'\n",
    "\n",
    "# ── Concatenate ────────────────────────────────────────────────────────────────\n",
    "X_knowledge = np.vstack(all_vecs).astype(np.float32)\n",
    "y_meta      = pd.concat(all_metas, ignore_index=True)\n",
    "\n",
    "assert X_knowledge.shape[0] == len(y_meta), (\n",
    "    f'ALIGNMENT ERROR: vectors={X_knowledge.shape[0]}  meta={len(y_meta)}'\n",
    ")\n",
    "assert X_knowledge.shape[1] == 114, (\n",
    "    f'DIM ERROR: {X_knowledge.shape[1]} != 114'\n",
    ")\n",
    "assert not np.any(np.isnan(X_knowledge)), 'NaN found in X_knowledge'\n",
    "\n",
    "# ── Save outputs ───────────────────────────────────────────────────────────────\n",
    "print('\\nSaving outputs …')\n",
    "np.save(str(VECTORS_OUT_PATH), X_knowledge)\n",
    "y_meta.to_parquet(str(META_OUT_PATH), index=False)\n",
    "\n",
    "vec_mb  = os.path.getsize(VECTORS_OUT_PATH) / 1e6\n",
    "meta_mb = os.path.getsize(META_OUT_PATH) / 1e6\n",
    "\n",
    "print(f'  X_knowledge_vectors_v51.npy      -> {VECTORS_OUT_PATH}')\n",
    "print(f'    shape={X_knowledge.shape}  dtype={X_knowledge.dtype}  {vec_mb:.1f} MB')\n",
    "print(f'  y_knowledge_metadata_v51.parquet -> {META_OUT_PATH}')\n",
    "print(f'    shape={y_meta.shape}  {meta_mb:.1f} MB')\n",
    "\n",
    "# ── Final Report ───────────────────────────────────────────────────────────────\n",
    "print()\n",
    "print(SEP)\n",
    "print('MASTER MEDOID DISTILLATION — COMPLETE')\n",
    "print(SEP)\n",
    "print(f'  Ocean rows consumed  : {TOTAL_ROWS_OCEAN:,}')\n",
    "print(f'  Total medoids        : {len(X_knowledge):,}')\n",
    "print(f'  Vector dimensions    : {X_knowledge.shape[1]}')\n",
    "print(f'  Compression ratio    : {TOTAL_ROWS_OCEAN / len(X_knowledge):,.0f}x')\n",
    "\n",
    "print()\n",
    "print(f'  {\"Archetype\":<14} {\"Medoids\":>10}   {\"% of KB\":>8}   {\"Top attack variant\"}')\n",
    "print('  ' + '-' * 72)\n",
    "vc = y_meta['ubt_archetype'].value_counts()\n",
    "for arch in ARCHETYPE_ALLOCATION.keys():\n",
    "    cnt        = vc.get(arch, 0)\n",
    "    pct        = cnt / len(y_meta) * 100 if len(y_meta) else 0\n",
    "    sub        = y_meta[y_meta['ubt_archetype'] == arch]\n",
    "    top_attack = sub['univ_specific_attack'].value_counts().index[0] if cnt > 0 else 'N/A'\n",
    "    top_src    = sub['dataset_source'].value_counts().index[0] if cnt > 0 else 'N/A'\n",
    "    print(f'  {arch:<14} {cnt:>10,}   {pct:>7.2f}%   {str(top_attack)[:35]} [{top_src}]')\n",
    "print('  ' + '-' * 72)\n",
    "print(f'  {\"TOTAL\":<14} {len(X_knowledge):>10,}   100.00%')\n",
    "\n",
    "print()\n",
    "print('  Artifacts:')\n",
    "print(f'    X_knowledge_vectors_v51.npy      {vec_mb:>8.1f} MB')\n",
    "print(f'    y_knowledge_metadata_v51.parquet {meta_mb:>8.1f} MB')\n",
    "print(SEP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdd28368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ARCHETYPE DIVERSITY (Top 3 Attacks per Archetype) ---\n",
      "\n",
      "Archetype: EXPLOIT\n",
      "univ_specific_attack\n",
      "xss           45604\n",
      "injection      4282\n",
      "ransomware      157\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Archetype: BOTNET_C2\n",
      "univ_specific_attack\n",
      "backdoor         2187\n",
      "C&C               796\n",
      "C&C-HeartBeat     601\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Archetype: BRUTE_FORCE\n",
      "univ_specific_attack\n",
      "password    24202\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Archetype: SCAN\n",
      "univ_specific_attack\n",
      "Service_Scan                 5608\n",
      "PartOfAHorizontalPortScan     668\n",
      "scanning                      218\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Archetype: DOS_DDOS\n",
      "univ_specific_attack\n",
      "UDP     7890\n",
      "TCP     1779\n",
      "ddos    1777\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Archetype: NORMAL\n",
      "univ_specific_attack\n",
      "Benign    3851\n",
      "normal     481\n",
      "benign      14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Archetype: THEFT_EXFIL\n",
      "univ_specific_attack\n",
      "Keylogging           73\n",
      "FileDownload         18\n",
      "Data_Exfiltration     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- VECTOR RANGE CHECK ---\n",
      "Vector Shape: (102505, 114)\n",
      "Global Min: -2.8248 | Global Max: 169.3271\n",
      "Mean Variance: 0.2265\n"
     ]
    }
   ],
   "source": [
    "# Diversity & Alignment Check\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the consolidated metadata\n",
    "y_meta = pd.read_parquet(VECTORS_DIR / 'y_knowledge_metadata_v51.parquet')\n",
    "\n",
    "print(\"--- ARCHETYPE DIVERSITY (Top 3 Attacks per Archetype) ---\")\n",
    "for arch in y_meta['ubt_archetype'].unique():\n",
    "    print(f\"\\nArchetype: {arch}\")\n",
    "    print(y_meta[y_meta['ubt_archetype'] == arch]['univ_specific_attack'].value_counts().head(3))\n",
    "\n",
    "print(\"\\n--- VECTOR RANGE CHECK ---\")\n",
    "X_kb = np.load(VECTORS_DIR / 'X_knowledge_vectors_v51.npy')\n",
    "print(f\"Vector Shape: {X_kb.shape}\")\n",
    "print(f\"Global Min: {X_kb.min():.4f} | Global Max: {X_kb.max():.4f}\")\n",
    "print(f\"Mean Variance: {np.var(X_kb, axis=0).mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
