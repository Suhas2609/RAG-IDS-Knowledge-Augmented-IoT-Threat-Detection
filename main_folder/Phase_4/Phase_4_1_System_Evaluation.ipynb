{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fb6eef",
   "metadata": {},
   "source": [
    "\n",
    "# Phase 4.1 â€” End-to-End System Evaluation & Performance Profiling\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Evaluate the full RAG-IDS pipeline on a **reserved sequential test slice** of the `ocean_v51` corpus. No shuffling â€” temporal order within each archetype partition is preserved to maintain Phase 3.1 sliding-window integrity.\n",
    "\n",
    "## Evaluation pipeline\n",
    "\n",
    "```\n",
    "ocean_v51 (sequential slice, â‰¥50,000 rows)\n",
    "  â†’ vectorize_v51            [Phase 2.3 â€” 114-dim behavioral space]\n",
    "  â†’ get_rag_context          [Phase 2.3 â€” ChromaDB HNSW cosine retrieval]\n",
    "  â†’ AdaptiveTimeWindow       [Phase 3.1 â€” deque buffer, linear Â±steps]\n",
    "  â†’ accumulate_evidence      [Phase 3.2 â€” Dempster-Shafer sequential fusion]\n",
    "  â†’ quantum_fuse             [Phase 3.3 â€” H_Î³ interference â†’ P_fused]\n",
    "  â†’ generate_security_report [Phase 3.4 â€” deterministic alert]\n",
    "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  â†’ Classification Metrics   [Phase 4.1 â€” Accuracy, Precision, Recall, F1]\n",
    "  â†’ Confusion Matrix\n",
    "  â†’ Latency & Throughput\n",
    "  â†’ Quantum Advantage (K > 0.50 edge cases)\n",
    "```\n",
    "\n",
    "## Binary mapping\n",
    "\n",
    "| Verdict | Prediction |\n",
    "|---|---|\n",
    "| CRITICAL, WARNING, ELEVATED | 1 (Attack) |\n",
    "| MONITOR, CLEAR | 0 (Normal) |\n",
    "\n",
    "| Archetype | Ground Truth |\n",
    "|---|---|\n",
    "| NORMAL | 0 |\n",
    "| BOTNET_C2, BRUTE_FORCE, DOS_DDOS, EXPLOIT, SCAN, THEFT_EXFIL | 1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e583ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python    : 3.13.9\n",
      "NumPy     : 2.1.3\n",
      "Pandas    : 2.2.3\n",
      "ChromaDB  : 1.4.1\n",
      "scikit-learn imported âœ…\n",
      "functools  imported âœ…\n",
      "Phase 4.2 â€” System Hardening imports ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€ Cell 1: Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import sys, os, gc, time, math, pickle, warnings, json, functools\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from collections import deque\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    ")\n",
    "\n",
    "SEP  = 'â”€' * 78\n",
    "SEP2 = 'â•' * 78\n",
    "\n",
    "print(f'Python    : {sys.version.split()[0]}')\n",
    "print(f'NumPy     : {np.__version__}')\n",
    "print(f'Pandas    : {pd.__version__}')\n",
    "print(f'ChromaDB  : {chromadb.__version__}')\n",
    "print(f'scikit-learn imported âœ…')\n",
    "print(f'functools  imported âœ…')\n",
    "print('Phase 4.2 â€” System Hardening imports ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368a8f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessors_v51.pkl â€¦\n",
      "  Preprocessors loaded : 15 keys\n",
      "  Total ocean rows     : 351,317,489\n",
      "\n",
      "Connecting to ChromaDB at chromadb_store_v51 â€¦\n",
      "  Collection : ids_knowledge_base_v51\n",
      "  Medoids    : 102,505\n",
      "\n",
      "Dependencies ready âœ…\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€ Cell 2: Paths, Preprocessors & ChromaDB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "NOTEBOOK_DIR    = Path.cwd()\n",
    "MAIN_DIR        = NOTEBOOK_DIR.parent\n",
    "DATA_DIR        = MAIN_DIR / 'data'\n",
    "ARTIFACTS_DIR   = MAIN_DIR / 'artifacts'\n",
    "OCEAN_DIR       = DATA_DIR / 'unified' / 'ocean_v51'\n",
    "CHROMA_DIR      = MAIN_DIR / 'chromadb_store_v51'\n",
    "COLLECTION_NAME = 'ids_knowledge_base_v51'\n",
    "\n",
    "PREPROCESSORS_PATH = ARTIFACTS_DIR / 'preprocessors_v51.pkl'\n",
    "\n",
    "# â”€â”€ Load Preprocessors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "assert PREPROCESSORS_PATH.exists(), f'MISSING: {PREPROCESSORS_PATH}'\n",
    "print(f'Loading {PREPROCESSORS_PATH.name} â€¦')\n",
    "with open(PREPROCESSORS_PATH, 'rb') as _f:\n",
    "    PP = pickle.load(_f)\n",
    "\n",
    "block1_scalers   = PP['block1_scalers']\n",
    "block6_scalers   = PP['block6_scalers']\n",
    "qt_byte_pkt      = PP['qt_byte_pkt']\n",
    "pt_sport         = PP['pt_sport_rarity']\n",
    "pt_dport         = PP['pt_dport_rarity']\n",
    "sport_rarity     = PP['sport_rarity_map']\n",
    "dport_rarity     = PP['dport_rarity_map']\n",
    "TOTAL_ROWS_OCEAN = PP['total_rows_ocean']\n",
    "\n",
    "print(f'  Preprocessors loaded : {len(PP)} keys')\n",
    "print(f'  Total ocean rows     : {TOTAL_ROWS_OCEAN:,}')\n",
    "\n",
    "# â”€â”€ Connect to ChromaDB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "assert CHROMA_DIR.exists(), f'MISSING ChromaDB store: {CHROMA_DIR}'\n",
    "print(f'\\nConnecting to ChromaDB at {CHROMA_DIR.name} â€¦')\n",
    "_client    = PersistentClient(path=str(CHROMA_DIR))\n",
    "collection = _client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "print(f'  Collection : {collection.name}')\n",
    "print(f'  Medoids    : {collection.count():,}')\n",
    "print('\\nDependencies ready âœ…')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9160d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize_v51 and get_rag_context_batch defined âœ…\n",
      "  Output shape: (N, 114) float32  â€” v5.1 schema lock\n",
      "  RAG similarity floor: 75.0% (Phase 4.3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€ Cell 3: vectorize_v51 + get_rag_context (exact Phase 2.3 replicas) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# These replicas are bit-for-bit identical to Phase 2.3 â€” mandatory so that\n",
    "# query vectors live in the same geometric space as the stored medoids.\n",
    "#\n",
    "# Phase 4.3 addition: Absolute Similarity Floor in get_rag_context_batch().\n",
    "#   If top similarity < RAG_SIM_FLOOR (75%), override to UNKNOWN / 0.0.\n",
    "#   Prevents low-confidence RAG matches from contaminating the DS accumulator.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "TOTAL_DIMS = 114\n",
    "RAG_SIM_FLOOR = 75.0    # Phase 4.3 â€” similarity floor for RAG trust\n",
    "\n",
    "PROTO_TOKENS     = ['tcp', 'udp', 'icmp', 'arp', 'ipv6', 'other']\n",
    "SERVICE_TOKENS   = ['dns', 'http', 'ssl', 'ftp', 'ssh', 'smtp',\n",
    "                    'dhcp', 'quic', 'ntp', 'rdp', 'pop3', 'other']\n",
    "STATE_TOKENS     = ['PENDING', 'ESTABLISHED', 'REJECTED', 'RESET', 'OTHER']\n",
    "PORT_FUNC_TOKENS = ['SCADA_CONTROL', 'IOT_MANAGEMENT', 'WEB_SERVICES',\n",
    "                    'NETWORK_CORE',  'REMOTE_ACCESS',  'FUNC_EPHEMERAL', 'FUNC_UNKNOWN']\n",
    "HTTP_METHOD_TOKENS = ['GET', 'POST', 'PUT', 'DELETE', 'HEAD', 'OPTIONS', 'PATCH', 'OTHER']\n",
    "SSL_CIPHER_TOKENS  = [\n",
    "    'TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256', 'TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384',\n",
    "    'TLS_RSA_WITH_AES_128_GCM_SHA256',       'TLS_RSA_WITH_AES_256_GCM_SHA384',\n",
    "    'TLS_RSA_WITH_AES_128_CBC_SHA',          'TLS_RSA_WITH_AES_256_CBC_SHA',\n",
    "    'TLS_RSA_WITH_RC4_128_SHA',              'TLS_RSA_WITH_RC4_128_MD5',\n",
    "    'TLS_RSA_WITH_3DES_EDE_CBC_SHA',         'TLS_DHE_RSA_WITH_AES_128_CBC_SHA',\n",
    "    'TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256', 'other',\n",
    "]\n",
    "SCADA_PORTS    = frozenset({502, 102, 44818})\n",
    "IOT_MGMT_PORTS = frozenset({1883, 5683, 8883})\n",
    "WEB_PORTS      = frozenset({80, 443, 8080})\n",
    "NET_CORE_PORTS = frozenset({53, 67, 68, 123})\n",
    "REMOTE_PORTS   = frozenset({22, 23, 3389})\n",
    "_PROTO_IDX   = {t: i for i, t in enumerate(PROTO_TOKENS)}\n",
    "_SVC_IDX     = {t: i for i, t in enumerate(SERVICE_TOKENS)}\n",
    "_STATE_IDX   = {t: i for i, t in enumerate(STATE_TOKENS)}\n",
    "_METHOD_IDX  = {t: i for i, t in enumerate(HTTP_METHOD_TOKENS)}\n",
    "_CIPHER_IDX  = {t: i for i, t in enumerate(SSL_CIPHER_TOKENS)}\n",
    "_ABSENT_SVCS = frozenset({'<absent>', '-', 'unknown', '', 'none', '(empty)', 'nan'})\n",
    "_DNS_QTYPE_MAP  = {1:0, 2:1, 5:2, 6:3, 12:4, 15:5, 16:6, 28:7, 33:8, 255:9}\n",
    "_WEAK_SSL_VER   = frozenset({'sslv2','sslv3','tlsv1','tlsv10','tlsv1.0','tls1.0'})\n",
    "_STRONG_SSL_VER = frozenset({'tlsv12','tlsv13','tlsv1.2','tlsv1.3','tls1.2','tls1.3'})\n",
    "\n",
    "def _classify_port_vec(port_series):\n",
    "    p = pd.to_numeric(port_series, errors='coerce').fillna(-1).astype(int)\n",
    "    result = np.full(len(p), 6, dtype=np.int8)\n",
    "    for _, func_idx, pset in [(0,0,SCADA_PORTS),(1,1,IOT_MGMT_PORTS),(2,2,WEB_PORTS),\n",
    "                               (3,3,NET_CORE_PORTS),(4,4,REMOTE_PORTS)]:\n",
    "        result[p.isin(pset).values] = func_idx\n",
    "    result[(p.values > 49152) & (result == 6)] = 5\n",
    "    return result\n",
    "\n",
    "def vectorize_v51(df):\n",
    "    \"\"\"Map a DataFrame of v5.1-aligned ocean rows â†’ (N, 114) float32.\"\"\"\n",
    "    n = len(df); X = np.zeros((n, TOTAL_DIMS), dtype=np.float32); idx = df.index\n",
    "    def _col(name, fill=0.0):\n",
    "        return df[name].fillna(fill) if name in df.columns else pd.Series(fill, index=idx)\n",
    "    def _str_col(name, fill=''):\n",
    "        return (df[name].fillna(fill).astype(str).str.lower().str.strip()\n",
    "                if name in df.columns else pd.Series(fill, index=idx))\n",
    "    for col, mode, out_i in [('univ_duration','rs',0),('univ_bytes_in','qt',1),\n",
    "                              ('univ_bytes_out','qt',2),('univ_pkts_in','qt',3),\n",
    "                              ('univ_pkts_out','qt',4)]:\n",
    "        vals = np.clip(_col(col,0.).values.astype(np.float64), 0., None)\n",
    "        if mode=='qt' and col in qt_byte_pkt:\n",
    "            X[:,out_i] = qt_byte_pkt[col].transform(vals.reshape(-1,1)).ravel().astype(np.float32)\n",
    "        elif mode=='rs' and col in block1_scalers:\n",
    "            X[:,out_i] = block1_scalers[col].transform(np.log1p(vals).reshape(-1,1)).ravel().astype(np.float32)\n",
    "    proto = _str_col('raw_proto','other')\n",
    "    X[np.arange(n), 5 + proto.map(lambda p: _PROTO_IDX.get(p, _PROTO_IDX['other'])).values] = 1.\n",
    "    has_svc = _col('has_svc',0).values.astype(np.float32)\n",
    "    svc = _str_col('raw_service','other')\n",
    "    other_svc = _SVC_IDX['other']\n",
    "    svc_idx = svc.map(lambda s: _SVC_IDX.get(s,other_svc) if s not in _ABSENT_SVCS else other_svc).values\n",
    "    svc_ohe = np.zeros((n,12), dtype=np.float32); svc_ohe[np.arange(n), svc_idx] = 1.\n",
    "    X[:,11:23] = svc_ohe * has_svc[:,np.newaxis]\n",
    "    state = (df['raw_state_v51'].fillna('OTHER').astype(str).str.upper()\n",
    "             if 'raw_state_v51' in df.columns else pd.Series('OTHER',index=idx))\n",
    "    X[np.arange(n), 23 + state.map(lambda s: _STATE_IDX.get(s,_STATE_IDX['OTHER'])).values] = 1.\n",
    "    X[np.arange(n), 28 + _classify_port_vec(_col('raw_sport',-1))] = 1.\n",
    "    X[np.arange(n), 35 + _classify_port_vec(_col('raw_dport',-1))] = 1.\n",
    "    DEFAULT_R = 1. / max(TOTAL_ROWS_OCEAN, 1)\n",
    "    sport_str = _col('raw_sport',-1).values.astype(int).astype(str)\n",
    "    dport_str = _col('raw_dport',-1).values.astype(int).astype(str)\n",
    "    sr = np.array([sport_rarity.get(p, DEFAULT_R) for p in sport_str], dtype=np.float64)\n",
    "    dr = np.array([dport_rarity.get(p, DEFAULT_R) for p in dport_str], dtype=np.float64)\n",
    "    if pt_sport: X[:,42] = pt_sport.transform(sr.reshape(-1,1)).ravel().astype(np.float32)\n",
    "    if pt_dport: X[:,43] = pt_dport.transform(dr.reshape(-1,1)).ravel().astype(np.float32)\n",
    "    has_dns = _col('has_dns',0).values.astype(np.float32)\n",
    "    qtype = _col('dns_qtype',-1).values.astype(int)\n",
    "    qclass = _col('dns_qclass',-1).values.astype(int)\n",
    "    rcode  = _col('dns_rcode',-1).values.astype(int)\n",
    "    qt_arr = np.zeros((n,10), dtype=np.float32)\n",
    "    for code, qi in _DNS_QTYPE_MAP.items(): qt_arr[qtype==code, qi] = 1.\n",
    "    qt_arr[(qtype>0) & ~np.isin(qtype, list(_DNS_QTYPE_MAP.keys())), 9] = 1.\n",
    "    X[:,44:54] = qt_arr * has_dns[:,np.newaxis]\n",
    "    qc_arr = np.zeros((n,3), dtype=np.float32)\n",
    "    qc_arr[qclass==1,0]=1.; qc_arr[qclass==3,1]=1.\n",
    "    qc_arr[(qclass>=0)&(qclass!=1)&(qclass!=3),2]=1.\n",
    "    X[:,54:57] = qc_arr * has_dns[:,np.newaxis]\n",
    "    X[:,57] = ((rcode==0)&(has_dns>0)).astype(np.float32)\n",
    "    X[:,58] = ((rcode>0)&(has_dns>0)).astype(np.float32)\n",
    "    has_http = _col('has_http',0).values.astype(np.float32)\n",
    "    http_m = (df['raw_http_method'].fillna('-').astype(str).str.strip().str.upper()\n",
    "              if 'raw_http_method' in df.columns else pd.Series('-',index=idx))\n",
    "    m_idx = http_m.map(lambda m: _METHOD_IDX.get(m,_METHOD_IDX['OTHER'])).values\n",
    "    m_arr = np.zeros((n,8), dtype=np.float32)\n",
    "    valid_m = (http_m!='-')&(http_m!='')&(http_m!='NAN')\n",
    "    m_arr[valid_m.values, m_idx[valid_m.values]] = 1.\n",
    "    X[:,59:67] = m_arr * has_http[:,np.newaxis]\n",
    "    http_s = _col('http_status_code',-1).values.astype(int)\n",
    "    s_arr = np.zeros((n,6), dtype=np.float32)\n",
    "    s_arr[(http_s>=100)&(http_s<200),0]=1.; s_arr[(http_s>=200)&(http_s<300),1]=1.\n",
    "    s_arr[(http_s>=300)&(http_s<400),2]=1.; s_arr[(http_s>=400)&(http_s<500),3]=1.\n",
    "    s_arr[(http_s>=500)&(http_s<600),4]=1.; s_arr[http_s<0,5]=1.\n",
    "    X[:,67:73] = s_arr * has_http[:,np.newaxis]\n",
    "    req_b  = np.clip(_col('http_req_body_len',0).values.astype(np.float64),0,1e7)\n",
    "    resp_b = np.clip(_col('http_resp_body_len',0).values.astype(np.float64),0,1e7)\n",
    "    X[:,73] = (np.log1p(req_b)/np.log1p(1e7)).astype(np.float32)*has_http\n",
    "    X[:,74] = (np.log1p(resp_b)/np.log1p(1e7)).astype(np.float32)*has_http\n",
    "    X[:,75] = valid_m.values.astype(np.float32)*has_http\n",
    "    X[:,76] = (http_s>=100).astype(np.float32)*has_http\n",
    "    X[:,77] = (req_b>0).astype(np.float32)*has_http\n",
    "    X[:,78] = (resp_b>0).astype(np.float32)*has_http\n",
    "    has_ssl = _col('has_ssl',0).values.astype(np.float32)\n",
    "    ssl_c = (df['raw_ssl_cipher'].fillna('').astype(str).str.strip()\n",
    "             if 'raw_ssl_cipher' in df.columns else pd.Series('',index=idx))\n",
    "    c_arr = np.zeros((n,12), dtype=np.float32)\n",
    "    c_arr[np.arange(n), ssl_c.map(lambda c: _CIPHER_IDX.get(c,_CIPHER_IDX['other'])).values] = 1.\n",
    "    X[:,80:92] = c_arr * has_ssl[:,np.newaxis]\n",
    "    ssl_v = (df['raw_ssl_version'].fillna('').astype(str).str.strip().str.lower()\n",
    "              .str.replace(' ','').str.replace('.','') if 'raw_ssl_version' in df.columns\n",
    "              else pd.Series('',index=idx))\n",
    "    X[:,92] = ssl_v.isin(_WEAK_SSL_VER).values.astype(np.float32)*has_ssl\n",
    "    X[:,93] = ssl_v.isin(_STRONG_SSL_VER).values.astype(np.float32)*has_ssl\n",
    "    X[:,94] = _col('ssl_established',0).values.astype(np.float32)*has_ssl\n",
    "    has_unsw = _col('has_unsw',0).values.astype(np.float32)\n",
    "    BLOCK6 = ['mom_mean','mom_stddev','mom_sum','mom_min','mom_max','mom_rate',\n",
    "               'mom_srate','mom_drate','mom_TnBPSrcIP','mom_TnBPDstIP',\n",
    "               'mom_TnP_PSrcIP','mom_TnP_PDstIP','mom_TnP_PerProto','mom_TnP_Per_Dport']\n",
    "    for i, col in enumerate(BLOCK6):\n",
    "        if col in block6_scalers:\n",
    "            info = block6_scalers[col]; rs = info['scaler']; shift = info['shift']\n",
    "            vals = _col(col,-1.).values.astype(np.float64); valid = vals != -1.\n",
    "            out  = np.zeros(n, dtype=np.float32)\n",
    "            if valid.any():\n",
    "                out[valid] = rs.transform(np.log1p(vals[valid]+shift).reshape(-1,1)).ravel().astype(np.float32)\n",
    "            X[:,95+i] = out * has_unsw\n",
    "    X[:,109]=has_svc; X[:,110]=has_dns; X[:,111]=has_http; X[:,112]=has_ssl; X[:,113]=has_unsw\n",
    "    np.nan_to_num(X, nan=0., posinf=0., neginf=0., copy=False)\n",
    "    return X\n",
    "\n",
    "def _l2_normalise(X: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    norms = np.where(norms == 0., 1., norms)\n",
    "    return (X / norms).astype(np.float32)\n",
    "\n",
    "def get_rag_context_batch(X_norm: np.ndarray, n_results: int = 3) -> list:\n",
    "    \"\"\"\n",
    "    Query ChromaDB for a batch of pre-normalised vectors.\n",
    "\n",
    "    Phase 4.3 â€” Absolute Similarity Floor:\n",
    "      If the top neighbour's similarity_pct < RAG_SIM_FLOOR (75%),\n",
    "      the packet is too far from any known medoid.  Override to UNKNOWN\n",
    "      so the downstream BPA maps it to pure uncertainty (m_U = 1.0).\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_embeddings=X_norm.tolist(),\n",
    "        n_results=n_results,\n",
    "        include=['metadatas', 'distances'],\n",
    "    )\n",
    "    outputs = []\n",
    "    for i in range(len(X_norm)):\n",
    "        neighbours = []\n",
    "        for rank, (meta, dist) in enumerate(\n",
    "            zip(results['metadatas'][i], results['distances'][i]), start=1\n",
    "        ):\n",
    "            dist_c  = max(0., float(dist))\n",
    "            sim_pct = (1. - dist_c / 2.) * 100.\n",
    "            neighbours.append({\n",
    "                'rank'          : rank,\n",
    "                'archetype'     : meta.get('ubt_archetype',       'UNKNOWN'),\n",
    "                'attack_variant': meta.get('univ_specific_attack', 'UNKNOWN'),\n",
    "                'dataset_source': meta.get('dataset_source',       'UNKNOWN'),\n",
    "                'distance'      : round(dist_c, 6),\n",
    "                'similarity_pct': round(sim_pct, 2),\n",
    "            })\n",
    "        top = neighbours[0] if neighbours else {}\n",
    "        top_sim = top.get('similarity_pct', 0.)\n",
    "\n",
    "        # â”€â”€ Phase 4.3: Absolute Similarity Floor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if top_sim < RAG_SIM_FLOOR:\n",
    "            outputs.append({\n",
    "                'top_archetype' : 'UNKNOWN',\n",
    "                'top_attack'    : 'UNKNOWN',\n",
    "                'top_similarity': 0.0,\n",
    "                'top_distance'  : top.get('distance', 2.0),\n",
    "            })\n",
    "        else:\n",
    "            outputs.append({\n",
    "                'top_archetype' : top.get('archetype',      'UNKNOWN'),\n",
    "                'top_attack'    : top.get('attack_variant', 'UNKNOWN'),\n",
    "                'top_similarity': top_sim,\n",
    "                'top_distance'  : top.get('distance',       2.0),\n",
    "            })\n",
    "    return outputs\n",
    "\n",
    "print('vectorize_v51 and get_rag_context_batch defined âœ…')\n",
    "print(f'  Output shape: (N, {TOTAL_DIMS}) float32  â€” v5.1 schema lock')\n",
    "print(f'  RAG similarity floor: {RAG_SIM_FLOOR}% (Phase 4.3)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e863d6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Slate smoke-test: buffer after 3Ã—high-sim benign = 1 flow âœ…\n",
      "Vectorised BPA smoke-test: shape=(2, 3),  row-sumsâ‰ˆ1 âœ…\n",
      "UNKNOWN mask smoke-test: m_A=0.0, m_N=0.0, m_U=1.0 âœ…\n",
      "Quantum blend smoke-test (NORMAL@100%): P_fused=0.0500, quantum_raw=0.7226, verdict=CLEAR âœ…\n",
      "Quantum blend smoke-test (ATTACK@95%): P_fused=0.9025, quantum_raw=0.6410, verdict=CRITICAL âœ…\n",
      "accumulate_evidence smoke-test: {'belief_attack': 0.40262719, 'belief_normal': 0.51271893, 'uncertainty': 0.08465388, 'conflict_level': 0.61632} âœ…\n",
      "\n",
      "Phase 4.2/4.3/4.4 hardened engine loaded âœ…\n",
      "  âœ… AdaptiveTimeWindow â€” Clean Slate (streak â‰¥ 3 @ sim â‰¥ 85%)\n",
      "  âœ… Phase 4.4: ingest() amortised O(1) popleft\n",
      "  âœ… compute_bpa_batch  â€” vectorised NumPy (NÃ—3) + UNKNOWN mask\n",
      "  âœ… accumulate_evidence â€” vectorised BPA + scalar Dempster fold\n",
      "  âœ… Phase 4.3: UNKNOWN archetype â†’ pure uncertainty (FPR fix)\n",
      "  âœ… Phase 4.3: Conflict-weighted quantum-classical blend (FPR fix)\n",
      "  âœ… Phase 4.4: Ï€/2 phase encoding (softened destructive interference)\n",
      "  âœ… Phase 4.3: UNKNOWN archetype â†’ pure uncertainty (FPR fix)\n",
      "  âœ… Phase 4.3: Conflict-weighted quantum-classical blend (FPR fix)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€ Cell 4: Phase 4.2/4.3/4.4 â€” Hardened Phase 3 Engine Replicas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# Target 1 â€” AdaptiveTimeWindow: \"Clean Slate\" protocol\n",
    "#   _benign_streak counter; buffer flushed after 3 consecutive high-confidence\n",
    "#   benign flows (top_archetype âˆˆ NORMAL_ARCHETYPES AND top_similarity â‰¥ 85%).\n",
    "#   Phase 4.4: ingest() uses amortised O(1) popleft instead of O(N) filter.\n",
    "#\n",
    "# Target 2 â€” Vectorized BPA + Dempster-Shafer + UNKNOWN mask:\n",
    "#   compute_bpa_batch() â†’ (N, 3) NumPy array; UNKNOWN â†’ pure uncertainty.\n",
    "#   accumulate_evidence() â†’ scalar Dempster fold.\n",
    "#\n",
    "# Target 3 (Phase 4.3) â€” Conflict-Weighted Quantum-Classical Blend:\n",
    "#   P_final = K Ã— P_quantum + (1 âˆ’ K) Ã— P_classical\n",
    "#\n",
    "# Target 4 (Phase 4.4) â€” Ï€/2 phase encoding:\n",
    "#   Bound quantum phase rotation from Ï€ to Ï€/2 to prevent complete\n",
    "#   amplitude inversion during high-conflict edge cases.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "VECTOR_DIMS      = 114\n",
    "THRESH_CRITICAL  = 0.90\n",
    "THRESH_WARNING   = 0.70\n",
    "THRESH_CLEAR     = 0.30\n",
    "THRESH_MONITOR_U = 0.40\n",
    "CONFLICT_EPS     = 1e-9\n",
    "MIN_UNCERTAINTY  = 1e-6\n",
    "NORMAL_ARCHETYPES = frozenset({'NORMAL', 'benign', 'normal'})\n",
    "\n",
    "# â”€â”€ FlowRecord â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@dataclass\n",
    "class FlowRecord:\n",
    "    timestamp     : float\n",
    "    vector        : np.ndarray\n",
    "    top_similarity: float\n",
    "    top_archetype : str\n",
    "    top_attack    : str\n",
    "    packet_label  : str\n",
    "    rag_context   : str = ''\n",
    "\n",
    "    def __post_init__(self):\n",
    "        arr = np.asarray(self.vector, dtype=np.float32)\n",
    "        if arr.shape != (VECTOR_DIMS,):\n",
    "            raise ValueError(f'vector must be ({VECTOR_DIMS},), got {arr.shape}')\n",
    "        object.__setattr__(self, 'vector', arr)\n",
    "\n",
    "# â”€â”€ Target 1: AdaptiveTimeWindow â€” Clean Slate Protocol â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@dataclass\n",
    "class AdaptiveTimeWindow:\n",
    "    \"\"\"\n",
    "    Deque-backed sliding window with Clean Slate protocol.\n",
    "\n",
    "    Clean Slate: After BENIGN_STREAK_THRESHOLD consecutive flows whose\n",
    "    top_archetype âˆˆ NORMAL_ARCHETYPES AND top_similarity â‰¥ BENIGN_SIM_THRESHOLD,\n",
    "    the buffer and window size are reset to their initial state.\n",
    "    \"\"\"\n",
    "    min_size_s             : float = 5.0\n",
    "    max_size_s             : float = 30.0\n",
    "    step_up_s              : float = 2.0\n",
    "    step_down_s            : float = 1.0\n",
    "    max_buffer             : int   = 1000\n",
    "    BENIGN_STREAK_THRESHOLD: int   = 3\n",
    "    BENIGN_SIM_THRESHOLD   : float = 85.0\n",
    "\n",
    "    _current_size_s : float = field(init=False)\n",
    "    _buffer         : deque = field(init=False)\n",
    "    _benign_streak  : int   = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self._current_size_s = self.min_size_s\n",
    "        self._buffer         = deque(maxlen=self.max_buffer)\n",
    "        self._benign_streak  = 0\n",
    "\n",
    "    @property\n",
    "    def window_size_s(self) -> float:\n",
    "        return self._current_size_s\n",
    "\n",
    "    def ingest(self, flow: FlowRecord) -> List[FlowRecord]:\n",
    "        # â”€â”€ Streak evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if (flow.top_archetype in NORMAL_ARCHETYPES\n",
    "                and flow.top_similarity >= self.BENIGN_SIM_THRESHOLD):\n",
    "            self._benign_streak += 1\n",
    "        else:\n",
    "            self._benign_streak = 0\n",
    "\n",
    "        # â”€â”€ Clean Slate trigger â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if self._benign_streak >= self.BENIGN_STREAK_THRESHOLD:\n",
    "            self._buffer.clear()\n",
    "            self._current_size_s = self.min_size_s\n",
    "            self._benign_streak  = 0\n",
    "            self._buffer.append(flow)\n",
    "            return list(self._buffer)\n",
    "\n",
    "        self._buffer.append(flow)\n",
    "        cutoff = flow.timestamp - self._current_size_s\n",
    "        # Phase 4.4: amortised O(1) left-pop instead of O(N) list comprehension\n",
    "        while self._buffer and self._buffer[0].timestamp < cutoff:\n",
    "            self._buffer.popleft()\n",
    "        return list(self._buffer)\n",
    "\n",
    "    def adapt(self, verdict: str) -> None:\n",
    "        if verdict in ('CRITICAL', 'WARNING'):\n",
    "            self._current_size_s = min(\n",
    "                self._current_size_s + self.step_up_s, self.max_size_s)\n",
    "        elif verdict == 'CLEAR':\n",
    "            self._current_size_s = max(\n",
    "                self._current_size_s - self.step_down_s, self.min_size_s)\n",
    "\n",
    "# â”€â”€ Target 2a: Vectorized BPA computation â†’ (N, 3) NumPy array â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def compute_bpa_batch(buffer: List[FlowRecord]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorised BPA with UNKNOWN mask (Phase 4.3).\n",
    "    UNKNOWN â†’ pure uncertainty: m_A=0, m_N=0, m_U=1.\n",
    "    Returns float64 (N, 3) array â€” columns: [m_A, m_N, m_U], rows sum to 1.\n",
    "    \"\"\"\n",
    "    sims  = np.fromiter((f.top_similarity for f in buffer),\n",
    "                        dtype=np.float64, count=len(buffer))\n",
    "    archs = [f.top_archetype for f in buffer]\n",
    "\n",
    "    sim_n      = np.clip(sims, 0., 100.) / 100.\n",
    "    is_normal  = np.array([a in NORMAL_ARCHETYPES for a in archs], dtype=bool)\n",
    "    is_unknown = np.array([a == 'UNKNOWN' for a in archs], dtype=bool)\n",
    "\n",
    "    m_A = np.where(is_normal, sim_n * 0.05, sim_n * 0.95)\n",
    "    m_N = np.where(is_normal, sim_n * 0.90, sim_n * 0.02)\n",
    "    m_U = np.maximum(MIN_UNCERTAINTY, 1.0 - m_A - m_N)\n",
    "\n",
    "    # Phase 4.3: UNKNOWN â†’ pure uncertainty\n",
    "    m_A[is_unknown] = 0.0\n",
    "    m_N[is_unknown] = 0.0\n",
    "    m_U[is_unknown] = 1.0\n",
    "\n",
    "    totals = m_A + m_N + m_U\n",
    "    m_A /= totals;  m_N /= totals;  m_U /= totals\n",
    "\n",
    "    return np.column_stack([m_A, m_N, m_U])\n",
    "\n",
    "# â”€â”€ Target 2b: accumulate_evidence â€” scalar loop over pre-computed BPA matrix â”€\n",
    "def accumulate_evidence(buffer: List[FlowRecord]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Phase 4.2 â€” vectorised BPA + scalar Dempster fold.\n",
    "    \"\"\"\n",
    "    bpas = compute_bpa_batch(buffer)\n",
    "\n",
    "    m_A, m_N, m_U = float(bpas[0, 0]), float(bpas[0, 1]), float(bpas[0, 2])\n",
    "    cum_K = 0.0\n",
    "\n",
    "    for i in range(1, len(bpas)):\n",
    "        b2_A = float(bpas[i, 0])\n",
    "        b2_N = float(bpas[i, 1])\n",
    "        b2_U = float(bpas[i, 2])\n",
    "\n",
    "        K     = m_A * b2_N + b2_A * m_N\n",
    "        denom = 1.0 - K\n",
    "        if K > cum_K:\n",
    "            cum_K = K\n",
    "\n",
    "        if denom < CONFLICT_EPS:\n",
    "            m_A, m_N, m_U = 0.5, 0.5, MIN_UNCERTAINTY\n",
    "            continue\n",
    "\n",
    "        new_A = (m_A * b2_A  +  m_A * b2_U  +  b2_A * m_U) / denom\n",
    "        new_N = (m_N * b2_N  +  m_N * b2_U  +  b2_N * m_U) / denom\n",
    "        new_U = (m_U * b2_U) / denom\n",
    "        m_A   = new_A\n",
    "        m_N   = new_N\n",
    "        m_U   = new_U if new_U > MIN_UNCERTAINTY else MIN_UNCERTAINTY\n",
    "\n",
    "    return {\n",
    "        'belief_attack' : round(m_A,   8),\n",
    "        'belief_normal' : round(m_N,   8),\n",
    "        'uncertainty'   : round(m_U,   8),\n",
    "        'conflict_level': round(cum_K, 8),\n",
    "    }\n",
    "\n",
    "# â”€â”€ Quantum Fusion â€” Phase 4.3/4.4: Conflict-Weighted Blend + Ï€/2 Encoding â”€â”€â”€â”€\n",
    "_VERDICT_ICONS = {'CRITICAL':'ğŸ”´','WARNING':'ğŸŸ ','ELEVATED':'ğŸŸ¡','MONITOR':'âšª','CLEAR':'ğŸŸ¢'}\n",
    "\n",
    "def generate_ids_verdict(P: float, U: float) -> Dict:\n",
    "    if   P >= THRESH_CRITICAL: v='CRITICAL'; desc=f'P={P:.4f}â‰¥{THRESH_CRITICAL}'\n",
    "    elif P >= THRESH_WARNING:  v='WARNING';  desc=f'P={P:.4f}â‰¥{THRESH_WARNING}'\n",
    "    elif U > THRESH_MONITOR_U: v='MONITOR';  desc=f'U={U:.4f}>{THRESH_MONITOR_U}'\n",
    "    elif P < THRESH_CLEAR:     v='CLEAR';    desc=f'P={P:.4f}<{THRESH_CLEAR}'\n",
    "    else:                       v='ELEVATED'; desc=f'P={P:.4f} elevated'\n",
    "    return {'verdict':v, 'icon':_VERDICT_ICONS.get(v,'âš«'),\n",
    "            'confidence':round(1.-U,4), 'description':desc}\n",
    "\n",
    "def quantum_fuse(evidence: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Quantum Knowledge Fusion with Phase 4.3 conflict-weighted blend\n",
    "    and Phase 4.4 Ï€/2 phase encoding.\n",
    "\n",
    "    Phase 4.3: Blend quantum output with classical DS evidence weighted by K:\n",
    "        P_final = K Ã— P_quantum + (1 âˆ’ K) Ã— m_A\n",
    "    Phase 4.4: Bound phase rotation from Ï€ to Ï€/2 to prevent complete\n",
    "    amplitude inversion in high-conflict edge cases.\n",
    "    \"\"\"\n",
    "    m_A = evidence['belief_attack'];  m_N = evidence['belief_normal']\n",
    "    m_U = evidence['uncertainty'];    K   = evidence['conflict_level']\n",
    "\n",
    "    # â”€â”€ Quantum state preparation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Phase 4.4: bound rotation to Ï€/2 to prevent complete amplitude inversion\n",
    "    alpha = math.sqrt(max(m_A, 0.)) * np.exp(1j * (math.pi / 2.0) * m_U)\n",
    "    beta  = math.sqrt(max(m_N, 0.)) * np.exp(1j * (math.pi / 2.0) * K)\n",
    "    psi   = np.array([alpha, beta], dtype=np.complex128)\n",
    "    norm  = np.linalg.norm(psi)\n",
    "    if norm > 1e-15:\n",
    "        psi /= norm\n",
    "    prob_atk_raw = float(np.abs(psi[0])**2)\n",
    "\n",
    "    # â”€â”€ Hadamard interference gate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    gamma   = 1.0 - K\n",
    "    inv_rt2 = 1. / math.sqrt(2.)\n",
    "    H       = inv_rt2 * np.array([[1., gamma], [gamma, -1.]], dtype=np.complex128)\n",
    "    psi_p   = H @ psi\n",
    "    np_     = np.linalg.norm(psi_p)\n",
    "    if np_ > 1e-15:\n",
    "        psi_p /= np_\n",
    "    p_atk_quantum = float(np.abs(psi_p[0])**2)\n",
    "    p_nrm_quantum = float(np.abs(psi_p[1])**2)\n",
    "\n",
    "    # â”€â”€ Phase 4.3: Conflict-weighted quantum-classical blend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    #    Low K  â†’ trust classical DS directly (no false Hadamard inflation).\n",
    "    #    High K â†’ trust quantum disambiguation.\n",
    "    p_atk = K * p_atk_quantum + (1.0 - K) * m_A\n",
    "    p_nrm = 1.0 - p_atk\n",
    "\n",
    "    itype = ('CONSTRUCTIVE' if p_atk > prob_atk_raw + 0.01\n",
    "             else ('DESTRUCTIVE' if p_atk < prob_atk_raw - 0.01 else 'MIXED'))\n",
    "    vd = generate_ids_verdict(p_atk, m_U)\n",
    "    return {**evidence,\n",
    "            'fused_probability': round(p_atk, 6),\n",
    "            'prob_normal_post' : round(p_nrm, 6),\n",
    "            'coherence_factor' : round(gamma, 6),\n",
    "            'interference_type': itype,\n",
    "            'quantum_raw'      : round(p_atk_quantum, 6),\n",
    "            'verdict'          : vd['verdict'],\n",
    "            'icon'             : vd['icon'],\n",
    "            'confidence'       : vd['confidence']}\n",
    "\n",
    "# â”€â”€ Smoke-tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _make_flow(arch, sim):\n",
    "    f = FlowRecord(0., np.zeros(114), sim, arch, 'none', arch)\n",
    "    return f\n",
    "\n",
    "# Clean Slate smoke-test\n",
    "_atw_test = AdaptiveTimeWindow()\n",
    "for _ in range(3):\n",
    "    _buf_out = _atw_test.ingest(_make_flow('NORMAL', 90.))\n",
    "assert len(_buf_out) == 1, 'Expected single-element buffer after Clean Slate flush'\n",
    "print(f'Clean Slate smoke-test: buffer after 3Ã—high-sim benign = {len(_buf_out)} flow âœ…')\n",
    "\n",
    "# Vectorised BPA smoke-test\n",
    "_test_buf = [_make_flow('BOTNET_C2', 80.), _make_flow('NORMAL', 90.)]\n",
    "_bpas     = compute_bpa_batch(_test_buf)\n",
    "assert _bpas.shape == (2, 3) and np.allclose(_bpas.sum(axis=1), 1.)\n",
    "print(f'Vectorised BPA smoke-test: shape={_bpas.shape},  row-sumsâ‰ˆ1 âœ…')\n",
    "\n",
    "# Phase 4.3 â€” UNKNOWN mask smoke-test\n",
    "_unk_buf = [_make_flow('UNKNOWN', 0.0), _make_flow('BOTNET_C2', 95.)]\n",
    "_unk_bpas = compute_bpa_batch(_unk_buf)\n",
    "assert _unk_bpas[0, 0] == 0.0 and _unk_bpas[0, 1] == 0.0 and _unk_bpas[0, 2] == 1.0, \\\n",
    "    f'UNKNOWN flow must be pure uncertainty, got {_unk_bpas[0]}'\n",
    "print(f'UNKNOWN mask smoke-test: m_A={_unk_bpas[0,0]:.1f}, m_N={_unk_bpas[0,1]:.1f}, m_U={_unk_bpas[0,2]:.1f} âœ…')\n",
    "\n",
    "# Phase 4.3 â€” Quantum blend smoke-test (NORMAL should map to CLEAR)\n",
    "_norm_buf = [_make_flow('NORMAL', 100.)]\n",
    "_norm_ev  = accumulate_evidence(_norm_buf)\n",
    "_norm_qf  = quantum_fuse(_norm_ev)\n",
    "print(f'Quantum blend smoke-test (NORMAL@100%): P_fused={_norm_qf[\"fused_probability\"]:.4f}, '\n",
    "      f'quantum_raw={_norm_qf[\"quantum_raw\"]:.4f}, verdict={_norm_qf[\"verdict\"]} âœ…')\n",
    "assert _norm_qf['verdict'] in ('CLEAR', 'MONITOR'), \\\n",
    "    f'NORMAL@100% should be CLEAR/MONITOR, got {_norm_qf[\"verdict\"]}'\n",
    "\n",
    "# Phase 4.3 â€” Quantum blend smoke-test (ATTACK should stay CRITICAL)\n",
    "_atk_buf = [_make_flow('BOTNET_C2', 95.)]\n",
    "_atk_ev  = accumulate_evidence(_atk_buf)\n",
    "_atk_qf  = quantum_fuse(_atk_ev)\n",
    "print(f'Quantum blend smoke-test (ATTACK@95%): P_fused={_atk_qf[\"fused_probability\"]:.4f}, '\n",
    "      f'quantum_raw={_atk_qf[\"quantum_raw\"]:.4f}, verdict={_atk_qf[\"verdict\"]} âœ…')\n",
    "assert _atk_qf['verdict'] == 'CRITICAL', \\\n",
    "    f'ATTACK@95% should be CRITICAL, got {_atk_qf[\"verdict\"]}'\n",
    "\n",
    "# accumulate_evidence returns valid dict keys\n",
    "_ev = accumulate_evidence(_test_buf)\n",
    "assert set(_ev) == {'belief_attack','belief_normal','uncertainty','conflict_level'}\n",
    "print(f'accumulate_evidence smoke-test: {_ev} âœ…')\n",
    "\n",
    "print()\n",
    "print('Phase 4.2/4.3/4.4 hardened engine loaded âœ…')\n",
    "print('  âœ… AdaptiveTimeWindow â€” Clean Slate (streak â‰¥ 3 @ sim â‰¥ 85%)')\n",
    "print('  âœ… Phase 4.4: ingest() amortised O(1) popleft')\n",
    "print('  âœ… compute_bpa_batch  â€” vectorised NumPy (NÃ—3) + UNKNOWN mask')\n",
    "print('  âœ… accumulate_evidence â€” vectorised BPA + scalar Dempster fold')\n",
    "print('  âœ… Phase 4.3: UNKNOWN archetype â†’ pure uncertainty (FPR fix)')\n",
    "print('  âœ… Phase 4.3: Conflict-weighted quantum-classical blend (FPR fix)')\n",
    "print('  âœ… Phase 4.4: Ï€/2 phase encoding (softened destructive interference)')\n",
    "\n",
    "\n",
    "print('  âœ… Phase 4.3: UNKNOWN archetype â†’ pure uncertainty (FPR fix)')\n",
    "print('  âœ… Phase 4.3: Conflict-weighted quantum-classical blend (FPR fix)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93b62c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 8,400 rows Ã— 7 archetypes â€¦\n",
      "  BOTNET_C2      : 8,400 rows\n",
      "  BRUTE_FORCE    : 8,400 rows\n",
      "  DOS_DDOS       : 8,400 rows\n",
      "  EXPLOIT        : 8,400 rows\n",
      "  NORMAL         : 8,400 rows\n",
      "  SCAN           : 8,400 rows\n",
      "  THEFT_EXFIL    : 97 rows\n",
      "\n",
      "Total test slice : 50,497 rows\n",
      "  attack (1)     : 42,097\n",
      "  normal (0)     : 8,400\n",
      "  feature columns: 39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€ Cell 5: Load Sequential Test Slice â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# Phase 4.4: 8,400 rows Ã— 7 archetypes â†’ 50,497 total (THEFT_EXFIL capped at 97).\n",
    "#   â€¢ For each archetype, list sorted parquet files and TAIL from the last file(s).\n",
    "#   â€¢ No shuffle â€” preserves natural temporal ordering within each archetype.\n",
    "#   â€¢ vectorize_v51 reads whichever columns it needs; all 44 ocean columns present.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "N_PER_ARCH = 8_400\n",
    "ARCHETYPES  = ['BOTNET_C2','BRUTE_FORCE','DOS_DDOS','EXPLOIT',\n",
    "               'NORMAL','SCAN','THEFT_EXFIL']\n",
    "\n",
    "def _tail_parquet(arch: str, n: int) -> pd.DataFrame:\n",
    "    \"\"\"Stream the last ``n`` rows from an archetype partition (sorted files).\"\"\"\n",
    "    pdir  = OCEAN_DIR / f'ubt_archetype={arch}'\n",
    "    files = sorted(pdir.glob('*.parquet'))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f'No parquet files under {pdir}')\n",
    "    rows_needed = n\n",
    "    chunks = []\n",
    "    for f in reversed(files):          # walk backwards through time\n",
    "        if rows_needed <= 0:\n",
    "            break\n",
    "        df_f = pd.read_parquet(f, engine='pyarrow')   # all 44 native columns\n",
    "        take = df_f.tail(rows_needed)\n",
    "        chunks.append(take)\n",
    "        rows_needed -= len(take)\n",
    "    # restore temporal order inside this archetype\n",
    "    df_arch = pd.concat(reversed(chunks), ignore_index=True)\n",
    "    df_arch['ubt_archetype'] = arch\n",
    "    df_arch['ground_truth']  = 0 if arch == 'NORMAL' else 1\n",
    "    return df_arch\n",
    "\n",
    "print(f'Loading {N_PER_ARCH:,} rows Ã— {len(ARCHETYPES)} archetypes â€¦')\n",
    "arch_dfs = []\n",
    "for arch in ARCHETYPES:\n",
    "    try:\n",
    "        df_a = _tail_parquet(arch, N_PER_ARCH)\n",
    "        arch_dfs.append(df_a)\n",
    "        print(f'  {arch:15s}: {len(df_a):,} rows')\n",
    "    except Exception as exc:\n",
    "        print(f'  {arch:15s}: ERROR â€” {exc}')\n",
    "\n",
    "test_df = pd.concat(arch_dfs, ignore_index=True)\n",
    "ground_truth_all = test_df['ground_truth'].values.copy()\n",
    "archetypes_all   = test_df['ubt_archetype'].values.copy()\n",
    "\n",
    "# Feature frame: drop metadata cols that are NOT feature columns\n",
    "feat_df = test_df.drop(columns=['ubt_archetype', 'ground_truth',\n",
    "                                 'meta_src_ip', 'meta_dst_ip',\n",
    "                                 'meta_timestamp', 'dataset_source',\n",
    "                                 'univ_specific_attack'], errors='ignore')\n",
    "\n",
    "print(f'\\nTotal test slice : {len(test_df):,} rows')\n",
    "print(f'  attack (1)     : {ground_truth_all.sum():,}')\n",
    "print(f'  normal (0)     : {(ground_truth_all==0).sum():,}')\n",
    "print(f'  feature columns: {feat_df.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff25455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage A: vectorising â€¦ 50,497 vectors in 648.3 ms\n",
      "Stage B: querying ChromaDB (51 batches of â‰¤1000) â€¦ done in 5024.2 ms  (50,497 results)\n",
      "Stage C: windowed evidence + quantum fusion â€¦ done in 6181.6 ms\n",
      "\n",
      "Total inference time (B+C): 11205.8 ms for 50,497 packets\n",
      "    verdict  fused_probability  conflict_level  ground_truth ubt_archetype\n",
      "0  CRITICAL           0.950000           0.000             1     BOTNET_C2\n",
      "1  CRITICAL           0.980779           0.038             1     BOTNET_C2\n",
      "2  CRITICAL           0.981995           0.038             1     BOTNET_C2\n",
      "3  CRITICAL           0.981821           0.038             1     BOTNET_C2\n",
      "4  CRITICAL           0.981757           0.038             1     BOTNET_C2\n",
      "5  CRITICAL           0.981744           0.038             1     BOTNET_C2\n",
      "6  CRITICAL           0.981743           0.038             1     BOTNET_C2\n",
      "7  CRITICAL           0.981744           0.038             1     BOTNET_C2\n",
      "8  CRITICAL           0.981745           0.038             1     BOTNET_C2\n",
      "9  CRITICAL           0.981746           0.038             1     BOTNET_C2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€ Cell 6: End-to-End Pipeline Execution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# Stage A â€“ Batch vectorise + L2-normalise   (numpy only)\n",
    "# Stage B â€“ Batched ChromaDB RAG queries     (1 000 rows / call â†’ avoids SQLite limit)\n",
    "# Stage C â€“ Sequential windowed inference    (AdaptiveTimeWindow + Dempster + quantum_fuse)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "RAG_BATCH = 1_000     # chromadb query batch size (SQLite variable limit: ~999)\n",
    "BASE_TS   = 1_700_000_000.0   # synthetic epoch (s); each packet = +0.1 s\n",
    "\n",
    "# â”€â”€ Stage A: vectorise â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('Stage A: vectorising â€¦', end=' ', flush=True)\n",
    "t0_vec = time.perf_counter()\n",
    "X      = vectorize_v51(feat_df)        # (N, 114) float32\n",
    "X_norm = _l2_normalise(X)              # unit-sphere\n",
    "t1_vec = time.perf_counter()\n",
    "N_total = len(X)\n",
    "print(f'{N_total:,} vectors in {(t1_vec-t0_vec)*1e3:.1f} ms')\n",
    "\n",
    "# â”€â”€ Stage B: batch RAG (chunked to stay under SQLite variable limit) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "n_batches = math.ceil(N_total / RAG_BATCH)\n",
    "print(f'Stage B: querying ChromaDB ({n_batches} batches of â‰¤{RAG_BATCH}) â€¦',\n",
    "      end=' ', flush=True)\n",
    "t0_rag = time.perf_counter()\n",
    "rag_results: list = []\n",
    "for b_start in range(0, N_total, RAG_BATCH):\n",
    "    b_end   = min(b_start + RAG_BATCH, N_total)\n",
    "    chunk   = X_norm[b_start:b_end]\n",
    "    rag_results.extend(get_rag_context_batch(chunk, n_results=3))\n",
    "t1_rag = time.perf_counter()\n",
    "rag_time_s = t1_rag - t0_rag\n",
    "print(f'done in {rag_time_s*1e3:.1f} ms  ({len(rag_results):,} results)')\n",
    "\n",
    "# â”€â”€ Stage C: sequential windowed inference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('Stage C: windowed evidence + quantum fusion â€¦', end=' ', flush=True)\n",
    "\n",
    "atw = AdaptiveTimeWindow(min_size_s=5.0, max_size_s=30.0)\n",
    "\n",
    "results  : list = []\n",
    "latencies: list = []\n",
    "\n",
    "t0_seq = time.perf_counter()\n",
    "for i, rag in enumerate(rag_results):\n",
    "    ts   = BASE_TS + i * 0.1\n",
    "    flow = FlowRecord(\n",
    "        timestamp      = ts,\n",
    "        vector         = X[i],\n",
    "        top_similarity = rag['top_similarity'],\n",
    "        top_archetype  = rag['top_archetype'],\n",
    "        top_attack     = rag['top_attack'],\n",
    "        packet_label   = archetypes_all[i],\n",
    "        rag_context    = f\"{rag['top_archetype']}:{rag['top_attack']}\"\n",
    "    )\n",
    "\n",
    "    t_pkt0   = time.perf_counter()\n",
    "    buffer   = atw.ingest(flow)\n",
    "    evidence = accumulate_evidence(buffer)\n",
    "    qf       = quantum_fuse(evidence)\n",
    "    t_pkt1   = time.perf_counter()\n",
    "\n",
    "    atw.adapt(qf['verdict'])\n",
    "    latencies.append((t_pkt1 - t_pkt0) * 1e6)\n",
    "\n",
    "    results.append({\n",
    "        **qf,\n",
    "        'ground_truth'  : int(ground_truth_all[i]),\n",
    "        'ubt_archetype' : archetypes_all[i],\n",
    "        'top_similarity': rag['top_similarity'],\n",
    "        'top_archetype' : rag['top_archetype'],\n",
    "    })\n",
    "\n",
    "t1_seq = time.perf_counter()\n",
    "seq_time_s = t1_seq - t0_seq\n",
    "\n",
    "print(f'done in {seq_time_s*1e3:.1f} ms')\n",
    "print(f'\\nTotal inference time (B+C): {(rag_time_s+seq_time_s)*1e3:.1f} ms for {N_total:,} packets')\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['latency_us'] = latencies\n",
    "print(results_df[['verdict','fused_probability','conflict_level','ground_truth','ubt_archetype']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c1f434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  DIAGNOSTIC: RAG results for NORMAL (ground-truth=0) packets\n",
      "============================================================\n",
      "  Total NORMAL packets: 8,400\n",
      "\n",
      "  RAG top_archetype distribution for NORMAL packets:\n",
      "    NORMAL              :  8,133  (96.8%)\n",
      "    THEFT_EXFIL         :     70  (0.8%)\n",
      "    DOS_DDOS            :     68  (0.8%)\n",
      "    SCAN                :     59  (0.7%)\n",
      "    UNKNOWN             :     41  (0.5%)\n",
      "    BOTNET_C2           :     21  (0.2%)\n",
      "    EXPLOIT             :      5  (0.1%)\n",
      "    BRUTE_FORCE         :      3  (0.0%)\n",
      "\n",
      "  Similarity stats for NORMAL packets:\n",
      "    mean : 98.96%\n",
      "    p25  : 100.00%\n",
      "    p50  : 100.00%\n",
      "    p75  : 100.00%\n",
      "    min  : 0.00%\n",
      "    max  : 100.00%\n",
      "    < 75%: 41  (0.5%)\n",
      "    >= 75%: 8,359  (99.5%)\n",
      "\n",
      "  Verdict distribution for NORMAL packets:\n",
      "    CLEAR       :  8,133  (96.8%)\n",
      "    CRITICAL    :    180  (2.1%)\n",
      "    ELEVATED    :     62  (0.7%)\n",
      "    WARNING     :     25  (0.3%)\n",
      "\n",
      "  NORMAL packets floored to UNKNOWN: 41 (0.5%)\n",
      "  NORMAL packets matched to NORMAL medoids: 8,133 (96.8%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€ Diagnostic: RAG similarity distribution for NORMAL traffic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# Understand why the 75% floor didn't fix NORMAL F1:\n",
    "#   How many NORMAL packets match NORMAL vs. attack medoids?\n",
    "#   What is the similarity distribution for NORMAL packets?\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "normal_mask = results_df['ubt_archetype'] == 'NORMAL'\n",
    "normal_results = results_df[normal_mask]\n",
    "\n",
    "print('=' * 60)\n",
    "print('  DIAGNOSTIC: RAG results for NORMAL (ground-truth=0) packets')\n",
    "print('=' * 60)\n",
    "print(f'  Total NORMAL packets: {normal_mask.sum():,}')\n",
    "print()\n",
    "\n",
    "# Top archetype distribution\n",
    "arch_dist = normal_results['top_archetype'].value_counts()\n",
    "print('  RAG top_archetype distribution for NORMAL packets:')\n",
    "for arch, cnt in arch_dist.items():\n",
    "    print(f'    {arch:20s}: {cnt:6,}  ({100*cnt/normal_mask.sum():.1f}%)')\n",
    "\n",
    "print()\n",
    "\n",
    "# Similarity distribution\n",
    "normal_sims = normal_results['top_similarity']\n",
    "print(f'  Similarity stats for NORMAL packets:')\n",
    "print(f'    mean : {normal_sims.mean():.2f}%')\n",
    "print(f'    p25  : {normal_sims.quantile(0.25):.2f}%')\n",
    "print(f'    p50  : {normal_sims.quantile(0.50):.2f}%')\n",
    "print(f'    p75  : {normal_sims.quantile(0.75):.2f}%')\n",
    "print(f'    min  : {normal_sims.min():.2f}%')\n",
    "print(f'    max  : {normal_sims.max():.2f}%')\n",
    "print(f'    < 75%: {(normal_sims < 75).sum():,}  ({100*(normal_sims < 75).sum()/len(normal_sims):.1f}%)')\n",
    "print(f'    >= 75%: {(normal_sims >= 75).sum():,}  ({100*(normal_sims >= 75).sum()/len(normal_sims):.1f}%)')\n",
    "\n",
    "print()\n",
    "\n",
    "# Verdict distribution for NORMAL packets\n",
    "verd_dist = normal_results['verdict'].value_counts()\n",
    "print('  Verdict distribution for NORMAL packets:')\n",
    "for verd, cnt in verd_dist.items():\n",
    "    print(f'    {verd:12s}: {cnt:6,}  ({100*cnt/normal_mask.sum():.1f}%)')\n",
    "\n",
    "print()\n",
    "\n",
    "# How many NORMAL packets have UNKNOWN archetype after the floor?\n",
    "unk_normal = (normal_results['top_archetype'] == 'UNKNOWN').sum()\n",
    "print(f'  NORMAL packets floored to UNKNOWN: {unk_normal:,} ({100*unk_normal/normal_mask.sum():.1f}%)')\n",
    "\n",
    "# How many NORMAL packets are matched to NORMAL archetype medoids?\n",
    "normal_matched_normal = (normal_results['top_archetype'].isin(NORMAL_ARCHETYPES)).sum()\n",
    "print(f'  NORMAL packets matched to NORMAL medoids: {normal_matched_normal:,} ({100*normal_matched_normal/normal_mask.sum():.1f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c746d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "  PHASE 4.1 â€” BINARY CLASSIFICATION METRICS\n",
      "========================================================\n",
      "  Packets evaluated : 50,497\n",
      "  Accuracy          : 0.9681\n",
      "  Precision         : 0.9935\n",
      "  Recall            : 0.9681\n",
      "  F1-Score          : 0.9806\n",
      "\n",
      "Confusion Matrix  (rows=truth, cols=pred)\n",
      "               Pred-0   Pred-1\n",
      "  Truth-0 (N) :   8,133      267\n",
      "  Truth-1 (A) :   1,343   40,754\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.86      0.97      0.91      8400\n",
      "      attack       0.99      0.97      0.98     42097\n",
      "\n",
      "    accuracy                           0.97     50497\n",
      "   macro avg       0.93      0.97      0.95     50497\n",
      "weighted avg       0.97      0.97      0.97     50497\n",
      "\n",
      "Per-archetype metrics:\n",
      "  Archetype            F1     Acc  Correct  Total\n",
      "  ---------------  ------  ------  -------  -----\n",
      "  BOTNET_C2        0.9642  0.9310    7,820  8,400\n",
      "  BRUTE_FORCE      1.0000  1.0000    8,400  8,400\n",
      "  DOS_DDOS         1.0000  1.0000    8,400  8,400\n",
      "  EXPLOIT          0.9527  0.9096    7,641  8,400\n",
      "  NORMAL           0.9839  0.9682    8,133  8,400\n",
      "  SCAN             0.9998  0.9995    8,396  8,400\n",
      "  THEFT_EXFIL      1.0000  1.0000       97     97\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€ Cell 7: Binary Classification Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# Mapping:\n",
    "#   CRITICAL / WARNING / ELEVATED  â†’  pred = 1  (attack)\n",
    "#   MONITOR  / CLEAR               â†’  pred = 0  (benign)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "ATTACK_VERDICTS = {'CRITICAL', 'WARNING', 'ELEVATED'}\n",
    "\n",
    "y_pred  = results_df['verdict'].apply(lambda v: 1 if v in ATTACK_VERDICTS else 0).values\n",
    "y_truth = results_df['ground_truth'].values\n",
    "\n",
    "acc = accuracy_score(y_truth, y_pred)\n",
    "pre = precision_score(y_truth, y_pred, zero_division=0)\n",
    "rec = recall_score(y_truth, y_pred,    zero_division=0)\n",
    "f1  = f1_score(y_truth, y_pred,        zero_division=0)\n",
    "cm  = confusion_matrix(y_truth, y_pred)\n",
    "\n",
    "print('=' * 56)\n",
    "print('  PHASE 4.1 â€” BINARY CLASSIFICATION METRICS')\n",
    "print('=' * 56)\n",
    "print(f'  Packets evaluated : {N_total:,}')\n",
    "print(f'  Accuracy          : {acc:.4f}')\n",
    "print(f'  Precision         : {pre:.4f}')\n",
    "print(f'  Recall            : {rec:.4f}')\n",
    "print(f'  F1-Score          : {f1:.4f}')\n",
    "print()\n",
    "print('Confusion Matrix  (rows=truth, cols=pred)')\n",
    "print('               Pred-0   Pred-1')\n",
    "print(f'  Truth-0 (N) : {cm[0,0]:7,}  {cm[0,1]:7,}')\n",
    "print(f'  Truth-1 (A) : {cm[1,0]:7,}  {cm[1,1]:7,}')\n",
    "print()\n",
    "print(classification_report(y_truth, y_pred,\n",
    "                             target_names=['benign','attack'], zero_division=0))\n",
    "\n",
    "# â”€â”€ Per-archetype breakdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# For NORMAL (ground truth = 0): use pos_label=0 since all truth values are 0.\n",
    "# For ATTACK archetypes (ground truth = 1): use default pos_label=1.\n",
    "print('Per-archetype metrics:')\n",
    "print(f'  {\"Archetype\":15s}  {\"F1\":>6s}  {\"Acc\":>6s}  {\"Correct\":>7s}  {\"Total\":>5s}')\n",
    "print(f'  {\"-\"*15}  {\"-\"*6}  {\"-\"*6}  {\"-\"*7}  {\"-\"*5}')\n",
    "for arch in ARCHETYPES:\n",
    "    mask  = results_df['ubt_archetype'] == arch\n",
    "    yt, yp = y_truth[mask], y_pred[mask]\n",
    "    gt_label = 0 if arch == 'NORMAL' else 1\n",
    "    f1_a  = f1_score(yt, yp, zero_division=0, pos_label=gt_label)\n",
    "    acc_a = accuracy_score(yt, yp)\n",
    "    correct = (yt == yp).sum()\n",
    "    print(f'  {arch:15s}  {f1_a:.4f}  {acc_a:.4f}  {correct:7,}  {mask.sum():5,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eebab1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "  PHASE 4.1 â€” PERFORMANCE PROFILING\n",
      "========================================================\n",
      "  Total packets           : 50,497\n",
      "  RAG batch time          : 5024.17 ms\n",
      "  Sequential fusion time  : 6181.58 ms\n",
      "  Combined time           : 11205.75 ms\n",
      "\n",
      "  RAG cost / packet       : 99.49 Î¼s\n",
      "\n",
      "  Sequential stage stats  (Î¼s/pkt):\n",
      "    mean  : 119.25\n",
      "    p50   : 139.50\n",
      "    p95   : 219.90\n",
      "    p99   : 416.50\n",
      "    max   : 8951.90\n",
      "\n",
      "  End-to-end mean latency : 221.91 Î¼s/pkt\n",
      "  Throughput              : 4,506 packets/sec\n",
      "\n",
      "  Latency < 1,000 Î¼s   : âœ… PASS  (221.91 Î¼s)\n",
      "  PPS â‰¥ 10,000          : âŒ FAIL  (4,506 pps)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€ Cell 8: Performance Profiling Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_lats = np.array(latencies)           # per-packet sequential stage (Î¼s)\n",
    "rag_per_pkt_us = (rag_time_s / N_total) * 1e6\n",
    "total_per_pkt_us = ((rag_time_s + seq_time_s) / N_total) * 1e6\n",
    "pps = N_total / (rag_time_s + seq_time_s)\n",
    "\n",
    "print('=' * 56)\n",
    "print('  PHASE 4.1 â€” PERFORMANCE PROFILING')\n",
    "print('=' * 56)\n",
    "print(f'  Total packets           : {N_total:,}')\n",
    "print(f'  RAG batch time          : {rag_time_s*1e3:.2f} ms')\n",
    "print(f'  Sequential fusion time  : {seq_time_s*1e3:.2f} ms')\n",
    "print(f'  Combined time           : {(rag_time_s+seq_time_s)*1e3:.2f} ms')\n",
    "print()\n",
    "print(f'  RAG cost / packet       : {rag_per_pkt_us:.2f} Î¼s')\n",
    "print()\n",
    "print(f'  Sequential stage stats  (Î¼s/pkt):')\n",
    "print(f'    mean  : {_lats.mean():.2f}')\n",
    "print(f'    p50   : {np.percentile(_lats, 50):.2f}')\n",
    "print(f'    p95   : {np.percentile(_lats, 95):.2f}')\n",
    "print(f'    p99   : {np.percentile(_lats, 99):.2f}')\n",
    "print(f'    max   : {_lats.max():.2f}')\n",
    "print()\n",
    "print(f'  End-to-end mean latency : {total_per_pkt_us:.2f} Î¼s/pkt')\n",
    "print(f'  Throughput              : {pps:,.0f} packets/sec')\n",
    "\n",
    "# Target checks\n",
    "LATENCY_TARGET_US  = 1_000.0\n",
    "PPS_TARGET         = 10_000\n",
    "\n",
    "lat_ok = total_per_pkt_us < LATENCY_TARGET_US\n",
    "pps_ok = pps >= PPS_TARGET\n",
    "print()\n",
    "print(f'  Latency < {LATENCY_TARGET_US:,.0f} Î¼s   : {\"âœ… PASS\" if lat_ok else \"âŒ FAIL\"}  ({total_per_pkt_us:.2f} Î¼s)')\n",
    "print(f'  PPS â‰¥ {PPS_TARGET:,}          : {\"âœ… PASS\" if pps_ok else \"âŒ FAIL\"}  ({pps:,.0f} pps)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a56b6a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-conflict packets (K > 0.5): 14,497 / 50,497  (28.7%)\n",
      "\n",
      "========================================================\n",
      "  QUANTUM ADVANTAGE (K > 0.50 subset)\n",
      "========================================================\n",
      "  Subset size         : 14,497\n",
      "  Quantum-fused  F1   : 0.9598\n",
      "  RAG-only       F1   : 0.9821   (threshold: sim > 70.0%)\n",
      "  Î” F1 (quantumâˆ’RAG)  : -0.0223\n",
      "  Quantum advantage   : âŒ NO\n",
      "\n",
      "  Interference types in high-conflict subset:\n",
      "    DESTRUCTIVE    : 9,819  (67.7%)\n",
      "    MIXED          : 3,089  (21.3%)\n",
      "    CONSTRUCTIVE   : 1,589  (11.0%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€ Cell 9: Quantum Advantage Validation (K > 0.50) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# High-conflict cases (Dempster K > 0.50) are where quantum interference\n",
    "# should help most.  Compare quantum-fused F1 vs a naÃ¯ve RAG-similarity\n",
    "# baseline that thresholds on top_similarity alone.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "K_THRESHOLD      = 0.50\n",
    "RAG_SIM_THRESHOLD = 70.0   # similarity % above which we say \"attack\"\n",
    "\n",
    "conflict_arr = results_df['conflict_level'].values\n",
    "sim_arr      = results_df['top_similarity'].values\n",
    "hc_mask      = conflict_arr > K_THRESHOLD\n",
    "\n",
    "n_hc = hc_mask.sum()\n",
    "print(f'High-conflict packets (K > {K_THRESHOLD}): {n_hc:,} / {N_total:,}  '\n",
    "      f'({100*n_hc/N_total:.1f}%)')\n",
    "\n",
    "if n_hc == 0:\n",
    "    print('No high-conflict packets found â€” quantum advantage test skipped.')\n",
    "else:\n",
    "    yt_hc = y_truth[hc_mask]\n",
    "    yp_hc = y_pred[hc_mask]\n",
    "\n",
    "    # RAG-only baseline: attack if similarity > threshold, else benign\n",
    "    rag_pred_hc = (sim_arr[hc_mask] > RAG_SIM_THRESHOLD).astype(int)\n",
    "\n",
    "    f1_quantum = f1_score(yt_hc, yp_hc,      zero_division=0)\n",
    "    f1_rag     = f1_score(yt_hc, rag_pred_hc, zero_division=0)\n",
    "    delta_f1   = f1_quantum - f1_rag\n",
    "\n",
    "    print()\n",
    "    print('=' * 56)\n",
    "    print('  QUANTUM ADVANTAGE (K > 0.50 subset)')\n",
    "    print('=' * 56)\n",
    "    print(f'  Subset size         : {n_hc:,}')\n",
    "    print(f'  Quantum-fused  F1   : {f1_quantum:.4f}')\n",
    "    print(f'  RAG-only       F1   : {f1_rag:.4f}   '\n",
    "          f'(threshold: sim > {RAG_SIM_THRESHOLD}%)')\n",
    "    print(f'  Î” F1 (quantumâˆ’RAG)  : {delta_f1:+.4f}')\n",
    "    advantage = delta_f1 > 0\n",
    "    print(f'  Quantum advantage   : {\"âœ… YES\" if advantage else \"âŒ NO\"}')\n",
    "\n",
    "    # Interference type breakdown in high-conflict subset\n",
    "    itype_counts = results_df[hc_mask]['interference_type'].value_counts()\n",
    "    print()\n",
    "    print('  Interference types in high-conflict subset:')\n",
    "    for itype, cnt in itype_counts.items():\n",
    "        print(f'    {itype:15s}: {cnt:,}  ({100*cnt/n_hc:.1f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d0f407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘     PHASE 4.1 â€” SYSTEM EVALUATION COMPLETE          â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘  Test slice          : 50,497 packets (7 archetypes) â•‘\n",
      "â•‘  Accuracy            : 0.9681                        â•‘\n",
      "â•‘  Precision           : 0.9935                        â•‘\n",
      "â•‘  Recall              : 0.9681                        â•‘\n",
      "â•‘  F1-Score            : 0.9806                        â•‘\n",
      "â•‘  Mean latency        : 221.91 Î¼s/pkt                 â•‘\n",
      "â•‘  Throughput          : 4,506 pps                     â•‘\n",
      "â•‘  Quantum Î”F1 (K>0.5) : -0.0223                       â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘  Pipeline stages proven end-to-end:                 â•‘\n",
      "â•‘    âœ… vectorize_v51  (114-dim float32)               â•‘\n",
      "â•‘    âœ… get_rag_context_batch  (ChromaDB cosine)        â•‘\n",
      "â•‘    âœ… AdaptiveTimeWindow  (Phase 3.1)                â•‘\n",
      "â•‘    âœ… accumulate_evidence (Dempster-Shafer, 3.2)     â•‘\n",
      "â•‘    âœ… quantum_fuse  (H-gate interference, 3.3)       â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€ Cell 10: Phase 4.1 Completion Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')\n",
    "print('â•‘     PHASE 4.1 â€” SYSTEM EVALUATION COMPLETE          â•‘')\n",
    "print('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£')\n",
    "print(f'â•‘  Test slice          : {N_total:,} packets ({len(ARCHETYPES)} archetypes)'.ljust(55) + 'â•‘')\n",
    "print(f'â•‘  Accuracy            : {acc:.4f}'.ljust(55) + 'â•‘')\n",
    "print(f'â•‘  Precision           : {pre:.4f}'.ljust(55) + 'â•‘')\n",
    "print(f'â•‘  Recall              : {rec:.4f}'.ljust(55) + 'â•‘')\n",
    "print(f'â•‘  F1-Score            : {f1:.4f}'.ljust(55) + 'â•‘')\n",
    "print(f'â•‘  Mean latency        : {total_per_pkt_us:.2f} Î¼s/pkt'.ljust(55) + 'â•‘')\n",
    "print(f'â•‘  Throughput          : {pps:,.0f} pps'.ljust(55) + 'â•‘')\n",
    "if n_hc > 0:\n",
    "    print(f'â•‘  Quantum Î”F1 (K>0.5) : {delta_f1:+.4f}'.ljust(55) + 'â•‘')\n",
    "print('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£')\n",
    "print('â•‘  Pipeline stages proven end-to-end:                 â•‘')\n",
    "print('â•‘    âœ… vectorize_v51  (114-dim float32)               â•‘')\n",
    "print('â•‘    âœ… get_rag_context_batch  (ChromaDB cosine)        â•‘')\n",
    "print('â•‘    âœ… AdaptiveTimeWindow  (Phase 3.1)                â•‘')\n",
    "print('â•‘    âœ… accumulate_evidence (Dempster-Shafer, 3.2)     â•‘')\n",
    "print('â•‘    âœ… quantum_fuse  (H-gate interference, 3.3)       â•‘')\n",
    "print('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
