{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08e4ca1",
   "metadata": {},
   "source": [
    "# üîß Phase-2.2: Encoding & Normalization\n",
    "## Quantum-RAG Knowledge Fusion for Adaptive IoT Intrusion Detection\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Phase-2.2 Objective\n",
    "\n",
    "**This notebook applies frozen schema transformations exactly as specified in Phase-1.**\n",
    "\n",
    "Phase-2.2 Goals:\n",
    "1. ‚úÖ Load cleaned features from Phase-2.1\n",
    "2. ‚úÖ Apply placeholder handling (`\"-\"` ‚Üí protocol-aware semantics)\n",
    "3. ‚úÖ Encode categorical features (one-hot, ordinal, binary)\n",
    "4. ‚úÖ Transform numerical features (log, robust/standard scaling)\n",
    "5. ‚úÖ Validate encoded data integrity\n",
    "6. ‚úÖ Save encoded dataset for vector generation\n",
    "\n",
    "### üîí Phase-2.2 Rules\n",
    "\n",
    "| Rule | Status |\n",
    "|------|--------|\n",
    "| ‚ùå No new encoding decisions | Strict |\n",
    "| ‚ùå No schema modifications | Strict |\n",
    "| ‚úî Apply frozen schema exactly | Required |\n",
    "| ‚úî Preserve placeholder semantics | Required |\n",
    "| ‚úî Deterministic transformations | Required |\n",
    "\n",
    "### üìä Key Principles\n",
    "\n",
    "- **Protocol-Aware Placeholders**: `\"-\"` ‚â† missing, but \"not applicable\"\n",
    "- **Semantic Preservation**: Maintain feature interpretability\n",
    "- **Deterministic Pipeline**: Same input ‚Üí same output (reproducible)\n",
    "- **Type Safety**: Ensure numeric columns after transformation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7182b",
   "metadata": {},
   "source": [
    "## üì¶ Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2692d960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "üìÅ Phase-1 artifacts: ../artifacts/phase_1\n",
      "üìÅ Phase-2 artifacts: ../artifacts/phase_2\n"
     ]
    }
   ],
   "source": [
    "# Core data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Encoding & Scaling\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    LabelEncoder\n",
    ")\n",
    "\n",
    "# File handling\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Display utilities\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Define paths\n",
    "PHASE_1_DIR = \"../artifacts/phase_1\"\n",
    "PHASE_2_DIR = \"../artifacts/phase_2\"\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÅ Phase-1 artifacts: {PHASE_1_DIR}\")\n",
    "print(f\"üìÅ Phase-2 artifacts: {PHASE_2_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b97522",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîí SECTION 1 ‚Äî Load Frozen Schema & Phase-2.1 Data\n",
    "\n",
    "### Objectives:\n",
    "1. Load frozen schema (immutable ground truth)\n",
    "2. Load Phase-1 encoding/normalization strategies\n",
    "3. Load cleaned features from Phase-2.1\n",
    "4. Validate data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902b26dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîí Frozen Schema Loaded:\n",
      "  ‚Ä¢ Schema Version: 1.0\n",
      "  ‚Ä¢ Total Features: 33\n",
      "\n",
      "‚úÖ Phase-1 Strategies Loaded:\n",
      "  ‚Ä¢ Placeholder Strategies: 33 features\n",
      "  ‚Ä¢ Encoding Strategies: 21 features\n",
      "  ‚Ä¢ Numerical Treatment: 12 features\n"
     ]
    }
   ],
   "source": [
    "# Load frozen schema\n",
    "with open(f\"{PHASE_1_DIR}/frozen_schema.json\", 'r', encoding='utf-8') as f:\n",
    "    frozen_schema = json.load(f)\n",
    "\n",
    "# Load Phase-1 strategy CSVs\n",
    "placeholder_strategies = pd.read_csv(f\"{PHASE_1_DIR}/phase1_placeholder_strategies.csv\")\n",
    "encoding_strategies = pd.read_csv(f\"{PHASE_1_DIR}/phase1_encoding_strategies.csv\")\n",
    "numerical_treatment = pd.read_csv(f\"{PHASE_1_DIR}/phase1_numerical_treatment.csv\")\n",
    "\n",
    "print(\"üîí Frozen Schema Loaded:\")\n",
    "print(f\"  ‚Ä¢ Schema Version: {frozen_schema['schema_version']}\")\n",
    "print(f\"  ‚Ä¢ Total Features: {frozen_schema['total_features']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Phase-1 Strategies Loaded:\")\n",
    "print(f\"  ‚Ä¢ Placeholder Strategies: {len(placeholder_strategies)} features\")\n",
    "print(f\"  ‚Ä¢ Encoding Strategies: {len(encoding_strategies)} features\")\n",
    "print(f\"  ‚Ä¢ Numerical Treatment: {len(numerical_treatment)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47719348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading and processing cleaned features from Phase-2.1...\n",
      "\n",
      "‚ö†Ô∏è  Using chunked processing to handle large dataset efficiently\n",
      "\n",
      "üìä Analyzing dataset structure...\n",
      "‚úÖ Dataset structure:\n",
      "  ‚Ä¢ Total rows: 22,339,021\n",
      "  ‚Ä¢ Columns: 33\n",
      "  ‚Ä¢ Chunk size: 500,000 rows\n",
      "  ‚Ä¢ Estimated chunks: 45\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned features from Phase-2.1 using CHUNKED processing\n",
    "print(\"üì• Loading and processing cleaned features from Phase-2.1...\\n\")\n",
    "print(\"‚ö†Ô∏è  Using chunked processing to handle large dataset efficiently\\n\")\n",
    "\n",
    "# Define chunk size (adjust based on available memory)\n",
    "CHUNK_SIZE = 500000  # Process 500k rows at a time\n",
    "\n",
    "# First pass: get total row count and column names\n",
    "print(\"üìä Analyzing dataset structure...\")\n",
    "chunk_iter = pd.read_csv(\n",
    "    f\"{PHASE_2_DIR}/cleaned_features.csv.gz\",\n",
    "    compression='gzip',\n",
    "    chunksize=CHUNK_SIZE,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "first_chunk = next(chunk_iter)\n",
    "total_cols = len(first_chunk.columns)\n",
    "col_names = first_chunk.columns.tolist()\n",
    "\n",
    "# Count total rows\n",
    "total_rows = len(first_chunk)\n",
    "for chunk in chunk_iter:\n",
    "    total_rows += len(chunk)\n",
    "\n",
    "print(f\"‚úÖ Dataset structure:\")\n",
    "print(f\"  ‚Ä¢ Total rows: {total_rows:,}\")\n",
    "print(f\"  ‚Ä¢ Columns: {total_cols}\")\n",
    "print(f\"  ‚Ä¢ Chunk size: {CHUNK_SIZE:,} rows\")\n",
    "print(f\"  ‚Ä¢ Estimated chunks: {(total_rows // CHUNK_SIZE) + 1}\")\n",
    "\n",
    "# Store metadata for later\n",
    "dataset_info = {\n",
    "    'total_rows': total_rows,\n",
    "    'total_cols': total_cols,\n",
    "    'col_names': col_names,\n",
    "    'chunk_size': CHUNK_SIZE\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe407c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß SECTION 2 ‚Äî Define Transformation Functions\n",
    "\n",
    "### Objectives:\n",
    "1. Define reusable transformation functions for chunked processing\n",
    "2. Create placeholder handling function\n",
    "3. Create encoding functions\n",
    "4. Create normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc754050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Defining transformation functions...\n",
      "\n",
      "‚úÖ Placeholder handling function defined\n",
      "‚úÖ Encoding mappings prepared (16 one-hot features)\n",
      "‚úÖ Treatment mappings prepared (12 numerical features)\n"
     ]
    }
   ],
   "source": [
    "# Define transformation functions for chunked processing\n",
    "print(\"üîß Defining transformation functions...\\n\")\n",
    "\n",
    "# Create strategy mappings\n",
    "strategy_map = dict(zip(placeholder_strategies['column'], placeholder_strategies['strategy']))\n",
    "encoding_map = dict(zip(encoding_strategies['column'], encoding_strategies['encoding_method']))\n",
    "treatment_map = dict(zip(numerical_treatment['column'], numerical_treatment['treatment']))\n",
    "\n",
    "def apply_placeholder_handling(chunk_df):\n",
    "    \"\"\"Apply placeholder transformations to a chunk\"\"\"\n",
    "    for col in chunk_df.columns:\n",
    "        if chunk_df[col].dtype == 'object' and col in strategy_map:\n",
    "            strategy = strategy_map[col]\n",
    "            if strategy == 'protocol_na':\n",
    "                chunk_df[col] = chunk_df[col].replace('-', 'NOT_APPLICABLE')\n",
    "            elif strategy == 'unknown_service':\n",
    "                chunk_df[col] = chunk_df[col].replace('-', 'UNKNOWN')\n",
    "            elif strategy == 'boolean_false':\n",
    "                chunk_df[col] = chunk_df[col].replace('-', 'False')\n",
    "    return chunk_df\n",
    "\n",
    "print(\"‚úÖ Placeholder handling function defined\")\n",
    "\n",
    "# Store one-hot encoding mappings (will be built from first chunk)\n",
    "onehot_categories = {}\n",
    "onehot_cols = [col for col, method in encoding_map.items() if method == 'one_hot']\n",
    "\n",
    "print(f\"‚úÖ Encoding mappings prepared ({len(onehot_cols)} one-hot features)\")\n",
    "print(f\"‚úÖ Treatment mappings prepared ({len(treatment_map)} numerical features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa76430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Building one-hot encoding categories...\n",
      "\n",
      "  proto                         : 3 categories\n",
      "  conn_state                    : 13 categories\n",
      "  service                       : 10 categories\n",
      "  dns_qclass                    : 3 categories\n",
      "  dns_qtype                     : 11 categories\n",
      "  dns_rcode                     : 3 categories\n",
      "  dns_AA                        : 3 categories\n",
      "  dns_RD                        : 3 categories\n",
      "  dns_RA                        : 3 categories\n",
      "  dns_rejected                  : 3 categories\n",
      "  http_method                   : 4 categories\n",
      "  http_orig_mime_types          : 3 categories\n",
      "  http_resp_mime_types          : 9 categories\n",
      "  ssl_cipher                    : 5 categories\n",
      "  ssl_resumed                   : 3 categories\n",
      "  ssl_established               : 3 categories\n",
      "\n",
      "‚úÖ One-hot categories established for 16 features\n"
     ]
    }
   ],
   "source": [
    "# Build one-hot encoding categories from first chunk\n",
    "print(\"üî§ Building one-hot encoding categories...\\n\")\n",
    "\n",
    "# Load first chunk to get category values\n",
    "first_chunk = pd.read_csv(\n",
    "    f\"{PHASE_2_DIR}/cleaned_features.csv.gz\",\n",
    "    compression='gzip',\n",
    "    nrows=CHUNK_SIZE,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Apply placeholder handling to first chunk\n",
    "first_chunk = apply_placeholder_handling(first_chunk)\n",
    "\n",
    "# Get unique categories for one-hot features\n",
    "for col in onehot_cols:\n",
    "    if col in first_chunk.columns:\n",
    "        unique_vals = first_chunk[col].unique()\n",
    "        onehot_categories[col] = sorted([str(v) for v in unique_vals if pd.notna(v)])\n",
    "        print(f\"  {col:30s}: {len(onehot_categories[col])} categories\")\n",
    "\n",
    "del first_chunk\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ One-hot categories established for {len(onehot_categories)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ae42eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete encoding function defined\n"
     ]
    }
   ],
   "source": [
    "# Define complete encoding function\n",
    "def apply_encoding(chunk_df):\n",
    "    \"\"\"Apply all encoding transformations to a chunk\"\"\"\n",
    "    \n",
    "    # ONE-HOT ENCODING\n",
    "    encoded_dfs = []\n",
    "    non_onehot_cols = [c for c in chunk_df.columns if c not in onehot_cols]\n",
    "    \n",
    "    # Keep non-onehot columns\n",
    "    base_df = chunk_df[non_onehot_cols].copy()\n",
    "    for col in base_df.select_dtypes(include=['int64']).columns:\n",
    "        base_df[col] = base_df[col].astype('int32')\n",
    "    encoded_dfs.append(base_df)\n",
    "    \n",
    "    # One-hot encode categorical columns\n",
    "    for col in onehot_cols:\n",
    "        if col in chunk_df.columns:\n",
    "            # Create dummy columns matching established categories\n",
    "            for cat in onehot_categories.get(col, []):\n",
    "                encoded_dfs.append(\n",
    "                    pd.DataFrame({f\"{col}_{cat}\": (chunk_df[col].astype(str) == cat).astype('uint8')})\n",
    "                )\n",
    "    \n",
    "    result_df = pd.concat(encoded_dfs, axis=1)\n",
    "    \n",
    "    # ORDINAL ENCODING\n",
    "    ordinal_mappings = {\n",
    "        'http_version': {'NOT_APPLICABLE': -1, 'HTTP/0.9': 0, 'HTTP/1.0': 1, 'HTTP/1.1': 2, 'HTTP/2.0': 3, 'HTTP/3.0': 4},\n",
    "        'http_status_code': {'NOT_APPLICABLE': 0},\n",
    "        'ssl_version': {'NOT_APPLICABLE': -1, 'SSLv2': 0, 'SSLv3': 1, 'TLSv1.0': 2, 'TLSv1.1': 3, 'TLSv1.2': 4, 'TLSv1.3': 5},\n",
    "        'missed_bytes': {'NOT_APPLICABLE': -1}\n",
    "    }\n",
    "    \n",
    "    for col, mapping in ordinal_mappings.items():\n",
    "        if col in result_df.columns:\n",
    "            if col in ['http_status_code', 'missed_bytes']:\n",
    "                result_df[col] = result_df[col].replace(mapping)\n",
    "                result_df[col] = pd.to_numeric(result_df[col], errors='coerce').fillna(-1).astype('int16')\n",
    "            else:\n",
    "                result_df[col] = result_df[col].map(mapping).fillna(-1).astype('int8')\n",
    "    \n",
    "    # BINARY ENCODING\n",
    "    binary_cols = [col for col, method in encoding_map.items() if method == 'binary' and col in result_df.columns]\n",
    "    for col in binary_cols:\n",
    "        mapping = {'True': 1, 'False': 0, 'T': 1, 'F': 0}\n",
    "        result_df[col] = result_df[col].map(mapping).fillna(0).astype('uint8')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "print(\"‚úÖ Complete encoding function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8532d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ SECTION 3 ‚Äî Process All Chunks\n",
    "\n",
    "### Objectives:\n",
    "1. Process data in memory-efficient chunks\n",
    "2. Apply all transformations per chunk\n",
    "3. Save encoded chunks progressively\n",
    "4. Track progress and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f59fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing all chunks...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  22%|‚ñà‚ñà‚ñè       | 10/45 [01:38<05:52, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Processed 10/45 chunks, memory: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [03:10<03:50,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Processed 20/45 chunks, memory: 1.86 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [04:39<02:23,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Processed 30/45 chunks, memory: 2.79 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [06:21<00:52, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üíæ Processed 40/45 chunks, memory: 3.73 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [06:45<00:00,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All 45 chunks processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all chunks and save progressively\n",
    "print(\"üîÑ Processing all chunks...\\n\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "chunk_iter = pd.read_csv(\n",
    "    f\"{PHASE_2_DIR}/cleaned_features.csv.gz\",\n",
    "    compression='gzip',\n",
    "    chunksize=CHUNK_SIZE,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Process each chunk\n",
    "encoded_chunks = []\n",
    "chunk_num = 0\n",
    "total_chunks = (dataset_info['total_rows'] // CHUNK_SIZE) + 1\n",
    "\n",
    "for chunk in tqdm(chunk_iter, total=total_chunks, desc=\"Processing chunks\"):\n",
    "    chunk_num += 1\n",
    "    \n",
    "    # Apply placeholder handling\n",
    "    chunk = apply_placeholder_handling(chunk)\n",
    "    \n",
    "    # Apply encoding\n",
    "    chunk_encoded = apply_encoding(chunk)\n",
    "    \n",
    "    # Apply log transforms\n",
    "    log_cols = [col for col, treatment in treatment_map.items() if 'log' in treatment and col in chunk_encoded.columns]\n",
    "    for col in log_cols:\n",
    "        treatment = treatment_map[col]\n",
    "        if chunk_encoded[col].dtype == 'object':\n",
    "            chunk_encoded[col] = chunk_encoded[col].replace(['NOT_APPLICABLE', 'UNKNOWN', '-'], '-1')\n",
    "            chunk_encoded[col] = pd.to_numeric(chunk_encoded[col], errors='coerce').fillna(-1)\n",
    "        \n",
    "        chunk_encoded[col] = chunk_encoded[col].astype(np.float32)\n",
    "        \n",
    "        offset = 2 if 'log_scale_with_na' in treatment else 1\n",
    "        chunk_encoded[f\"{col}_log\"] = np.log1p(chunk_encoded[col] + offset - 1).astype(np.float32)\n",
    "        chunk_encoded.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # Store encoded chunk\n",
    "    encoded_chunks.append(chunk_encoded)\n",
    "    \n",
    "    # Periodic save to avoid memory overflow\n",
    "    if chunk_num % 10 == 0:\n",
    "        print(f\"  üíæ Processed {chunk_num}/{total_chunks} chunks, memory: {sum(c.memory_usage(deep=True).sum() for c in encoded_chunks) / (1024**3):.2f} GB\")\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ All {chunk_num} chunks processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d41da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Concatenating all encoded chunks...\n",
      "\n",
      "‚úÖ Concatenation complete!\n",
      "  ‚Ä¢ Rows: 22,339,021\n",
      "  ‚Ä¢ Columns: 99\n",
      "  ‚Ä¢ Memory: 4.16 GB\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all encoded chunks\n",
    "print(\"üîó Concatenating all encoded chunks...\\n\")\n",
    "\n",
    "feature_df_encoded = pd.concat(encoded_chunks, axis=0, ignore_index=True)\n",
    "\n",
    "# Free memory\n",
    "del encoded_chunks\n",
    "gc.collect()\n",
    "\n",
    "print(f\"‚úÖ Concatenation complete!\")\n",
    "print(f\"  ‚Ä¢ Rows: {len(feature_df_encoded):,}\")\n",
    "print(f\"  ‚Ä¢ Columns: {len(feature_df_encoded.columns)}\")\n",
    "print(f\"  ‚Ä¢ Memory: {feature_df_encoded.memory_usage(deep=True).sum() / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7a11a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying ROBUST scaling...\n",
      "\n",
      "  ‚úÖ duration                      : RobustScaler applied\n",
      "\n",
      "‚úÖ Robust scaling complete! (1 features)\n"
     ]
    }
   ],
   "source": [
    "# Apply ROBUST SCALER (outlier-resistant)\n",
    "print(\"üîß Applying ROBUST scaling...\\n\")\n",
    "\n",
    "robust_cols = [col for col, treatment in treatment_map.items() \n",
    "               if treatment == 'robust_scale' and col in feature_df_encoded.columns]\n",
    "\n",
    "robust_scalers = {}\n",
    "\n",
    "for col in robust_cols:\n",
    "    scaler = RobustScaler()\n",
    "    if feature_df_encoded[col].dtype != 'float32':\n",
    "        feature_df_encoded[col] = feature_df_encoded[col].astype(np.float32)\n",
    "    feature_df_encoded[col] = scaler.fit_transform(feature_df_encoded[[col]]).astype(np.float32)\n",
    "    robust_scalers[col] = scaler\n",
    "    print(f\"  ‚úÖ {col:30s}: RobustScaler applied\")\n",
    "\n",
    "print(f\"\\n‚úÖ Robust scaling complete! ({len(robust_scalers)} features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c387e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Applying STANDARD scaling...\n",
      "\n",
      "  ‚úÖ src_port                      : StandardScaler applied\n",
      "  ‚úÖ dst_port                      : StandardScaler applied\n",
      "  ‚úÖ http_trans_depth              : StandardScaler applied\n",
      "  ‚úÖ src_bytes_log                 : StandardScaler applied\n",
      "  ‚úÖ dst_bytes_log                 : StandardScaler applied\n",
      "  ‚úÖ src_pkts_log                  : StandardScaler applied\n",
      "  ‚úÖ dst_pkts_log                  : StandardScaler applied\n",
      "  ‚úÖ src_ip_bytes_log              : StandardScaler applied\n",
      "  ‚úÖ dst_ip_bytes_log              : StandardScaler applied\n",
      "  ‚úÖ http_request_body_len_log     : StandardScaler applied\n",
      "  ‚úÖ http_response_body_len_log    : StandardScaler applied\n",
      "\n",
      "‚úÖ Standard scaling complete! (11 features)\n"
     ]
    }
   ],
   "source": [
    "# Apply STANDARD SCALER\n",
    "print(\"üìè Applying STANDARD scaling...\\n\")\n",
    "\n",
    "# Identify columns needing standard scaling\n",
    "standard_cols = [col for col, treatment in treatment_map.items() \n",
    "                 if 'standard' in treatment.lower() and col in feature_df_encoded.columns]\n",
    "\n",
    "# Also scale log-transformed columns\n",
    "log_transformed_cols = [col for col in feature_df_encoded.columns if col.endswith('_log')]\n",
    "standard_cols.extend(log_transformed_cols)\n",
    "\n",
    "standard_scalers = {}\n",
    "\n",
    "for col in standard_cols:\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Handle potential string values in columns\n",
    "    if feature_df_encoded[col].dtype == 'object':\n",
    "        feature_df_encoded[col] = feature_df_encoded[col].replace(['NOT_APPLICABLE', 'UNKNOWN', '-'], '-1')\n",
    "        feature_df_encoded[col] = pd.to_numeric(feature_df_encoded[col], errors='coerce').fillna(-1)\n",
    "    \n",
    "    # Convert to float32\n",
    "    if feature_df_encoded[col].dtype != 'float32':\n",
    "        feature_df_encoded[col] = feature_df_encoded[col].astype(np.float32)\n",
    "    \n",
    "    # Replace Inf values with large finite number\n",
    "    feature_df_encoded[col] = feature_df_encoded[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "    feature_df_encoded[col] = scaler.fit_transform(feature_df_encoded[[col]]).astype(np.float32)\n",
    "    standard_scalers[col] = scaler\n",
    "    print(f\"  ‚úÖ {col:30s}: StandardScaler applied\")\n",
    "\n",
    "print(f\"\\n‚úÖ Standard scaling complete! ({len(standard_scalers)} features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98376a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving fitted scalers...\n",
      "\n",
      "‚úÖ Saved: ../artifacts/phase_2/fitted_scalers.pkl\n",
      "  ‚Ä¢ RobustScalers: 1\n",
      "  ‚Ä¢ StandardScalers: 11\n"
     ]
    }
   ],
   "source": [
    "# Save fitted scalers for inference\n",
    "print(\"üíæ Saving fitted scalers...\\n\")\n",
    "\n",
    "scalers = {\n",
    "    'robust': robust_scalers,\n",
    "    'standard': standard_scalers\n",
    "}\n",
    "\n",
    "scaler_path = f\"{PHASE_2_DIR}/fitted_scalers.pkl\"\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scalers, f)\n",
    "\n",
    "print(f\"‚úÖ Saved: {scaler_path}\")\n",
    "print(f\"  ‚Ä¢ RobustScalers: {len(robust_scalers)}\")\n",
    "print(f\"  ‚Ä¢ StandardScalers: {len(standard_scalers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32707d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ SECTION 5 ‚Äî Validation & Export\n",
    "\n",
    "### Objectives:\n",
    "1. Validate final encoded dataset\n",
    "2. Check for NaN/Inf values\n",
    "3. Verify all columns are numeric\n",
    "4. Save encoded dataset\n",
    "5. Generate encoding summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d039eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving encoded dataset...\n",
      "\n",
      "‚úÖ Saved: ../artifacts/phase_2/encoded_features.parquet\n",
      "  ‚Ä¢ Rows: 22,339,021\n",
      "  ‚Ä¢ Features: 99\n",
      "  ‚Ä¢ Size: 208.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Save encoded dataset\n",
    "print(\"üíæ Saving encoded dataset...\\n\")\n",
    "\n",
    "# Save as Parquet with compression (now all numeric, Parquet-safe)\n",
    "encoded_output_path = f\"{PHASE_2_DIR}/encoded_features.parquet\"\n",
    "feature_df_encoded.to_parquet(\n",
    "    encoded_output_path, \n",
    "    index=False,\n",
    "    compression='snappy',  # Fast compression\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Saved: {encoded_output_path}\")\n",
    "print(f\"  ‚Ä¢ Rows: {len(feature_df_encoded):,}\")\n",
    "print(f\"  ‚Ä¢ Features: {len(feature_df_encoded.columns)}\")\n",
    "print(f\"  ‚Ä¢ Size: {os.path.getsize(encoded_output_path) / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3602008",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Phase-2.2 Complete!\n",
    "\n",
    "### ‚úÖ Deliverables\n",
    "1. ‚úÖ Applied placeholder handling (protocol-aware)\n",
    "2. ‚úÖ Applied categorical encoding (one-hot, ordinal, binary)\n",
    "3. ‚úÖ Applied numerical transformations (log, robust, standard)\n",
    "4. ‚úÖ Validated all columns numeric, no NaN/Inf\n",
    "5. ‚úÖ Saved encoded dataset (Parquet format)\n",
    "6. ‚úÖ Saved fitted scalers for inference\n",
    "\n",
    "### üìÇ Output Files\n",
    "- `artifacts/phase_2/encoded_features.parquet` (all numeric, ready for vectors)\n",
    "- `artifacts/phase_2/fitted_scalers.pkl` (for inference pipeline)\n",
    "- `artifacts/phase_2/phase2_2_summary.json`\n",
    "\n",
    "### üöÄ Next: Phase-2.3 (Vector Generation)\n",
    "Ready to:\n",
    "- Construct fixed-length feature vectors\n",
    "- Validate vector dimensionality\n",
    "- Prepare for ChromaDB ingestion\n",
    "\n",
    "---\n",
    "\n",
    "**Status**: ‚úÖ **READY FOR PHASE-2.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59382dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Final Validation...\n",
      "\n",
      "‚úÖ No NaN values detected\n",
      "‚úÖ No Inf values detected\n",
      "‚úÖ All columns are numeric\n",
      "\n",
      "üìä Final Dataset Shape: (22339021, 99)\n",
      "  ‚Ä¢ Rows: 22,339,021\n",
      "  ‚Ä¢ Features: 99\n",
      "  ‚Ä¢ Memory: 2.85 GB\n"
     ]
    }
   ],
   "source": [
    "# Final validation\n",
    "print(\"üîç Final Validation...\\n\")\n",
    "\n",
    "# Check for NaN values\n",
    "nan_counts = feature_df_encoded.isna().sum()\n",
    "nan_cols = nan_counts[nan_counts > 0]\n",
    "\n",
    "if len(nan_cols) == 0:\n",
    "    print(\"‚úÖ No NaN values detected\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  NaN values detected in {len(nan_cols)} columns:\")\n",
    "    for col, count in nan_cols.items():\n",
    "        print(f\"  ‚Ä¢ {col}: {count:,} NaNs\")\n",
    "\n",
    "# Check for Inf values\n",
    "inf_counts = np.isinf(feature_df_encoded.select_dtypes(include=['number'])).sum()\n",
    "inf_cols = inf_counts[inf_counts > 0]\n",
    "\n",
    "if len(inf_cols) == 0:\n",
    "    print(\"‚úÖ No Inf values detected\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Inf values detected in {len(inf_cols)} columns:\")\n",
    "    for col, count in inf_cols.items():\n",
    "        print(f\"  ‚Ä¢ {col}: {count:,} Infs\")\n",
    "\n",
    "# Check all numeric\n",
    "non_numeric = feature_df_encoded.select_dtypes(exclude=['number']).columns\n",
    "if len(non_numeric) == 0:\n",
    "    print(\"‚úÖ All columns are numeric\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  {len(non_numeric)} non-numeric columns:\")\n",
    "    print(f\"  {list(non_numeric)}\")\n",
    "\n",
    "print(f\"\\nüìä Final Dataset Shape: {feature_df_encoded.shape}\")\n",
    "print(f\"  ‚Ä¢ Rows: {feature_df_encoded.shape[0]:,}\")\n",
    "print(f\"  ‚Ä¢ Features: {feature_df_encoded.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Memory: {feature_df_encoded.memory_usage(deep=True).sum() / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b50e6d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Phase-2.2 Summary:\n",
      "{\n",
      "  \"phase\": \"Phase-2.2 (Encoding & Normalization)\",\n",
      "  \"timestamp\": \"2026-02-04 22:02:28\",\n",
      "  \"frozen_schema_version\": \"1.0\",\n",
      "  \"input_shape\": [\n",
      "    22339021,\n",
      "    33\n",
      "  ],\n",
      "  \"output_shape\": [\n",
      "    22339021,\n",
      "    99\n",
      "  ],\n",
      "  \"transformations\": {\n",
      "    \"placeholder_handling\": 33,\n",
      "    \"onehot_encoded\": 16,\n",
      "    \"ordinal_encoded\": 4,\n",
      "    \"binary_encoded\": 1,\n",
      "    \"log_transformed\": 8,\n",
      "    \"robust_scaled\": 1,\n",
      "    \"standard_scaled\": 11\n",
      "  },\n",
      "  \"feature_expansion\": {\n",
      "    \"original_features\": 33,\n",
      "    \"encoded_features\": 99,\n",
      "    \"expansion_factor\": 3.0\n",
      "  },\n",
      "  \"validation\": {\n",
      "    \"nan_columns\": 0,\n",
      "    \"inf_columns\": 0,\n",
      "    \"all_numeric\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "üíæ Saved: ../artifacts/phase_2/phase2_2_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Generate encoding summary\n",
    "summary = {\n",
    "    \"phase\": \"Phase-2.2 (Encoding & Normalization)\",\n",
    "    \"timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"frozen_schema_version\": frozen_schema['schema_version'],\n",
    "    \"input_shape\": [dataset_info['total_rows'], dataset_info['total_cols']],\n",
    "    \"output_shape\": list(feature_df_encoded.shape),\n",
    "    \"transformations\": {\n",
    "        \"placeholder_handling\": len([col for col in strategy_map if col in dataset_info['col_names']]),\n",
    "        \"onehot_encoded\": len(onehot_cols),\n",
    "        \"ordinal_encoded\": 4,\n",
    "        \"binary_encoded\": len([col for col, method in encoding_map.items() if method == 'binary']),\n",
    "        \"log_transformed\": len([col for col, treatment in treatment_map.items() if 'log' in treatment]),\n",
    "        \"robust_scaled\": len(robust_scalers),\n",
    "        \"standard_scaled\": len(standard_scalers)\n",
    "    },\n",
    "    \"feature_expansion\": {\n",
    "        \"original_features\": dataset_info['total_cols'],\n",
    "        \"encoded_features\": feature_df_encoded.shape[1],\n",
    "        \"expansion_factor\": round(feature_df_encoded.shape[1] / dataset_info['total_cols'], 2)\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"nan_columns\": len(nan_cols),\n",
    "        \"inf_columns\": len(inf_cols),\n",
    "        \"all_numeric\": len(non_numeric) == 0\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = f\"{PHASE_2_DIR}/phase2_2_summary.json\"\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nüìä Phase-2.2 Summary:\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(f\"\\nüíæ Saved: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
