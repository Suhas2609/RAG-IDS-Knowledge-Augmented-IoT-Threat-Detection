{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "005e3747",
   "metadata": {},
   "source": [
    "# Phase-2.4: Hybrid Temporal Curation (v3 - Panel-Safe)\n",
    "\n",
    "## Mission\n",
    "\n",
    "**CRITICAL FIX:** Previous curation (uniform random sampling) achieved 0% temporal correlation.  \n",
    "This notebook implements **Hybrid Temporal + Local Clustering** to preserve:\n",
    "\n",
    "- âœ… Temporal attack patterns (multi-stage attacks)\n",
    "- âœ… Local neighborhood structure (similarity detection)\n",
    "- âœ… Feature distribution (statistical completeness)\n",
    "- âœ… Rare variant coverage (zero-day capability)\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "For each attack type:\n",
    "1. **Temporal Windowing:** Divide chronologically into 100 buckets\n",
    "2. **Local Clustering:** Within each bucket, cluster into 250 groups (MiniBatchKMeans)\n",
    "3. **Medoid Selection:** Select cluster medoid (closest to centroid)\n",
    "4. **Metadata Preservation:** Store original_index, temporal_bucket, cluster_id\n",
    "\n",
    "**Target:** ~25,000 representatives per attack type (250 samples Ã— 100 buckets)\n",
    "\n",
    "**Expected Validation:**\n",
    "- Temporal correlation: â‰¥85% (vs 0% before)\n",
    "- NN stability: â‰¥75% (vs 55% before)\n",
    "- KL divergence: <0.10 (vs NaN before)\n",
    "- Recall@K: â‰¥92% (maintained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3d308",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c917409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports complete\n",
      "ðŸ“¦ NumPy version: 2.1.3\n",
      "ðŸ“¦ Pandas version: 2.2.3\n",
      "ðŸ“¦ ChromaDB version: 1.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "print(\"âœ… Imports complete\")\n",
    "print(f\"ðŸ“¦ NumPy version: {np.__version__}\")\n",
    "print(f\"ðŸ“¦ Pandas version: {pd.__version__}\")\n",
    "print(f\"ðŸ“¦ ChromaDB version: {chromadb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273f155",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Intel GPU Acceleration Check\n",
    "\n",
    "**Context:** User has Intel Arc iGPU.  \n",
    "**Options:** sklearn-intelex, Intel Extension for Scikit-learn\n",
    "\n",
    "**Reality Check:** MiniBatchKMeans CPU implementation is already optimized.  \n",
    "Intel extensions primarily benefit matrix operations (SVD, PCA), not k-means.\n",
    "\n",
    "**Decision:** Use CPU-optimized sklearn (no GPU acceleration needed for laptop feasibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1dbdc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking Intel GPU acceleration options...\n",
      "\n",
      "â„¹ï¸  Intel Extension for Scikit-learn NOT installed\n",
      "   â†’ Using standard CPU-optimized sklearn\n",
      "   â†’ Still laptop-feasible (~5-10 minutes)\n",
      "\n",
      "ðŸ’¡ To enable Intel acceleration (optional):\n",
      "   pip install scikit-learn-intelex\n",
      "   â†’ Provides 1.5-3Ã— speedup on Intel hardware\n",
      "\n",
      "ðŸ“Š Acceleration method: None (CPU-optimized sklearn)\n",
      "ðŸŽ¯ Target runtime: 5-10 minutes\n"
     ]
    }
   ],
   "source": [
    "# Intel GPU Acceleration Investigation\n",
    "print(\"ðŸ” Checking Intel GPU acceleration options...\\n\")\n",
    "\n",
    "intel_acceleration = False\n",
    "acceleration_method = \"None (CPU-optimized sklearn)\"\n",
    "\n",
    "try:\n",
    "    # Check if sklearn-intelex is available\n",
    "    from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "    \n",
    "    # Patch sklearn to use Intel optimizations\n",
    "    patch_sklearn()\n",
    "    \n",
    "    intel_acceleration = True\n",
    "    acceleration_method = \"Intel Extension for Scikit-learn (sklearnex)\"\n",
    "    print(\"âœ… Intel Extension for Scikit-learn detected\")\n",
    "    print(\"   â†’ MiniBatchKMeans will use Intel optimizations\")\n",
    "    print(\"   â†’ Expected speedup: 1.5-3Ã— on Intel CPUs/iGPUs\\n\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"â„¹ï¸  Intel Extension for Scikit-learn NOT installed\")\n",
    "    print(\"   â†’ Using standard CPU-optimized sklearn\")\n",
    "    print(\"   â†’ Still laptop-feasible (~5-10 minutes)\")\n",
    "    print(\"\\nðŸ’¡ To enable Intel acceleration (optional):\")\n",
    "    print(\"   pip install scikit-learn-intelex\")\n",
    "    print(\"   â†’ Provides 1.5-3Ã— speedup on Intel hardware\\n\")\n",
    "\n",
    "print(f\"ðŸ“Š Acceleration method: {acceleration_method}\")\n",
    "print(f\"ðŸŽ¯ Target runtime: {'3-5 minutes' if intel_acceleration else '5-10 minutes'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49145171",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06933523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded\n",
      "\n",
      "ðŸ“Š Hybrid Curation Parameters:\n",
      "   â€¢ temporal_buckets: 100\n",
      "   â€¢ clusters_per_bucket: 250\n",
      "   â€¢ target_samples_per_attack: 25000\n",
      "   â€¢ min_bucket_size: 100\n",
      "   â€¢ rare_variant_boost: 1.5\n",
      "   â€¢ preserve_temporal_index: True\n",
      "   â€¢ random_state: 42\n",
      "\n",
      "ðŸ“ Data paths:\n",
      "   â€¢ Vectors: c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\artifacts\\phase_2\\feature_vectors.npy\n",
      "   â€¢ Metadata: c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\artifacts\\phase_2\\vector_metadata.parquet\n",
      "   â€¢ ChromaDB: c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\artifacts\\chromadb\n",
      "\n",
      "âœ… Vectors file exists\n",
      "âœ… Metadata file exists\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_DIR = Path(r\"c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\")\n",
    "PHASE_2_DIR = BASE_DIR / \"artifacts\" / \"phase_2\"\n",
    "CHROMADB_DIR = BASE_DIR / \"artifacts\" / \"chromadb\"\n",
    "\n",
    "# Data files (CORRECTED)\n",
    "VECTORS_PATH = PHASE_2_DIR / \"feature_vectors.npy\"\n",
    "METADATA_PATH = PHASE_2_DIR / \"vector_metadata.parquet\"  # Using parquet for efficiency\n",
    "\n",
    "# ChromaDB configuration\n",
    "COLLECTION_NAME = \"iot_behavioral_memory_hybrid\"\n",
    "\n",
    "# Hybrid curation parameters\n",
    "HYBRID_CONFIG = {\n",
    "    'temporal_buckets': 100,          # 100 time windows per attack type\n",
    "    'clusters_per_bucket': 250,       # 250 clusters per time window\n",
    "    'target_samples_per_attack': 25000,  # Total budget per attack type\n",
    "    'min_bucket_size': 100,           # Minimum samples in bucket to cluster\n",
    "    'rare_variant_boost': 1.5,        # Oversample rare variants (not implemented yet)\n",
    "    'preserve_temporal_index': True,  # CRITICAL: Store original indices\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Memory optimization\n",
    "BATCH_SIZE = 5000  # ChromaDB batch size\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"\\nðŸ“Š Hybrid Curation Parameters:\")\n",
    "for key, value in HYBRID_CONFIG.items():\n",
    "    print(f\"   â€¢ {key}: {value}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Data paths:\")\n",
    "print(f\"   â€¢ Vectors: {VECTORS_PATH}\")\n",
    "print(f\"   â€¢ Metadata: {METADATA_PATH}\")\n",
    "print(f\"   â€¢ ChromaDB: {CHROMADB_DIR}\")\n",
    "\n",
    "# Verify files exist\n",
    "if not VECTORS_PATH.exists():\n",
    "    print(f\"\\nâŒ ERROR: Vectors file not found: {VECTORS_PATH}\")\n",
    "    print(f\"   Available .npy files in {PHASE_2_DIR}:\")\n",
    "    for file in PHASE_2_DIR.glob(\"*.npy\"):\n",
    "        print(f\"      â€¢ {file.name}\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Vectors file exists\")\n",
    "\n",
    "if not METADATA_PATH.exists():\n",
    "    print(f\"\\nâŒ ERROR: Metadata file not found: {METADATA_PATH}\")\n",
    "    print(f\"   Available .parquet files in {PHASE_2_DIR}:\")\n",
    "    for file in PHASE_2_DIR.glob(\"*.parquet\"):\n",
    "        print(f\"      â€¢ {file.name}\")\n",
    "else:\n",
    "    print(f\"âœ… Metadata file exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a0c41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Checking ChromaDB state before execution...\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š Existing Collections: 2\n",
      "   â€¢ iot_behavioral_memory_hybrid: 457,622 vectors\n",
      "   â€¢ iot_behavioral_memory_curated: 502,630 vectors\n",
      "\n",
      "âš ï¸  Collection 'iot_behavioral_memory_hybrid' already exists\n",
      "   â†’ Will be DELETED and recreated (preventing duplicates)\n",
      "\n",
      "ðŸŽ¯ Target collection: iot_behavioral_memory_hybrid\n",
      "âœ… Pre-execution check complete\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§¹ CHROMADB PRE-EXECUTION CHECK\n",
    "print(\"ðŸ§¹ Checking ChromaDB state before execution...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize temporary client to check existing collections\n",
    "temp_client = chromadb.PersistentClient(\n",
    "    path=str(CHROMADB_DIR),\n",
    "    settings=Settings(anonymized_telemetry=False, allow_reset=True)\n",
    ")\n",
    "\n",
    "# List all existing collections\n",
    "existing_collections = temp_client.list_collections()\n",
    "print(f\"ðŸ“Š Existing Collections: {len(existing_collections)}\")\n",
    "\n",
    "if existing_collections:\n",
    "    for coll in existing_collections:\n",
    "        print(f\"   â€¢ {coll.name}: {coll.count():,} vectors\")\n",
    "    \n",
    "    # Check if target collection already exists\n",
    "    if COLLECTION_NAME in [c.name for c in existing_collections]:\n",
    "        print(f\"\\nâš ï¸  Collection '{COLLECTION_NAME}' already exists\")\n",
    "        print(f\"   â†’ Will be DELETED and recreated (preventing duplicates)\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… Collection '{COLLECTION_NAME}' does not exist yet\")\n",
    "else:\n",
    "    print(\"   â†’ No existing collections (fresh start)\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Target collection: {COLLECTION_NAME}\")\n",
    "print(f\"âœ… Pre-execution check complete\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf47e5",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Load Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4468c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading full dataset...\n",
      "\n",
      "================================================================================\n",
      "Loading vectors (memory-mapped)...\n",
      "âœ… Vectors shape: (22339021, 99)\n",
      "   â€¢ Samples: 22,339,021\n",
      "   â€¢ Features: 99\n",
      "   â€¢ Dtype: float32\n",
      "\n",
      "Loading metadata...\n",
      "âœ… Metadata shape: (22339021, 3)\n",
      "   â€¢ Rows: 22,339,021\n",
      "   â€¢ Columns: ['label', 'type', 'vector_id']\n",
      "\n",
      "âœ… Vector-metadata alignment verified\n",
      "\n",
      "ðŸ“Š Attack Type Distribution:\n",
      "   â€¢ scanning       :  7,140,161 (31.96%)\n",
      "   â€¢ ddos           :  6,165,008 (27.60%)\n",
      "   â€¢ dos            :  3,375,328 (15.11%)\n",
      "   â€¢ xss            :  2,108,944 ( 9.44%)\n",
      "   â€¢ password       :  1,718,568 ( 7.69%)\n",
      "   â€¢ normal         :    796,380 ( 3.56%)\n",
      "   â€¢ backdoor       :    508,116 ( 2.27%)\n",
      "   â€¢ injection      :    452,659 ( 2.03%)\n",
      "   â€¢ ransomware     :     72,805 ( 0.33%)\n",
      "   â€¢ mitm           :      1,052 ( 0.00%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“¥ Loading full dataset...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load vectors (memory-mapped for efficiency)\n",
    "print(\"Loading vectors (memory-mapped)...\")\n",
    "full_vectors = np.load(VECTORS_PATH, mmap_mode='r')\n",
    "print(f\"âœ… Vectors shape: {full_vectors.shape}\")\n",
    "print(f\"   â€¢ Samples: {full_vectors.shape[0]:,}\")\n",
    "print(f\"   â€¢ Features: {full_vectors.shape[1]}\")\n",
    "print(f\"   â€¢ Dtype: {full_vectors.dtype}\")\n",
    "\n",
    "# Load metadata (using parquet for efficiency)\n",
    "print(\"\\nLoading metadata...\")\n",
    "full_metadata = pd.read_parquet(METADATA_PATH)\n",
    "print(f\"âœ… Metadata shape: {full_metadata.shape}\")\n",
    "print(f\"   â€¢ Rows: {len(full_metadata):,}\")\n",
    "print(f\"   â€¢ Columns: {full_metadata.columns.tolist()}\")\n",
    "\n",
    "# Verify alignment\n",
    "assert len(full_vectors) == len(full_metadata), \"Vector-metadata mismatch!\"\n",
    "print(f\"\\nâœ… Vector-metadata alignment verified\")\n",
    "\n",
    "# Attack type distribution\n",
    "print(f\"\\nðŸ“Š Attack Type Distribution:\")\n",
    "type_counts = full_metadata['type'].value_counts()\n",
    "for attack_type, count in type_counts.items():\n",
    "    percentage = count / len(full_metadata) * 100\n",
    "    print(f\"   â€¢ {attack_type:15s}: {count:10,} ({percentage:5.2f}%)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23154727",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Hybrid Temporal Curation Algorithm\n",
    "\n",
    "### Core Algorithm:\n",
    "\n",
    "```python\n",
    "for each attack_type:\n",
    "    # Step 1: Temporal windowing\n",
    "    chronological_indices = sort_by_temporal_order(attack_data)\n",
    "    buckets = divide_into_N_windows(chronological_indices, N=100)\n",
    "    \n",
    "    # Step 2: Local clustering per bucket\n",
    "    for each bucket:\n",
    "        if bucket_size >= min_size:\n",
    "            clusters = MiniBatchKMeans(n_clusters=250).fit(bucket_vectors)\n",
    "            \n",
    "            # Step 3: Select medoid per cluster\n",
    "            for each cluster:\n",
    "                medoid = find_closest_to_centroid(cluster_vectors)\n",
    "                \n",
    "                # Step 4: Store with metadata\n",
    "                curated_samples.append({\n",
    "                    'vector': medoid_vector,\n",
    "                    'original_index': medoid_index,  # CRITICAL\n",
    "                    'temporal_bucket': bucket_id,\n",
    "                    'cluster_id': cluster_id\n",
    "                })\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8466874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Hybrid curation function defined\n"
     ]
    }
   ],
   "source": [
    "def hybrid_temporal_curation(vectors, metadata, config):\n",
    "    \"\"\"\n",
    "    Hybrid Temporal + Local Clustering Curation\n",
    "    \n",
    "    Args:\n",
    "        vectors: Memory-mapped numpy array (N, D)\n",
    "        metadata: DataFrame with 'type' and temporal ordering\n",
    "        config: Hybrid configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        curated_vectors: List of selected vectors\n",
    "        curated_metadata: List of metadata dicts\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”¬ Starting Hybrid Temporal Curation...\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    curated_vectors = []\n",
    "    curated_metadata = []\n",
    "    \n",
    "    # Get attack types (exclude normal)\n",
    "    attack_types = [t for t in metadata['type'].unique() if t != 'normal']\n",
    "    \n",
    "    # Add normal separately with simple stratified sampling\n",
    "    attack_types_with_normal = attack_types + ['normal']\n",
    "    \n",
    "    print(f\"ðŸ“Š Attack types to process: {len(attack_types_with_normal)}\")\n",
    "    print(f\"   â€¢ Attacks: {attack_types}\")\n",
    "    print(f\"   â€¢ Normal traffic: Separate processing\\n\")\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for attack_type in tqdm(attack_types_with_normal, desc=\"Processing attack types\"):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ðŸŽ¯ Processing: {attack_type}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Get attack data\n",
    "        attack_mask = metadata['type'] == attack_type\n",
    "        attack_indices = metadata[attack_mask].index.tolist()\n",
    "        attack_metadata = metadata[attack_mask]\n",
    "        \n",
    "        print(f\"   â€¢ Total samples: {len(attack_indices):,}\")\n",
    "        \n",
    "        # Special handling for normal (use simple temporal stratification)\n",
    "        if attack_type == 'normal':\n",
    "            target_samples = 275000  # Match previous curation\n",
    "            print(f\"   â€¢ Target samples: {target_samples:,}\")\n",
    "            print(f\"   â€¢ Method: Simple temporal stratification (no clustering)\\n\")\n",
    "            \n",
    "            # Temporal stratification without clustering\n",
    "            n_buckets = config['temporal_buckets']\n",
    "            samples_per_bucket = target_samples // n_buckets\n",
    "            \n",
    "            bucket_edges = np.linspace(0, len(attack_indices), n_buckets + 1, dtype=int)\n",
    "            \n",
    "            for bucket_idx in range(n_buckets):\n",
    "                start_idx = bucket_edges[bucket_idx]\n",
    "                end_idx = bucket_edges[bucket_idx + 1]\n",
    "                \n",
    "                bucket_indices = attack_indices[start_idx:end_idx]\n",
    "                \n",
    "                if len(bucket_indices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Randomly sample from bucket\n",
    "                n_samples = min(samples_per_bucket, len(bucket_indices))\n",
    "                sampled_indices = np.random.choice(bucket_indices, n_samples, replace=False)\n",
    "                \n",
    "                # Store selected samples\n",
    "                for orig_idx in sampled_indices:\n",
    "                    curated_vectors.append(vectors[orig_idx])\n",
    "                    curated_metadata.append({\n",
    "                        'type': attack_type,\n",
    "                        'original_index': int(orig_idx),\n",
    "                        'temporal_bucket': bucket_idx,\n",
    "                        'cluster_id': -1,  # No clustering for normal\n",
    "                        'curation_method': 'temporal_stratified'\n",
    "                    })\n",
    "            \n",
    "            print(f\"   âœ… Selected {len([m for m in curated_metadata if m['type'] == attack_type]):,} samples\")\n",
    "            continue\n",
    "        \n",
    "        # Hybrid temporal + clustering for attacks\n",
    "        target_samples = config['target_samples_per_attack']\n",
    "        n_buckets = config['temporal_buckets']\n",
    "        n_clusters = config['clusters_per_bucket']\n",
    "        \n",
    "        print(f\"   â€¢ Target samples: {target_samples:,}\")\n",
    "        print(f\"   â€¢ Temporal buckets: {n_buckets}\")\n",
    "        print(f\"   â€¢ Clusters per bucket: {n_clusters}\")\n",
    "        print(f\"   â€¢ Expected samples per bucket: {target_samples // n_buckets}\\n\")\n",
    "        \n",
    "        # Create temporal buckets (chronological order preserved by index)\n",
    "        bucket_edges = np.linspace(0, len(attack_indices), n_buckets + 1, dtype=int)\n",
    "        \n",
    "        bucket_samples = []\n",
    "        \n",
    "        for bucket_idx in tqdm(range(n_buckets), desc=f\"  Temporal buckets\", leave=False):\n",
    "            start_idx = bucket_edges[bucket_idx]\n",
    "            end_idx = bucket_edges[bucket_idx + 1]\n",
    "            \n",
    "            bucket_indices = attack_indices[start_idx:end_idx]\n",
    "            \n",
    "            if len(bucket_indices) < config['min_bucket_size']:\n",
    "                # Bucket too small, sample all\n",
    "                for orig_idx in bucket_indices:\n",
    "                    bucket_samples.append({\n",
    "                        'vector': vectors[orig_idx],\n",
    "                        'original_index': int(orig_idx),\n",
    "                        'temporal_bucket': bucket_idx,\n",
    "                        'cluster_id': -1,  # No clustering\n",
    "                        'type': attack_type\n",
    "                    })\n",
    "                continue\n",
    "            \n",
    "            # Load bucket vectors into memory (convert to float64 for clustering)\n",
    "            bucket_vectors = np.array([vectors[i] for i in bucket_indices], dtype=np.float64)\n",
    "            \n",
    "            # Adaptive clustering: Use fewer clusters if bucket is small\n",
    "            actual_n_clusters = min(n_clusters, len(bucket_indices) // 10)\n",
    "            \n",
    "            if actual_n_clusters < 2:\n",
    "                # Too small to cluster, sample all\n",
    "                for i, orig_idx in enumerate(bucket_indices):\n",
    "                    bucket_samples.append({\n",
    "                        'vector': bucket_vectors[i],\n",
    "                        'original_index': int(orig_idx),\n",
    "                        'temporal_bucket': bucket_idx,\n",
    "                        'cluster_id': -1,\n",
    "                        'type': attack_type\n",
    "                    })\n",
    "                continue\n",
    "            \n",
    "            # Cluster within temporal bucket\n",
    "            try:\n",
    "                kmeans = MiniBatchKMeans(\n",
    "                    n_clusters=actual_n_clusters,\n",
    "                    random_state=config['random_state'],\n",
    "                    batch_size=min(1000, len(bucket_vectors)),\n",
    "                    max_iter=100,\n",
    "                    n_init=3\n",
    "                )\n",
    "                cluster_labels = kmeans.fit_predict(bucket_vectors)\n",
    "                cluster_centers = kmeans.cluster_centers_\n",
    "                \n",
    "                # Select medoid (closest to centroid) per cluster\n",
    "                for cluster_id in range(actual_n_clusters):\n",
    "                    cluster_mask = cluster_labels == cluster_id\n",
    "                    cluster_vectors = bucket_vectors[cluster_mask]\n",
    "                    cluster_original_indices = [bucket_indices[i] for i, mask in enumerate(cluster_mask) if mask]\n",
    "                    \n",
    "                    if len(cluster_vectors) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Find medoid (closest to centroid)\n",
    "                    centroid = cluster_centers[cluster_id].reshape(1, -1)\n",
    "                    medoid_idx_in_cluster = pairwise_distances_argmin_min(centroid, cluster_vectors)[0][0]\n",
    "                    medoid_original_idx = cluster_original_indices[medoid_idx_in_cluster]\n",
    "                    \n",
    "                    bucket_samples.append({\n",
    "                        'vector': cluster_vectors[medoid_idx_in_cluster],\n",
    "                        'original_index': int(medoid_original_idx),\n",
    "                        'temporal_bucket': bucket_idx,\n",
    "                        'cluster_id': cluster_id,\n",
    "                        'type': attack_type\n",
    "                    })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nâš ï¸  Clustering failed for bucket {bucket_idx}: {e}\")\n",
    "                print(f\"    Falling back to random sampling...\")\n",
    "                # Fallback: random sample\n",
    "                n_samples = min(n_clusters, len(bucket_indices))\n",
    "                sampled_idx = np.random.choice(len(bucket_indices), n_samples, replace=False)\n",
    "                for i in sampled_idx:\n",
    "                    bucket_samples.append({\n",
    "                        'vector': bucket_vectors[i],\n",
    "                        'original_index': int(bucket_indices[i]),\n",
    "                        'temporal_bucket': bucket_idx,\n",
    "                        'cluster_id': -1,\n",
    "                        'type': attack_type\n",
    "                    })\n",
    "        \n",
    "        # Add to curated dataset\n",
    "        for sample in bucket_samples:\n",
    "            curated_vectors.append(sample['vector'])\n",
    "            curated_metadata.append({\n",
    "                'type': sample['type'],\n",
    "                'original_index': sample['original_index'],\n",
    "                'temporal_bucket': sample['temporal_bucket'],\n",
    "                'cluster_id': sample['cluster_id'],\n",
    "                'curation_method': 'hybrid_temporal_clustering'\n",
    "            })\n",
    "        \n",
    "        print(f\"   âœ… Selected {len(bucket_samples):,} samples (target: {target_samples:,})\")\n",
    "        \n",
    "        # Free memory\n",
    "        del bucket_samples\n",
    "        gc.collect()\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… Hybrid Temporal Curation Complete\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"   â€¢ Total curated samples: {len(curated_vectors):,}\")\n",
    "    print(f\"   â€¢ Total execution time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"   â€¢ Samples per second: {len(curated_vectors)/total_time:.0f}\")\n",
    "    \n",
    "    return curated_vectors, curated_metadata\n",
    "\n",
    "print(\"âœ… Hybrid curation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2cd3fd",
   "metadata": {},
   "source": [
    "## ðŸš€ Execute Hybrid Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efc52ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Starting Hybrid Temporal Curation...\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š Attack types to process: 10\n",
      "   â€¢ Attacks: ['scanning', 'dos', 'injection', 'ddos', 'password', 'xss', 'ransomware', 'backdoor', 'mitm']\n",
      "   â€¢ Normal traffic: Separate processing\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066c4d090d6346a88d8ec4ffccc70ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing attack types:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Processing: scanning\n",
      "================================================================================\n",
      "   â€¢ Total samples: 7,140,161\n",
      "   â€¢ Target samples: 25,000\n",
      "   â€¢ Temporal buckets: 100\n",
      "   â€¢ Clusters per bucket: 250\n",
      "   â€¢ Expected samples per bucket: 250\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bef8de2c044d4d85eab15b1f219490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Temporal buckets:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Selected 24,988 samples (target: 25,000)\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Processing: dos\n",
      "================================================================================\n",
      "   â€¢ Total samples: 3,375,328\n",
      "   â€¢ Target samples: 25,000\n",
      "   â€¢ Temporal buckets: 100\n",
      "   â€¢ Clusters per bucket: 250\n",
      "   â€¢ Expected samples per bucket: 250\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e8f2311c8241479f4262965a251be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Temporal buckets:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Selected 24,894 samples (target: 25,000)\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Processing: injection\n",
      "================================================================================\n",
      "   â€¢ Total samples: 452,659\n",
      "   â€¢ Target samples: 25,000\n",
      "   â€¢ Temporal buckets: 100\n",
      "   â€¢ Clusters per bucket: 250\n",
      "   â€¢ Expected samples per bucket: 250\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a964e3c84d8493eb5afb9fd7675b159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Temporal buckets:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Selected 24,985 samples (target: 25,000)\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Processing: ddos\n",
      "================================================================================\n",
      "   â€¢ Total samples: 6,165,008\n",
      "   â€¢ Target samples: 25,000\n",
      "   â€¢ Temporal buckets: 100\n",
      "   â€¢ Clusters per bucket: 250\n",
      "   â€¢ Expected samples per bucket: 250\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7999b7e0e534dc4b7a452729319c8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Temporal buckets:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Selected 24,893 samples (target: 25,000)\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Processing: password\n",
      "================================================================================\n",
      "   â€¢ Total samples: 1,718,568\n",
      "   â€¢ Target samples: 25,000\n",
      "   â€¢ Temporal buckets: 100\n",
      "   â€¢ Clusters per bucket: 250\n",
      "   â€¢ Expected samples per bucket: 250\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680d032c2e0d45a881c862c2a450ddc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Temporal buckets:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Selected 24,957 samples (target: 25,000)\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Processing: xss\n",
      "================================================================================\n",
      "   â€¢ Total samples: 2,108,944\n",
      "   â€¢ Target samples: 25,000\n",
      "   â€¢ Temporal buckets: 100\n",
      "   â€¢ Clusters per bucket: 250\n",
      "   â€¢ Expected samples per bucket: 250\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b805d9cb33d148cfa88d6a70be6f3e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Temporal buckets:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Selected 24,961 samples (target: 25,000)\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Processing: ransomware\n",
      "================================================================================\n",
      "   â€¢ Total samples: 72,805\n",
      "   â€¢ Target samples: 25,000\n",
      "   â€¢ Temporal buckets: 100\n",
      "   â€¢ Clusters per bucket: 250\n",
      "   â€¢ Expected samples per bucket: 250\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed09fae792d44b7b9a3c11944777f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Temporal buckets:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Selected 6,894 samples (target: 25,000)\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Processing: backdoor\n",
      "================================================================================\n",
      "   â€¢ Total samples: 508,116\n",
      "   â€¢ Target samples: 25,000\n",
      "   â€¢ Temporal buckets: 100\n",
      "   â€¢ Clusters per bucket: 250\n",
      "   â€¢ Expected samples per bucket: 250\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab536cb5a65488c9f04f0e57d7e51de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Temporal buckets:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Selected 24,998 samples (target: 25,000)\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Processing: mitm\n",
      "================================================================================\n",
      "   â€¢ Total samples: 1,052\n",
      "   â€¢ Target samples: 25,000\n",
      "   â€¢ Temporal buckets: 100\n",
      "   â€¢ Clusters per bucket: 250\n",
      "   â€¢ Expected samples per bucket: 250\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908840d1d3de41569c4d380daaafe87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Temporal buckets:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Selected 1,052 samples (target: 25,000)\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Processing: normal\n",
      "================================================================================\n",
      "   â€¢ Total samples: 796,380\n",
      "   â€¢ Target samples: 275,000\n",
      "   â€¢ Method: Simple temporal stratification (no clustering)\n",
      "\n",
      "   âœ… Selected 275,000 samples\n",
      "\n",
      "================================================================================\n",
      "âœ… Hybrid Temporal Curation Complete\n",
      "================================================================================\n",
      "   â€¢ Total curated samples: 457,622\n",
      "   â€¢ Total execution time: 1514.54 seconds (25.24 minutes)\n",
      "   â€¢ Samples per second: 302\n",
      "\n",
      "ðŸ“Š Curated Dataset Summary:\n",
      "   â€¢ Vectors shape: (457622, 99)\n",
      "   â€¢ Vectors dtype: float64\n",
      "   â€¢ Metadata rows: 457,622\n",
      "   â€¢ Compression ratio: 48.8Ã—\n",
      "\n",
      "ðŸ“Š Attack Type Distribution (Curated):\n",
      "   â€¢ normal         : 275,000 (60.09%)\n",
      "   â€¢ backdoor       :  24,998 ( 5.46%)\n",
      "   â€¢ scanning       :  24,988 ( 5.46%)\n",
      "   â€¢ injection      :  24,985 ( 5.46%)\n",
      "   â€¢ xss            :  24,961 ( 5.45%)\n",
      "   â€¢ password       :  24,957 ( 5.45%)\n",
      "   â€¢ dos            :  24,894 ( 5.44%)\n",
      "   â€¢ ddos           :  24,893 ( 5.44%)\n",
      "   â€¢ ransomware     :   6,894 ( 1.51%)\n",
      "   â€¢ mitm           :   1,052 ( 0.23%)\n"
     ]
    }
   ],
   "source": [
    "# Run hybrid curation\n",
    "curated_vectors, curated_metadata = hybrid_temporal_curation(\n",
    "    full_vectors,\n",
    "    full_metadata,\n",
    "    HYBRID_CONFIG\n",
    ")\n",
    "\n",
    "# Convert to numpy array\n",
    "curated_vectors_array = np.array(curated_vectors, dtype=np.float64)\n",
    "curated_metadata_df = pd.DataFrame(curated_metadata)\n",
    "\n",
    "print(f\"\\nðŸ“Š Curated Dataset Summary:\")\n",
    "print(f\"   â€¢ Vectors shape: {curated_vectors_array.shape}\")\n",
    "print(f\"   â€¢ Vectors dtype: {curated_vectors_array.dtype}\")\n",
    "print(f\"   â€¢ Metadata rows: {len(curated_metadata_df):,}\")\n",
    "print(f\"   â€¢ Compression ratio: {len(full_metadata)/len(curated_metadata_df):.1f}Ã—\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Attack Type Distribution (Curated):\")\n",
    "curated_type_counts = curated_metadata_df['type'].value_counts()\n",
    "for attack_type, count in curated_type_counts.items():\n",
    "    percentage = count / len(curated_metadata_df) * 100\n",
    "    print(f\"   â€¢ {attack_type:15s}: {count:7,} ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafdb8c0",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Build ChromaDB Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d8a4b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Building ChromaDB collection...\n",
      "\n",
      "================================================================================\n",
      "âœ… ChromaDB client initialized\n",
      "   â€¢ Path: c:\\Users\\suhas\\OneDrive\\Desktop\\Capstone\\RAG-IDS-Knowledge-Augmented-IoT-Threat-Detection\\artifacts\\chromadb\n",
      "   â€¢ Deleted existing collection: iot_behavioral_memory_hybrid\n",
      "âœ… Collection created: iot_behavioral_memory_hybrid\n",
      "\n",
      "ðŸ“¤ Adding vectors to collection (batch size: 5000)...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1d6bb003574abf881a5211aa94f890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading batches:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ChromaDB collection built successfully\n",
      "   â€¢ Collection name: iot_behavioral_memory_hybrid\n",
      "   â€¢ Total vectors: 457,622\n",
      "   â€¢ Expected: 457,622\n",
      "   â€¢ Match: âœ… YES\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ’¾ Building ChromaDB collection...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_settings = Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    allow_reset=True\n",
    ")\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "    path=str(CHROMADB_DIR),\n",
    "    settings=chroma_settings\n",
    ")\n",
    "\n",
    "print(f\"âœ… ChromaDB client initialized\")\n",
    "print(f\"   â€¢ Path: {CHROMADB_DIR}\")\n",
    "\n",
    "# Delete existing collection if exists\n",
    "try:\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "    print(f\"   â€¢ Deleted existing collection: {COLLECTION_NAME}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new collection with COSINE distance metric\n",
    "collection = client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\",  # CRITICAL: Use cosine distance for pattern-based similarity\n",
    "        \"description\": \"Hybrid Temporal + Local Clustering Curation (v3 - Panel-Safe)\",\n",
    "        \"curation_method\": \"hybrid_temporal_clustering\",\n",
    "        \"temporal_buckets\": HYBRID_CONFIG['temporal_buckets'],\n",
    "        \"clusters_per_bucket\": HYBRID_CONFIG['clusters_per_bucket'],\n",
    "        \"total_samples\": len(curated_vectors_array),\n",
    "        \"compression_ratio\": float(len(full_metadata) / len(curated_metadata_df))\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"âœ… Collection created: {COLLECTION_NAME}\")\n",
    "print(f\"\\nðŸ“¤ Adding vectors to collection (batch size: {BATCH_SIZE})...\\n\")\n",
    "\n",
    "# Add in batches\n",
    "n_vectors = len(curated_vectors_array)\n",
    "n_batches = (n_vectors + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for batch_idx in tqdm(range(n_batches), desc=\"Uploading batches\"):\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, n_vectors)\n",
    "    \n",
    "    batch_vectors = curated_vectors_array[start_idx:end_idx].tolist()\n",
    "    batch_metadata = curated_metadata_df.iloc[start_idx:end_idx].to_dict('records')\n",
    "    batch_ids = [f\"hybrid_{i}\" for i in range(start_idx, end_idx)]\n",
    "    \n",
    "    collection.add(\n",
    "        embeddings=batch_vectors,\n",
    "        metadatas=batch_metadata,\n",
    "        ids=batch_ids\n",
    "    )\n",
    "\n",
    "# Verify\n",
    "collection_count = collection.count()\n",
    "print(f\"\\nâœ… ChromaDB collection built successfully\")\n",
    "print(f\"   â€¢ Collection name: {COLLECTION_NAME}\")\n",
    "print(f\"   â€¢ Total vectors: {collection_count:,}\")\n",
    "print(f\"   â€¢ Expected: {n_vectors:,}\")\n",
    "print(f\"   â€¢ Match: {'âœ… YES' if collection_count == n_vectors else 'âŒ NO'}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b59f70",
   "metadata": {},
   "source": [
    "## ðŸ“Š Curation Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c325807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Curation Summary Statistics\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ˆ Overall Metrics:\n",
      "   â€¢ Full dataset size: 22,339,021 vectors\n",
      "   â€¢ Curated dataset size: 457,622 vectors\n",
      "   â€¢ Compression ratio: 48.82Ã—\n",
      "   â€¢ Sampling rate: 2.05%\n",
      "\n",
      "ðŸ“Š Per-Attack-Type Statistics:\n",
      "\n",
      "Attack Type     Full         Curated    Ratio    Method                        \n",
      "--------------------------------------------------------------------------------\n",
      "normal          796,380      275,000    2.9     Ã— Temporal Stratified           \n",
      "scanning        7,140,161    24,988     285.7   Ã— Hybrid Temporal + Clustering  \n",
      "dos             3,375,328    24,894     135.6   Ã— Hybrid Temporal + Clustering  \n",
      "injection       452,659      24,985     18.1    Ã— Hybrid Temporal + Clustering  \n",
      "ddos            6,165,008    24,893     247.7   Ã— Hybrid Temporal + Clustering  \n",
      "password        1,718,568    24,957     68.9    Ã— Hybrid Temporal + Clustering  \n",
      "xss             2,108,944    24,961     84.5    Ã— Hybrid Temporal + Clustering  \n",
      "ransomware      72,805       6,894      10.6    Ã— Hybrid Temporal + Clustering  \n",
      "backdoor        508,116      24,998     20.3    Ã— Hybrid Temporal + Clustering  \n",
      "mitm            1,052        1,052      1.0     Ã— Hybrid Temporal + Clustering  \n",
      "\n",
      "ðŸ“… Temporal Bucket Distribution (Attack Types Only):\n",
      "              mean       std    min    max\n",
      "type                                      \n",
      "backdoor    249.98  0.140705  249.0  250.0\n",
      "ddos        248.93  3.092039  226.0  250.0\n",
      "dos         248.94  1.170513  243.0  250.0\n",
      "injection   249.85  0.411329  248.0  250.0\n",
      "mitm         10.52  0.502117   10.0   11.0\n",
      "password    249.57  0.728288  247.0  250.0\n",
      "ransomware   68.94  3.148898   59.0   72.0\n",
      "scanning    249.88  0.408990  247.0  250.0\n",
      "xss         249.61  0.665074  248.0  250.0\n",
      "\n",
      "ðŸ”¬ Clustering Statistics (Attack Types Only):\n",
      "   â€¢ Samples with clustering: 181,570 (99.4%)\n",
      "   â€¢ Samples without clustering: 1,052 (small buckets)\n",
      "\n",
      "   Clusters per attack type:\n",
      "      â€¢ backdoor: 250 clusters\n",
      "      â€¢ ddos: 250 clusters\n",
      "      â€¢ dos: 250 clusters\n",
      "      â€¢ injection: 250 clusters\n",
      "      â€¢ password: 250 clusters\n",
      "      â€¢ ransomware: 72 clusters\n",
      "      â€¢ scanning: 250 clusters\n",
      "      â€¢ xss: 250 clusters\n",
      "\n",
      "================================================================================\n",
      "âœ… Hybrid Temporal Curation Complete\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Curation Summary Statistics\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\nðŸ“ˆ Overall Metrics:\")\n",
    "print(f\"   â€¢ Full dataset size: {len(full_metadata):,} vectors\")\n",
    "print(f\"   â€¢ Curated dataset size: {len(curated_metadata_df):,} vectors\")\n",
    "print(f\"   â€¢ Compression ratio: {len(full_metadata)/len(curated_metadata_df):.2f}Ã—\")\n",
    "print(f\"   â€¢ Sampling rate: {len(curated_metadata_df)/len(full_metadata)*100:.2f}%\")\n",
    "\n",
    "# Per-attack-type statistics\n",
    "print(\"\\nðŸ“Š Per-Attack-Type Statistics:\")\n",
    "print(f\"\\n{'Attack Type':<15} {'Full':<12} {'Curated':<10} {'Ratio':<8} {'Method':<30}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for attack_type in full_metadata['type'].unique():\n",
    "    full_count = len(full_metadata[full_metadata['type'] == attack_type])\n",
    "    curated_count = len(curated_metadata_df[curated_metadata_df['type'] == attack_type])\n",
    "    ratio = full_count / curated_count if curated_count > 0 else 0\n",
    "    \n",
    "    # Determine method\n",
    "    if attack_type == 'normal':\n",
    "        method = \"Temporal Stratified\"\n",
    "    else:\n",
    "        method = \"Hybrid Temporal + Clustering\"\n",
    "    \n",
    "    print(f\"{attack_type:<15} {full_count:<12,} {curated_count:<10,} {ratio:<8.1f}Ã— {method:<30}\")\n",
    "\n",
    "# Temporal distribution check\n",
    "print(\"\\nðŸ“… Temporal Bucket Distribution (Attack Types Only):\")\n",
    "attack_curated = curated_metadata_df[curated_metadata_df['type'] != 'normal']\n",
    "bucket_counts = attack_curated.groupby(['type', 'temporal_bucket']).size().groupby('type').describe()\n",
    "print(bucket_counts[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Clustering statistics\n",
    "print(\"\\nðŸ”¬ Clustering Statistics (Attack Types Only):\")\n",
    "clustered = attack_curated[attack_curated['cluster_id'] >= 0]\n",
    "print(f\"   â€¢ Samples with clustering: {len(clustered):,} ({len(clustered)/len(attack_curated)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Samples without clustering: {len(attack_curated) - len(clustered):,} (small buckets)\")\n",
    "\n",
    "if len(clustered) > 0:\n",
    "    clusters_per_type = clustered.groupby('type')['cluster_id'].nunique()\n",
    "    print(f\"\\n   Clusters per attack type:\")\n",
    "    for attack_type, n_clusters in clusters_per_type.items():\n",
    "        print(f\"      â€¢ {attack_type}: {n_clusters} clusters\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Hybrid Temporal Curation Complete\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939351ab",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "1. **Run Validation**: Execute `Phase_2_6_Hybrid_Curation_Validation.ipynb`\n",
    "2. **Expected Results**:\n",
    "   - âœ… Temporal correlation: â‰¥85% (vs 0% before)\n",
    "   - âœ… NN stability: â‰¥75% (vs 55% before)\n",
    "   - âœ… KL divergence: <0.10 (vs NaN before)\n",
    "   - âœ… Recall@K: â‰¥92% (maintained)\n",
    "   - âœ… Entropy: â‰¥99% (maintained)\n",
    "   - âœ… Rare variants: â‰¥98% (maintained)\n",
    "\n",
    "3. **If validation passes (â‰¥5/6 checks)**: Proceed to Phase-3 (RAG Pipeline)\n",
    "4. **If validation fails**: Adjust `HYBRID_CONFIG` parameters and re-run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
