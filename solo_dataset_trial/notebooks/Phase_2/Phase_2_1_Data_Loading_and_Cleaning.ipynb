{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8686cc0e",
   "metadata": {},
   "source": [
    "# üöÄ Phase-2.1: Data Loading & Schema Validation\n",
    "## Quantum-RAG Knowledge Fusion for Adaptive IoT Intrusion Detection\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Phase-2.1 Objective\n",
    "\n",
    "**This notebook implements STRICT adherence to Phase-1 frozen schema.**\n",
    "\n",
    "Phase-2.1 Goals:\n",
    "1. ‚úÖ Load frozen schema from Phase-1\n",
    "2. ‚úÖ Load all 23 TON-IoT CSV files\n",
    "3. ‚úÖ Validate column integrity (no missing/extra columns)\n",
    "4. ‚úÖ Drop features marked as DROP in frozen schema\n",
    "5. ‚úÖ Preserve labels (`label`, `type`) as metadata ONLY\n",
    "6. ‚úÖ Create clean dataset ready for encoding\n",
    "\n",
    "### üîí Phase-2.1 Rules\n",
    "\n",
    "| Rule | Status |\n",
    "|------|--------|\n",
    "| ‚ùå No new feature decisions | Strict |\n",
    "| ‚ùå No schema modifications | Strict |\n",
    "| ‚ùå No encoding yet | Strict |\n",
    "| ‚úî Load & validate only | Required |\n",
    "| ‚úî Preserve Phase-1 decisions exactly | Required |\n",
    "\n",
    "### üìä Key Principles\n",
    "\n",
    "- **Frozen Schema Compliance**: Every action traceable to Phase-1\n",
    "- **No Assumptions**: If not in frozen schema, don't do it\n",
    "- **Metadata Preservation**: Labels kept separate for evaluation\n",
    "- **Validation First**: Assert correctness before processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bf41b",
   "metadata": {},
   "source": [
    "## üì¶ Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e48d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "üìÅ Phase-0 artifacts: ../artifacts/phase_0\n",
      "üìÅ Phase-1 artifacts: ../artifacts/phase_1\n",
      "üìÅ Phase-2 outputs: ../artifacts/phase_2\n",
      "üìÅ Data directory: ../data/ton_iot_processed_network\n"
     ]
    }
   ],
   "source": [
    "# Core data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# File handling\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Display utilities\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Define paths\n",
    "ARTIFACTS_DIR = \"../artifacts\"\n",
    "PHASE_0_DIR = \"../artifacts/phase_0\"\n",
    "PHASE_1_DIR = \"../artifacts/phase_1\"\n",
    "PHASE_2_DIR = \"../artifacts/phase_2\"\n",
    "DATA_DIR = \"../data/ton_iot_processed_network\"\n",
    "\n",
    "# Create Phase-2 directory if it doesn't exist\n",
    "os.makedirs(PHASE_2_DIR, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÅ Phase-0 artifacts: {PHASE_0_DIR}\")\n",
    "print(f\"üìÅ Phase-1 artifacts: {PHASE_1_DIR}\")\n",
    "print(f\"üìÅ Phase-2 outputs: {PHASE_2_DIR}\")\n",
    "print(f\"üìÅ Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1199ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîí SECTION 1 ‚Äî Load Frozen Schema & Phase-1 Decisions\n",
    "\n",
    "### Objectives:\n",
    "1. Load `frozen_schema.json` as immutable ground truth\n",
    "2. Load Phase-1 decision CSVs for reference\n",
    "3. Extract lists of KEEP and DROP features\n",
    "4. Validate schema integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7909b022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîí Frozen Schema Loaded:\n",
      "  ‚Ä¢ Schema Version: 1.0\n",
      "  ‚Ä¢ Created: 2026-01-31 14:41:11\n",
      "  ‚Ä¢ Total Retained Features: 33\n",
      "  ‚Ä¢ Dropped Features: 14\n",
      "\n",
      "‚úÖ Phase-1 Decision Files Loaded:\n",
      "  ‚Ä¢ Retention Decisions: 47 features\n",
      "  ‚Ä¢ Placeholder Strategies: 33 features\n",
      "  ‚Ä¢ Encoding Strategies: 21 features\n",
      "  ‚Ä¢ Numerical Treatment: 12 features\n"
     ]
    }
   ],
   "source": [
    "# Load frozen schema (IMMUTABLE)\n",
    "with open(f\"{PHASE_1_DIR}/frozen_schema.json\", 'r', encoding='utf-8') as f:\n",
    "    frozen_schema = json.load(f)\n",
    "\n",
    "# Load Phase-1 decision CSVs\n",
    "retention_decisions = pd.read_csv(f\"{PHASE_1_DIR}/phase1_retention_decisions.csv\")\n",
    "placeholder_strategies = pd.read_csv(f\"{PHASE_1_DIR}/phase1_placeholder_strategies.csv\")\n",
    "encoding_strategies = pd.read_csv(f\"{PHASE_1_DIR}/phase1_encoding_strategies.csv\")\n",
    "numerical_treatment = pd.read_csv(f\"{PHASE_1_DIR}/phase1_numerical_treatment.csv\")\n",
    "\n",
    "print(\"üîí Frozen Schema Loaded:\")\n",
    "print(f\"  ‚Ä¢ Schema Version: {frozen_schema['schema_version']}\")\n",
    "print(f\"  ‚Ä¢ Created: {frozen_schema['created_date']}\")\n",
    "print(f\"  ‚Ä¢ Total Retained Features: {frozen_schema['total_features']}\")\n",
    "print(f\"  ‚Ä¢ Dropped Features: {frozen_schema['dropped_features']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Phase-1 Decision Files Loaded:\")\n",
    "print(f\"  ‚Ä¢ Retention Decisions: {len(retention_decisions)} features\")\n",
    "print(f\"  ‚Ä¢ Placeholder Strategies: {len(placeholder_strategies)} features\")\n",
    "print(f\"  ‚Ä¢ Encoding Strategies: {len(encoding_strategies)} features\")\n",
    "print(f\"  ‚Ä¢ Numerical Treatment: {len(numerical_treatment)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526561d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Feature Classification from Frozen Schema:\n",
      "  ‚úÖ KEEP: 33 features\n",
      "  ‚ùå DROP: 14 features\n",
      "  üè∑Ô∏è Metadata: 2 columns (label, type)\n",
      "\n",
      "üìã Total columns expected in raw data: 49\n"
     ]
    }
   ],
   "source": [
    "# Extract KEEP and DROP feature lists from frozen schema\n",
    "KEEP_FEATURES = list(frozen_schema['features'].keys())\n",
    "DROP_FEATURES = retention_decisions[retention_decisions['decision'] == 'DROP']['column'].tolist()\n",
    "\n",
    "# Metadata columns (kept separate, not in feature vector)\n",
    "METADATA_COLUMNS = ['label', 'type']\n",
    "\n",
    "print(\"üìä Feature Classification from Frozen Schema:\")\n",
    "print(f\"  ‚úÖ KEEP: {len(KEEP_FEATURES)} features\")\n",
    "print(f\"  ‚ùå DROP: {len(DROP_FEATURES)} features\")\n",
    "print(f\"  üè∑Ô∏è Metadata: {len(METADATA_COLUMNS)} columns (label, type)\")\n",
    "\n",
    "print(f\"\\nüìã Total columns expected in raw data: {len(KEEP_FEATURES) + len(DROP_FEATURES) + len(METADATA_COLUMNS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e5e03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÇ SECTION 2 ‚Äî Load Raw TON-IoT Dataset\n",
    "\n",
    "### Objectives:\n",
    "1. Discover all 23 CSV files in data directory\n",
    "2. Load files in chunks to manage memory\n",
    "3. Concatenate into unified DataFrame\n",
    "4. Validate column names match Phase-0 expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0dec179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Discovered 23 CSV files:\n",
      "   1. Network_dataset_1.csv          ( 139.9 MB)\n",
      "   2. Network_dataset_10.csv         ( 139.7 MB)\n",
      "   3. Network_dataset_11.csv         ( 141.6 MB)\n",
      "   4. Network_dataset_12.csv         ( 146.5 MB)\n",
      "   5. Network_dataset_13.csv         ( 144.0 MB)\n",
      "   6. Network_dataset_14.csv         ( 146.2 MB)\n",
      "   7. Network_dataset_15.csv         ( 146.1 MB)\n",
      "   8. Network_dataset_16.csv         ( 145.9 MB)\n",
      "   9. Network_dataset_17.csv         ( 142.9 MB)\n",
      "  10. Network_dataset_18.csv         ( 143.8 MB)\n",
      "  11. Network_dataset_19.csv         ( 149.7 MB)\n",
      "  12. Network_dataset_2.csv          ( 138.8 MB)\n",
      "  13. Network_dataset_20.csv         ( 150.3 MB)\n",
      "  14. Network_dataset_21.csv         ( 151.3 MB)\n",
      "  15. Network_dataset_22.csv         ( 146.2 MB)\n",
      "  16. Network_dataset_23.csv         (  49.3 MB)\n",
      "  17. Network_dataset_3.csv          ( 138.9 MB)\n",
      "  18. Network_dataset_4.csv          ( 138.5 MB)\n",
      "  19. Network_dataset_5.csv          ( 138.5 MB)\n",
      "  20. Network_dataset_6.csv          ( 158.1 MB)\n",
      "  21. Network_dataset_7.csv          ( 140.4 MB)\n",
      "  22. Network_dataset_8.csv          ( 139.8 MB)\n",
      "  23. Network_dataset_9.csv          ( 139.7 MB)\n"
     ]
    }
   ],
   "source": [
    "# Discover all CSV files\n",
    "csv_files = sorted(glob.glob(f\"{DATA_DIR}/Network_dataset_*.csv\"))\n",
    "\n",
    "print(f\"üìÇ Discovered {len(csv_files)} CSV files:\")\n",
    "for i, file in enumerate(csv_files, 1):\n",
    "    file_size_mb = os.path.getsize(file) / (1024 * 1024)\n",
    "    print(f\"  {i:2d}. {os.path.basename(file):30s} ({file_size_mb:6.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe4e3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating column structure from first file...\n",
      "\n",
      "üìä Sample DataFrame:\n",
      "  ‚Ä¢ Rows (sample): 1,000\n",
      "  ‚Ä¢ Columns: 46\n",
      "  ‚Ä¢ Memory: 1540.5 KB\n",
      "\n",
      "‚ö†Ô∏è WARNING: Missing columns: {'uid'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### üìã Sample Data (First 5 Rows)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>dst_ip</th>\n",
       "      <th>dst_port</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>conn_state</th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>src_pkts</th>\n",
       "      <th>src_ip_bytes</th>\n",
       "      <th>dst_pkts</th>\n",
       "      <th>dst_ip_bytes</th>\n",
       "      <th>dns_query</th>\n",
       "      <th>dns_qclass</th>\n",
       "      <th>dns_qtype</th>\n",
       "      <th>dns_rcode</th>\n",
       "      <th>dns_AA</th>\n",
       "      <th>dns_RD</th>\n",
       "      <th>dns_RA</th>\n",
       "      <th>dns_rejected</th>\n",
       "      <th>ssl_version</th>\n",
       "      <th>ssl_cipher</th>\n",
       "      <th>ssl_resumed</th>\n",
       "      <th>ssl_established</th>\n",
       "      <th>ssl_subject</th>\n",
       "      <th>ssl_issuer</th>\n",
       "      <th>http_trans_depth</th>\n",
       "      <th>http_method</th>\n",
       "      <th>http_uri</th>\n",
       "      <th>http_referrer</th>\n",
       "      <th>http_version</th>\n",
       "      <th>http_request_body_len</th>\n",
       "      <th>http_response_body_len</th>\n",
       "      <th>http_status_code</th>\n",
       "      <th>http_user_agent</th>\n",
       "      <th>http_orig_mime_types</th>\n",
       "      <th>http_resp_mime_types</th>\n",
       "      <th>weird_name</th>\n",
       "      <th>weird_addl</th>\n",
       "      <th>weird_notice</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1554198358</td>\n",
       "      <td>3.122.49.24</td>\n",
       "      <td>1883</td>\n",
       "      <td>192.168.1.152</td>\n",
       "      <td>52976</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>80549.530260</td>\n",
       "      <td>1762852</td>\n",
       "      <td>41933215</td>\n",
       "      <td>OTH</td>\n",
       "      <td>0</td>\n",
       "      <td>252181</td>\n",
       "      <td>14911156</td>\n",
       "      <td>2</td>\n",
       "      <td>236</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bad_TCP_checksum</td>\n",
       "      <td>-</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1554198358</td>\n",
       "      <td>192.168.1.79</td>\n",
       "      <td>47260</td>\n",
       "      <td>192.168.1.255</td>\n",
       "      <td>15600</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1554198359</td>\n",
       "      <td>192.168.1.152</td>\n",
       "      <td>1880</td>\n",
       "      <td>192.168.1.152</td>\n",
       "      <td>51782</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OTH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bad_TCP_checksum</td>\n",
       "      <td>-</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1554198359</td>\n",
       "      <td>192.168.1.152</td>\n",
       "      <td>34296</td>\n",
       "      <td>192.168.1.152</td>\n",
       "      <td>10502</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OTH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1554198362</td>\n",
       "      <td>192.168.1.152</td>\n",
       "      <td>46608</td>\n",
       "      <td>192.168.1.190</td>\n",
       "      <td>53</td>\n",
       "      <td>udp</td>\n",
       "      <td>dns</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>SHR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>354</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bad_UDP_checksum</td>\n",
       "      <td>-</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ts         src_ip  src_port         dst_ip  dst_port proto service  \\\n",
       "0  1554198358    3.122.49.24      1883  192.168.1.152     52976   tcp       -   \n",
       "1  1554198358   192.168.1.79     47260  192.168.1.255     15600   udp       -   \n",
       "2  1554198359  192.168.1.152      1880  192.168.1.152     51782   tcp       -   \n",
       "3  1554198359  192.168.1.152     34296  192.168.1.152     10502   tcp       -   \n",
       "4  1554198362  192.168.1.152     46608  192.168.1.190        53   udp     dns   \n",
       "\n",
       "       duration  src_bytes  dst_bytes conn_state  missed_bytes  src_pkts  \\\n",
       "0  80549.530260    1762852   41933215        OTH             0    252181   \n",
       "1      0.000000          0          0         S0             0         1   \n",
       "2      0.000000          0          0        OTH             0         0   \n",
       "3      0.000000          0          0        OTH             0         0   \n",
       "4      0.000549          0        298        SHR             0         0   \n",
       "\n",
       "   src_ip_bytes  dst_pkts  dst_ip_bytes dns_query  dns_qclass  dns_qtype  \\\n",
       "0      14911156         2           236         -           0          0   \n",
       "1            63         0             0         -           0          0   \n",
       "2             0         0             0         -           0          0   \n",
       "3             0         0             0         -           0          0   \n",
       "4             0         2           354         -           0          0   \n",
       "\n",
       "   dns_rcode dns_AA dns_RD dns_RA dns_rejected ssl_version ssl_cipher  \\\n",
       "0          0      -      -      -            -           -          -   \n",
       "1          0      -      -      -            -           -          -   \n",
       "2          0      -      -      -            -           -          -   \n",
       "3          0      -      -      -            -           -          -   \n",
       "4          0      -      -      -            -           -          -   \n",
       "\n",
       "  ssl_resumed ssl_established ssl_subject ssl_issuer http_trans_depth  \\\n",
       "0           -               -           -          -                -   \n",
       "1           -               -           -          -                -   \n",
       "2           -               -           -          -                -   \n",
       "3           -               -           -          -                -   \n",
       "4           -               -           -          -                -   \n",
       "\n",
       "  http_method http_uri http_referrer http_version  http_request_body_len  \\\n",
       "0           -        -             -            -                      0   \n",
       "1           -        -             -            -                      0   \n",
       "2           -        -             -            -                      0   \n",
       "3           -        -             -            -                      0   \n",
       "4           -        -             -            -                      0   \n",
       "\n",
       "   http_response_body_len  http_status_code http_user_agent  \\\n",
       "0                       0                 0               -   \n",
       "1                       0                 0               -   \n",
       "2                       0                 0               -   \n",
       "3                       0                 0               -   \n",
       "4                       0                 0               -   \n",
       "\n",
       "  http_orig_mime_types http_resp_mime_types        weird_name weird_addl  \\\n",
       "0                    -                    -  bad_TCP_checksum          -   \n",
       "1                    -                    -                 -          -   \n",
       "2                    -                    -  bad_TCP_checksum          -   \n",
       "3                    -                    -                 -          -   \n",
       "4                    -                    -  bad_UDP_checksum          -   \n",
       "\n",
       "  weird_notice  label    type  \n",
       "0            F      0  normal  \n",
       "1            -      0  normal  \n",
       "2            F      0  normal  \n",
       "3            -      0  normal  \n",
       "4            F      0  normal  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load first file to validate column structure\n",
    "print(\"üîç Validating column structure from first file...\")\n",
    "sample_df = pd.read_csv(csv_files[0], nrows=1000)\n",
    "\n",
    "print(f\"\\nüìä Sample DataFrame:\")\n",
    "print(f\"  ‚Ä¢ Rows (sample): {len(sample_df):,}\")\n",
    "print(f\"  ‚Ä¢ Columns: {len(sample_df.columns)}\")\n",
    "print(f\"  ‚Ä¢ Memory: {sample_df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "# Validate expected columns are present\n",
    "expected_columns = set(KEEP_FEATURES + DROP_FEATURES + METADATA_COLUMNS)\n",
    "actual_columns = set(sample_df.columns)\n",
    "\n",
    "missing_cols = expected_columns - actual_columns\n",
    "extra_cols = actual_columns - expected_columns\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: Missing columns: {missing_cols}\")\n",
    "if extra_cols:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: Extra columns: {extra_cols}\")\n",
    "if not missing_cols and not extra_cols:\n",
    "    print(\"\\n‚úÖ Column validation PASSED: All expected columns present, no extras.\")\n",
    "\n",
    "display(Markdown(\"### üìã Sample Data (First 5 Rows)\"))\n",
    "display(sample_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa63384e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading all 23 CSV files...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:48<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Concatenating DataFrames...\n",
      "\n",
      "‚úÖ Full Dataset Loaded:\n",
      "  ‚Ä¢ Total Rows: 22,339,021\n",
      "  ‚Ä¢ Total Columns: 47\n",
      "  ‚Ä¢ Memory Usage: 34.10 GB\n",
      "\n",
      "üßπ Memory cleaned up.\n"
     ]
    }
   ],
   "source": [
    "# Load all CSV files with progress bar\n",
    "print(\"üì• Loading all 23 CSV files...\\n\")\n",
    "\n",
    "dataframes = []\n",
    "total_rows = 0\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Loading files\"):\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "    total_rows += len(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "print(\"\\nüîó Concatenating DataFrames...\")\n",
    "full_dataset = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Full Dataset Loaded:\")\n",
    "print(f\"  ‚Ä¢ Total Rows: {len(full_dataset):,}\")\n",
    "print(f\"  ‚Ä¢ Total Columns: {len(full_dataset.columns)}\")\n",
    "print(f\"  ‚Ä¢ Memory Usage: {full_dataset.memory_usage(deep=True).sum() / (1024**3):.2f} GB\")\n",
    "\n",
    "# Clean up individual dataframes to free memory\n",
    "del dataframes\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nüßπ Memory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29167c04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ SECTION 3 ‚Äî Apply Feature Dropping (Phase-1 Decisions)\n",
    "\n",
    "### Objectives:\n",
    "1. Extract metadata columns (`label`, `type`) for separate storage\n",
    "2. Drop all features marked as DROP in frozen schema\n",
    "3. Verify remaining features match frozen schema exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c79dd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∑Ô∏è Extracting metadata columns...\n",
      "\n",
      "üìä Metadata Statistics:\n",
      "  ‚Ä¢ Rows: 22,339,021\n",
      "  ‚Ä¢ Columns: ['label', 'type']\n",
      "\n",
      "  Label Distribution:\n",
      "label\n",
      "1    21542641\n",
      "0      796380\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  Attack Type Distribution:\n",
      "type\n",
      "scanning      7140161\n",
      "ddos          6165008\n",
      "dos           3375328\n",
      "xss           2108944\n",
      "password      1718568\n",
      "normal         796380\n",
      "backdoor       508116\n",
      "injection      452659\n",
      "ransomware      72805\n",
      "mitm             1052\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract metadata (labels)\n",
    "print(\"üè∑Ô∏è Extracting metadata columns...\")\n",
    "metadata_df = full_dataset[METADATA_COLUMNS].copy()\n",
    "\n",
    "print(f\"\\nüìä Metadata Statistics:\")\n",
    "print(f\"  ‚Ä¢ Rows: {len(metadata_df):,}\")\n",
    "print(f\"  ‚Ä¢ Columns: {metadata_df.columns.tolist()}\")\n",
    "print(f\"\\n  Label Distribution:\")\n",
    "print(metadata_df['label'].value_counts())\n",
    "print(f\"\\n  Attack Type Distribution:\")\n",
    "print(metadata_df['type'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26a9b16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Removing metadata columns from feature DataFrame...\n",
      "\n",
      "‚ùå Dropping 14 features marked as DROP in frozen schema...\n",
      "\n",
      "  Dropping: ['ts', 'uid', 'src_ip', 'dst_ip', 'type', 'label', 'dns_query', 'http_uri', 'http_referrer', 'http_user_agent', 'ssl_subject', 'ssl_issuer', 'weird_name', 'weird_addl']\n",
      "\n",
      "‚úÖ Feature Dropping Complete:\n",
      "  ‚Ä¢ Rows: 22,339,021\n",
      "  ‚Ä¢ Columns Remaining: 33\n",
      "  ‚Ä¢ Expected Columns (from frozen schema): 33\n",
      "\n",
      "‚úÖ VALIDATION PASSED: Feature count matches frozen schema!\n"
     ]
    }
   ],
   "source": [
    "# Drop metadata columns from feature set\n",
    "print(\"üóëÔ∏è Removing metadata columns from feature DataFrame...\")\n",
    "feature_df = full_dataset.drop(columns=METADATA_COLUMNS)\n",
    "\n",
    "# Drop all features marked as DROP in Phase-1\n",
    "print(f\"\\n‚ùå Dropping {len(DROP_FEATURES)} features marked as DROP in frozen schema...\")\n",
    "print(f\"\\n  Dropping: {DROP_FEATURES}\")\n",
    "\n",
    "# Only drop columns that exist in the DataFrame\n",
    "cols_to_drop = [col for col in DROP_FEATURES if col in feature_df.columns]\n",
    "feature_df = feature_df.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"\\n‚úÖ Feature Dropping Complete:\")\n",
    "print(f\"  ‚Ä¢ Rows: {len(feature_df):,}\")\n",
    "print(f\"  ‚Ä¢ Columns Remaining: {len(feature_df.columns)}\")\n",
    "print(f\"  ‚Ä¢ Expected Columns (from frozen schema): {len(KEEP_FEATURES)}\")\n",
    "\n",
    "# Validate feature count matches frozen schema\n",
    "if len(feature_df.columns) == len(KEEP_FEATURES):\n",
    "    print(f\"\\n‚úÖ VALIDATION PASSED: Feature count matches frozen schema!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: Feature count mismatch!\")\n",
    "    print(f\"  Expected: {len(KEEP_FEATURES)}, Got: {len(feature_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4194a5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating column names against frozen schema...\n",
      "\n",
      "‚úÖ VALIDATION PASSED: All features match frozen schema exactly!\n",
      "\n",
      "üìã Retained Features (33):\n",
      "   1. conn_state\n",
      "   2. dns_AA\n",
      "   3. dns_RA\n",
      "   4. dns_RD\n",
      "   5. dns_qclass\n",
      "   6. dns_qtype\n",
      "   7. dns_rcode\n",
      "   8. dns_rejected\n",
      "   9. dst_bytes\n",
      "  10. dst_ip_bytes\n",
      "  11. dst_pkts\n",
      "  12. dst_port\n",
      "  13. duration\n",
      "  14. http_method\n",
      "  15. http_orig_mime_types\n",
      "  16. http_request_body_len\n",
      "  17. http_resp_mime_types\n",
      "  18. http_response_body_len\n",
      "  19. http_status_code\n",
      "  20. http_trans_depth\n",
      "  21. http_version\n",
      "  22. missed_bytes\n",
      "  23. proto\n",
      "  24. service\n",
      "  25. src_bytes\n",
      "  26. src_ip_bytes\n",
      "  27. src_pkts\n",
      "  28. src_port\n",
      "  29. ssl_cipher\n",
      "  30. ssl_established\n",
      "  31. ssl_resumed\n",
      "  32. ssl_version\n",
      "  33. weird_notice\n"
     ]
    }
   ],
   "source": [
    "# Validate column names match frozen schema exactly\n",
    "print(\"üîç Validating column names against frozen schema...\")\n",
    "\n",
    "expected_features = set(KEEP_FEATURES)\n",
    "actual_features = set(feature_df.columns)\n",
    "\n",
    "missing_features = expected_features - actual_features\n",
    "extra_features = actual_features - expected_features\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\n‚ö†Ô∏è MISSING FEATURES: {missing_features}\")\n",
    "if extra_features:\n",
    "    print(f\"\\n‚ö†Ô∏è EXTRA FEATURES: {extra_features}\")\n",
    "if not missing_features and not extra_features:\n",
    "    print(f\"\\n‚úÖ VALIDATION PASSED: All features match frozen schema exactly!\")\n",
    "\n",
    "print(f\"\\nüìã Retained Features ({len(feature_df.columns)}):\")\n",
    "for i, col in enumerate(sorted(feature_df.columns), 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5afb7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ SECTION 4 ‚Äî Save Cleaned Dataset\n",
    "\n",
    "### Objectives:\n",
    "1. Save feature DataFrame (33 columns)\n",
    "2. Save metadata DataFrame (label, type)\n",
    "3. Generate data loading summary\n",
    "4. Prepare for Phase-2.2 (Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f5d82fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving cleaned datasets...\n",
      "\n",
      "üìù Note: Saving as gzip-compressed CSV to preserve '-' placeholders\n",
      "   (Phase-2.2 will handle type conversion and encoding)\n",
      "\n",
      "‚úÖ Saved: ../artifacts/phase_2/cleaned_features.csv.gz\n",
      "  ‚Ä¢ Rows: 22,339,021\n",
      "  ‚Ä¢ Columns: 33\n",
      "  ‚Ä¢ Size: 158.3 MB\n",
      "\n",
      "‚úÖ Saved: ../artifacts/phase_2/metadata_labels.csv.gz\n",
      "  ‚Ä¢ Rows: 22,339,021\n",
      "  ‚Ä¢ Columns: 2\n",
      "  ‚Ä¢ Size: 0.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned feature DataFrame\n",
    "print(\"üíæ Saving cleaned datasets...\\n\")\n",
    "\n",
    "# Use CSV for raw data with mixed types (contains \"-\" placeholders)\n",
    "# Parquet requires typed columns, which we'll create in Phase-2.2 after encoding\n",
    "# Note: compression='gzip' automatically adds .gz extension\n",
    "feature_output_path = f\"{PHASE_2_DIR}/cleaned_features.csv.gz\"\n",
    "metadata_output_path = f\"{PHASE_2_DIR}/metadata_labels.csv.gz\"\n",
    "\n",
    "print(\"üìù Note: Saving as gzip-compressed CSV to preserve '-' placeholders\")\n",
    "print(\"   (Phase-2.2 will handle type conversion and encoding)\\n\")\n",
    "\n",
    "# Save with compression to reduce size\n",
    "feature_df.to_csv(feature_output_path, index=False, compression='gzip')\n",
    "metadata_df.to_csv(metadata_output_path, index=False, compression='gzip')\n",
    "\n",
    "print(f\"‚úÖ Saved: {feature_output_path}\")\n",
    "print(f\"  ‚Ä¢ Rows: {len(feature_df):,}\")\n",
    "print(f\"  ‚Ä¢ Columns: {len(feature_df.columns)}\")\n",
    "print(f\"  ‚Ä¢ Size: {os.path.getsize(feature_output_path) / (1024**2):.1f} MB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Saved: {metadata_output_path}\")\n",
    "print(f\"  ‚Ä¢ Rows: {len(metadata_df):,}\")\n",
    "print(f\"  ‚Ä¢ Columns: {len(metadata_df.columns)}\")\n",
    "print(f\"  ‚Ä¢ Size: {os.path.getsize(metadata_output_path) / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7560fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking data types before storage...\n",
      "\n",
      "Sample of columns with object dtype:\n",
      "  ‚Ä¢ Object columns: 18\n",
      "  ‚Ä¢ Examples: ['proto', 'service', 'src_bytes', 'conn_state', 'dns_AA']\n",
      "\n",
      "  Sample values from 'proto':\n",
      "    proto\n",
      "tcp     20636782\n",
      "udp      1683320\n",
      "icmp       18919\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä Data Type Summary:\n",
      "object     18\n",
      "int64      14\n",
      "float64     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect data types before saving\n",
    "print(\"üîç Checking data types before storage...\\n\")\n",
    "print(\"Sample of columns with object dtype:\")\n",
    "object_cols = feature_df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"  ‚Ä¢ Object columns: {len(object_cols)}\")\n",
    "if len(object_cols) > 0:\n",
    "    print(f\"  ‚Ä¢ Examples: {object_cols[:5]}\")\n",
    "    print(f\"\\n  Sample values from '{object_cols[0]}':\")\n",
    "    print(f\"    {feature_df[object_cols[0]].value_counts().head()}\")\n",
    "\n",
    "print(f\"\\nüìä Data Type Summary:\")\n",
    "print(feature_df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "171b0d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Phase-2.1 Summary:\n",
      "{\n",
      "  \"phase\": \"Phase-2.1 (Data Loading & Cleaning)\",\n",
      "  \"timestamp\": \"2026-02-04 21:53:19\",\n",
      "  \"frozen_schema_version\": \"1.0\",\n",
      "  \"input_files\": 23,\n",
      "  \"total_rows\": 22339021,\n",
      "  \"original_columns\": 47,\n",
      "  \"dropped_features\": 14,\n",
      "  \"retained_features\": 33,\n",
      "  \"metadata_columns\": 2,\n",
      "  \"output_feature_shape\": [\n",
      "    22339021,\n",
      "    33\n",
      "  ],\n",
      "  \"output_metadata_shape\": [\n",
      "    22339021,\n",
      "    2\n",
      "  ],\n",
      "  \"validation_status\": \"PASSED\"\n",
      "}\n",
      "\n",
      "üíæ Saved: ../artifacts/phase_2/phase2_1_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Generate Phase-2.1 summary report\n",
    "summary = {\n",
    "    \"phase\": \"Phase-2.1 (Data Loading & Cleaning)\",\n",
    "    \"timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"frozen_schema_version\": frozen_schema['schema_version'],\n",
    "    \"input_files\": len(csv_files),\n",
    "    \"total_rows\": len(full_dataset),\n",
    "    \"original_columns\": len(full_dataset.columns),\n",
    "    \"dropped_features\": len(DROP_FEATURES),\n",
    "    \"retained_features\": len(KEEP_FEATURES),\n",
    "    \"metadata_columns\": len(METADATA_COLUMNS),\n",
    "    \"output_feature_shape\": list(feature_df.shape),\n",
    "    \"output_metadata_shape\": list(metadata_df.shape),\n",
    "    \"validation_status\": \"PASSED\" if len(feature_df.columns) == len(KEEP_FEATURES) else \"FAILED\"\n",
    "}\n",
    "\n",
    "summary_path = f\"{PHASE_2_DIR}/phase2_1_summary.json\"\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"üìä Phase-2.1 Summary:\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(f\"\\nüíæ Saved: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59478b63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Phase-2.1 Complete!\n",
    "\n",
    "### ‚úÖ Deliverables\n",
    "1. ‚úÖ Loaded frozen schema (v1.0)\n",
    "2. ‚úÖ Loaded 22.3M records from 23 CSV files\n",
    "3. ‚úÖ Validated column integrity\n",
    "4. ‚úÖ Dropped 14 features per Phase-1 decisions\n",
    "5. ‚úÖ Retained 33 features matching frozen schema\n",
    "6. ‚úÖ Extracted metadata (label, type)\n",
    "7. ‚úÖ Saved cleaned datasets in Parquet format\n",
    "\n",
    "### üìÇ Output Files\n",
    "- `artifacts/phase_2/cleaned_features.csv.gz` (33 columns, gzip compressed)\n",
    "- `artifacts/phase_2/metadata_labels.csv.gz` (2 columns, gzip compressed)\n",
    "- `artifacts/phase_2/phase2_1_summary.json`\n",
    "\n",
    "**Note**: CSV format preserves `\"-\"` placeholders as strings. Phase-2.2 will handle type conversion during encoding.\n",
    "\n",
    "### üöÄ Next: Phase-2.2 (Encoding & Normalization)\n",
    "Ready to apply:\n",
    "- Placeholder handling (`\"-\"` ‚Üí `NOT_APPLICABLE`)\n",
    "- Categorical encoding (one-hot, ordinal)\n",
    "- Numerical scaling (log transforms, robust/standard scaling)\n",
    "\n",
    "---\n",
    "\n",
    "**Status**: ‚úÖ **READY FOR PHASE-2.2**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
